{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6e71d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Fast Scraping Configuration:\n",
      "   Start ID: 650968\n",
      "   End ID: 650978\n",
      "   Total dogs: 10\n",
      "   Output: dogs3.csv\n",
      "\n",
      "ðŸš€ Starting fast scraping for 10 dogs (IDs 650968-650977)\n",
      "ðŸ“‚ Output file: dogs3.csv\n",
      "==================================================\n",
      "Loading existing data from dogs3.csv...\n",
      "Loaded 11042 existing records, 1961 unique races\n",
      "Collecting race URLs from all dogs...\n",
      "Processing dog 1/10: 650968\n",
      "Collecting race URLs from all dogs...\n",
      "Processing dog 1/10: 650968\n",
      "Processing dog 2/10: 650969\n",
      "Processing dog 2/10: 650969\n",
      "Processing dog 3/10: 650970\n",
      "Processing dog 3/10: 650970\n"
     ]
    }
   ],
   "source": [
    "# FAST SCRAPING NOTEBOOK - Single Cell Solution\n",
    "# Import and use the optimized fast scraping function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Add current directory to path to import our module\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Import the fast scraping function\n",
    "from scraping.fast_scraping import fast_scrape_multiple_dogs\n",
    "\n",
    "def run_fast_scraping(start_id=637322, end_id=637332, output_file=\"dogs3.csv\"):\n",
    "    \"\"\"\n",
    "    Run fast scraping for a range of dog IDs\n",
    "    \n",
    "    Args:\n",
    "        start_id: Starting dog ID\n",
    "        end_id: Ending dog ID (exclusive)\n",
    "        output_file: Output CSV file name\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate dog ID list\n",
    "    dog_ids = [str(i) for i in range(start_id, end_id)]\n",
    "    \n",
    "    print(f\"ðŸš€ Starting fast scraping for {len(dog_ids)} dogs (IDs {start_id}-{end_id-1})\")\n",
    "    print(f\"ðŸ“‚ Output file: {output_file}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Run the fast scraping\n",
    "        total_records = fast_scrape_multiple_dogs(\n",
    "            dog_ids=dog_ids,\n",
    "            output_file=output_file,\n",
    "            batch_size=25  # Save progress every 25 races\n",
    "        )\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        print(\"âœ… SCRAPING COMPLETED!\")\n",
    "        print(f\"ðŸ“Š Total records scraped: {total_records}\")\n",
    "        print(f\"â±ï¸ Total time: {elapsed_time:.1f} seconds\")\n",
    "        print(f\"ðŸ• Dogs processed: {len(dog_ids)}\")\n",
    "        print(f\"ðŸ“ˆ Speed: {len(dog_ids)/elapsed_time*60:.1f} dogs per minute\")\n",
    "        print(f\"ðŸ’¾ Data saved to: {output_file}\")\n",
    "        \n",
    "        # FIXED: Proper calculation and analysis\n",
    "        if total_records > 0:\n",
    "            avg_races_per_dog = total_records / len(dog_ids)\n",
    "            print(f\"ðŸ Average records per dog: {avg_races_per_dog:.1f}\")\n",
    "            \n",
    "            # Check if this seems reasonable\n",
    "            if avg_races_per_dog > 200:\n",
    "                print(\"âš ï¸  WARNING: Unusually high records per dog detected!\")\n",
    "                print(\"   This suggests the scraper may be extracting duplicate records\")\n",
    "                print(\"   or pulling data from multiple dogs per race page.\")\n",
    "                print(f\"   Expected: 10-100 races per dog, Got: {avg_races_per_dog:.1f}\")\n",
    "                \n",
    "                # Try to diagnose the issue\n",
    "                if os.path.exists(output_file):\n",
    "                    import pandas as pd\n",
    "                    try:\n",
    "                        df = pd.read_csv(output_file)\n",
    "                        unique_dogs = df['dog_id'].nunique() if 'dog_id' in df.columns else 0\n",
    "                        unique_races = df['race_id'].nunique() if 'race_id' in df.columns else 0\n",
    "                        \n",
    "                        print(f\"   ðŸ” Diagnosis:\")\n",
    "                        print(f\"     - Total records: {len(df)}\")\n",
    "                        print(f\"     - Unique dogs: {unique_dogs}\")\n",
    "                        print(f\"     - Unique races: {unique_races}\")\n",
    "                        print(f\"     - Dogs processed: {len(dog_ids)}\")\n",
    "                        \n",
    "                        if unique_dogs > len(dog_ids):\n",
    "                            print(f\"     âŒ ISSUE: Found {unique_dogs} unique dogs but only processed {len(dog_ids)}\")\n",
    "                            print(f\"        The scraper is extracting ALL dogs from each race page,\")\n",
    "                            print(f\"        not just the target dog!\")\n",
    "                        \n",
    "                        if unique_races > total_records / 6:  # Assuming ~6 dogs per race\n",
    "                            print(f\"     âŒ ISSUE: Too many unique races relative to records\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"   Error analyzing data: {e}\")\n",
    "            else:\n",
    "                print(\"âœ… Records per dog seems reasonable for greyhound racing\")\n",
    "        \n",
    "        return total_records\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during scraping: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "# CONFIGURATION - Edit these values as needed\n",
    "START_DOG_ID = 650968  # Starting dog ID  \n",
    "END_DOG_ID = 650978    # Ending dog ID (reduced to 10 dogs for testing)\n",
    "OUTPUT_FILE = \"dogs3.csv\"  # Output file name\n",
    "\n",
    "# RUN THE SCRAPING\n",
    "print(\"ðŸ”§ Fast Scraping Configuration:\")\n",
    "print(f\"   Start ID: {START_DOG_ID}\")\n",
    "print(f\"   End ID: {END_DOG_ID}\")\n",
    "print(f\"   Total dogs: {END_DOG_ID - START_DOG_ID}\")\n",
    "print(f\"   Output: {OUTPUT_FILE}\")\n",
    "print()\n",
    "\n",
    "# Execute the scraping\n",
    "total_scraped = run_fast_scraping(\n",
    "    start_id=START_DOG_ID,\n",
    "    end_id=END_DOG_ID,\n",
    "    output_file=OUTPUT_FILE\n",
    ")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\nðŸŽ¯ FINAL RESULT: {total_scraped} total records scraped\")\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    file_size = os.path.getsize(OUTPUT_FILE) / 1024  # KB\n",
    "    print(f\"ðŸ“ File size: {file_size:.1f} KB\")\n",
    "    \n",
    "    # Additional analysis\n",
    "    print(\"\\nðŸ”¬ DETAILED ANALYSIS:\")\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv(OUTPUT_FILE)\n",
    "        \n",
    "        if 'dog_id' in df.columns:\n",
    "            unique_dogs = df['dog_id'].nunique()\n",
    "            dogs_processed = END_DOG_ID - START_DOG_ID\n",
    "            \n",
    "            print(f\"   ðŸ“Š Data Breakdown:\")\n",
    "            print(f\"     - Total records: {len(df)}\")\n",
    "            print(f\"     - Unique dogs in data: {unique_dogs}\")  \n",
    "            print(f\"     - Dogs we tried to scrape: {dogs_processed}\")\n",
    "            print(f\"     - Records per unique dog: {len(df)/unique_dogs:.1f}\")\n",
    "            \n",
    "            if unique_dogs != dogs_processed:\n",
    "                print(f\"   âš ï¸  MISMATCH: Expected {dogs_processed} dogs, found {unique_dogs}\")\n",
    "                \n",
    "            # Show sample of dog IDs found\n",
    "            sample_dogs = sorted(df['dog_id'].unique())[:10]\n",
    "            print(f\"     - Sample dog IDs found: {sample_dogs}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"   âŒ No 'dog_id' column found in output\")\n",
    "            print(f\"     Available columns: {list(df.columns)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error analyzing output: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacbc93e",
   "metadata": {},
   "source": [
    "# Greyhound Racing Dataset - Column Explanations\n",
    "\n",
    "## Overview\n",
    "This CSV file contains comprehensive greyhound racing data with 42 columns capturing race information, dog characteristics, performance metrics, and derived features for machine learning analysis.\n",
    "\n",
    "## Column Descriptions\n",
    "\n",
    "### Basic Race Information\n",
    "- **Column 1-2**: `meeting_id`, `race_id` - Unique identifiers for the racing meeting and specific race\n",
    "- **Column 3**: `date` - Race date (e.g., \"Thursday 6th February 2025\")\n",
    "- **Column 4**: `track` - Racing venue (Newcastle, Towcester, Doncaster, etc.)\n",
    "- **Column 5**: `time` - Race start time\n",
    "- **Column 6**: `grade` - Race grade/class (A3, A4, B4, etc.)\n",
    "- **Column 7**: `distance` - Race distance in meters (450m, 480m, 500m, etc.)\n",
    "- **Column 8**: `prize_info` - Prize money breakdown for winners and placed dogs\n",
    "\n",
    "### Race Performance\n",
    "- **Column 9**: `finishing_position_text` - Finishing position as text (1st, 2nd, 3rd, etc.)\n",
    "- **Column 10**: `trap_number` - Starting trap number (1-6)\n",
    "- **Column 11**: `dog_id` - Unique identifier for the dog\n",
    "- **Column 12**: `dog_name` - Name of the greyhound\n",
    "- **Column 13**: `trainer` - Trainer's name\n",
    "- **Column 14**: `comment` - Race commentary describing the dog's performance\n",
    "- **Column 15**: `odds` - Betting odds (e.g., \"3/1\", \"11/8F\" where F = favorite)\n",
    "- **Column 16**: `sectional_time` - Split time at specific distance point\n",
    "- **Column 17**: `finish_time` - Final race time with margin behind winner\n",
    "- **Column 18**: `date_of_birth` - Dog's birth date\n",
    "- **Column 19**: `weight` - Dog's racing weight in kg\n",
    "- **Column 20**: `color_sex` - Color and sex code (e.g., \"b - bk\" = bitch - black)\n",
    "- **Column 21**: `sire` - Father's name\n",
    "- **Column 22**: `dam` - Mother's name\n",
    "- **Column 23**: `breeding_info` - Combined breeding information\n",
    "- **Column 24**: `url` - Link to race details\n",
    "\n",
    "### Processed Features\n",
    "- **Column 25**: `distance_numeric` - Race distance as numeric value\n",
    "- **Column 26**: `finishing_position` - Finishing position as number (1.0, 2.0, etc.)\n",
    "- **Column 27**: `weight_numeric` - Weight as numeric value\n",
    "- **Column 28**: `trap_number_numeric` - Trap number as numeric\n",
    "- **Column 29**: `sectional_time_numeric` - Sectional time as number\n",
    "- **Column 30**: `won_race` - Binary indicator (1 if won, 0 if not)\n",
    "\n",
    "### Performance Analysis Features\n",
    "- **Column 31**: `margin_lengths` - Margin behind winner in lengths\n",
    "- **Column 32**: `odds_numeric` - Converted odds as decimal number\n",
    "- **Column 33**: `color_code` - Simplified color code (bk, bd, be, f, etc.)\n",
    "- **Column 34**: `is_favorite` - Binary indicator if dog was favorite\n",
    "- **Column 35**: `early_pace` - Indicator of early speed/position\n",
    "- **Column 36**: `led_at_some_point` - Whether dog led during race\n",
    "- **Column 37**: `bumped_or_crowded` - Indicator of racing interference\n",
    "- **Column 38**: `clear_run` - Whether dog had unimpeded run\n",
    "- **Column 39**: `ran_on` - Whether dog finished strongly\n",
    "- **Column 40**: `checked_or_blocked` - Racing trouble indicators\n",
    "- **Column 41**: `wide_run` - Whether dog raced wide\n",
    "\n",
    "### Statistical Features\n",
    "- **Column 42**: `performance_score` - Calculated performance metric\n",
    "- **Column 43**: `is_short_distance` - Binary indicator for sprint races\n",
    "- **Column 44**: `is_long_distance` - Binary indicator for distance races  \n",
    "- **Column 45**: `is_middle_distance` - Binary indicator for middle distance\n",
    "- **Column 46**: `track_type` - Numeric track type classification\n",
    "\n",
    "## Key Insights from the Data\n",
    "\n",
    "### Race Grades\n",
    "- **A grades**: Higher class races (A2, A3, A4, etc.)\n",
    "- **B grades**: Mid-level competition  \n",
    "- **D grades**: Lower class/maiden races\n",
    "- **HP/OR**: Handicap/Open races\n",
    "\n",
    "### Performance Indicators\n",
    "The comment field contains valuable racing information:\n",
    "- **\"ALed\"** = Always led\n",
    "- **\"QAw\"** = Quick away from traps\n",
    "- **\"Crd\"** = Crowded during race\n",
    "- **\"Bmp\"** = Bumped by other dogs\n",
    "- **\"RnOn\"** = Ran on strongly at finish (positive - dog accelerated/finished with strong pace in final stretch)\n",
    "- **\"SAw\"** = Slow away from traps\n",
    "- **\"Wide\"** = Raced wide around bends\n",
    "\n",
    "### Distance Categories\n",
    "- **Short**: 245m-285m (sprint races)\n",
    "- **Middle**: 400m-450m (standard distances)  \n",
    "- **Long**: 480m-500m+ (staying races)\n",
    "\n",
    "## Racing Commentary Explanation\n",
    "\n",
    "### \"Ran On\" (RnOn) - Detailed Meaning:\n",
    "In greyhound racing, **\"Ran On\"** is a **positive performance indicator** that means:\n",
    "\n",
    "1. **Strong Finish**: The dog accelerated or maintained strong pace in the final portion of the race\n",
    "2. **Late Speed**: Shows the dog has stamina and finishing kick\n",
    "3. **Closing Ground**: Often indicates the dog was gaining on leaders or maintaining position strongly\n",
    "4. **Good Fitness**: Suggests the dog is in good racing condition\n",
    "5. **Distance Suitability**: May indicate the dog suits longer distances where stamina matters\n",
    "\n",
    "**This is GOOD performance** - it shows the dog finished the race strongly rather than tiring. Dogs that \"run on\" are often considered to have good racing fitness and potential for improvement at longer distances.\n",
    "\n",
    "**Contrast with negative terms**:\n",
    "- \"Tired\" = Dog slowed significantly in final stretch\n",
    "- \"Faded\" = Dog lost position/pace late in race\n",
    "- \"Weakened\" = Dog showed lack of stamina\n",
    "\n",
    "This dataset appears designed for predictive modeling of greyhound race outcomes, with features capturing both historical performance and race-day factors that influence results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc5dd2d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d5c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME PROJECTION CALCULATOR\n",
    "# Based on actual performance: 21 dogs in 204 seconds\n",
    "\n",
    "def calculate_scraping_projections(dogs_scraped, time_taken, target_dogs=100000):\n",
    "    \"\"\"Calculate time projections for large-scale scraping\"\"\"\n",
    "    \n",
    "    seconds_per_dog = time_taken / dogs_scraped\n",
    "    \n",
    "    print(\"â±ï¸ SCRAPING TIME PROJECTIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Current Performance:\")\n",
    "    print(f\"  - Dogs scraped: {dogs_scraped}\")\n",
    "    print(f\"  - Time taken: {time_taken:.1f} seconds ({time_taken/60:.1f} minutes)\")\n",
    "    print(f\"  - Rate: {seconds_per_dog:.1f} seconds per dog\")\n",
    "    print(f\"  - Speed: {3600/seconds_per_dog:.1f} dogs per hour\")\n",
    "    \n",
    "    print(f\"\\nProjection for {target_dogs:,} dogs:\")\n",
    "    \n",
    "    total_seconds = target_dogs * seconds_per_dog\n",
    "    hours = total_seconds / 3600\n",
    "    days = hours / 24\n",
    "    \n",
    "    print(f\"  - Total time: {total_seconds:,.0f} seconds\")\n",
    "    print(f\"  - Hours: {hours:.1f} hours\")\n",
    "    print(f\"  - Days: {days:.1f} days\")\n",
    "    \n",
    "    if days > 7:\n",
    "        weeks = days / 7\n",
    "        print(f\"  - Weeks: {weeks:.1f} weeks\")\n",
    "    \n",
    "    # Show different scenarios\n",
    "    print(f\"\\nTime estimates for different scales:\")\n",
    "    scales = [1000, 10000, 50000, 100000, 500000]\n",
    "    \n",
    "    for scale in scales:\n",
    "        scale_seconds = scale * seconds_per_dog\n",
    "        scale_hours = scale_seconds / 3600\n",
    "        scale_days = scale_hours / 24\n",
    "        \n",
    "        if scale_days < 1:\n",
    "            print(f\"  - {scale:,} dogs: {scale_hours:.1f} hours\")\n",
    "        elif scale_days < 7:\n",
    "            print(f\"  - {scale:,} dogs: {scale_days:.1f} days\")\n",
    "        else:\n",
    "            scale_weeks = scale_days / 7\n",
    "            print(f\"  - {scale:,} dogs: {scale_days:.1f} days ({scale_weeks:.1f} weeks)\")\n",
    "    \n",
    "    # Optimization suggestions\n",
    "    print(f\"\\nðŸ’¡ OPTIMIZATION RECOMMENDATIONS:\")\n",
    "    \n",
    "    if seconds_per_dog > 5:\n",
    "        print(\"  âš ï¸ Current speed is quite slow (>5 sec/dog)\")\n",
    "        print(\"  Suggestions:\")\n",
    "        print(\"    - Use more HTTP requests, less Selenium\")\n",
    "        print(\"    - Implement parallel processing\")\n",
    "        print(\"    - Cache/skip already processed dogs\")\n",
    "        print(\"    - Use headless browser optimization\")\n",
    "        \n",
    "    target_speed = 2.0  # 2 seconds per dog target\n",
    "    if seconds_per_dog > target_speed:\n",
    "        improvement_factor = seconds_per_dog / target_speed\n",
    "        optimized_time = total_seconds / improvement_factor\n",
    "        optimized_days = optimized_time / 86400\n",
    "        \n",
    "        print(f\"  ðŸŽ¯ If optimized to {target_speed} sec/dog:\")\n",
    "        print(f\"    - {target_dogs:,} dogs would take: {optimized_days:.1f} days\")\n",
    "        print(f\"    - Speed improvement needed: {improvement_factor:.1f}x faster\")\n",
    "\n",
    "# Calculate based on your actual results\n",
    "calculate_scraping_projections(\n",
    "    dogs_scraped=21,\n",
    "    time_taken=204,\n",
    "    target_dogs=100000\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ðŸš¨ REALITY CHECK:\")\n",
    "print(\"11+ days of continuous scraping is not practical!\")\n",
    "print(\"\\nBetter approach:\")\n",
    "print(\"1. ðŸ“Š Focus on specific valuable ranges (recent dogs)\")\n",
    "print(\"2. âš¡ Optimize scraper to <2 seconds per dog\") \n",
    "print(\"3. ðŸ”„ Use parallel processing\")\n",
    "print(\"4. ðŸ’¾ Implement smart caching/resume capability\")\n",
    "print(\"5. ðŸŽ¯ Target ~10,000-50,000 most valuable dogs instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1404e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAST SCRAPING NOTEBOOK - Single Cell Solution\n",
    "# Import and use the optimized fast scraping function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Add current directory to path to import our module\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Import the fast scraping function\n",
    "from fast_scraping import fast_scrape_multiple_dogs\n",
    "\n",
    "def run_fast_scraping(start_id=637322, end_id=637332, output_file=\"dogs3.csv\"):\n",
    "    \"\"\"\n",
    "    Run fast scraping for a range of dog IDs\n",
    "    \n",
    "    Args:\n",
    "        start_id: Starting dog ID\n",
    "        end_id: Ending dog ID (exclusive)\n",
    "        output_file: Output CSV file name\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate dog ID list\n",
    "    dog_ids = [str(i) for i in range(start_id, end_id)]\n",
    "    \n",
    "    print(f\"ðŸš€ Starting fast scraping for {len(dog_ids)} dogs (IDs {start_id}-{end_id-1})\")\n",
    "    print(f\"ðŸ“‚ Output file: {output_file}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Run the fast scraping\n",
    "        total_records = fast_scrape_multiple_dogs(\n",
    "            dog_ids=dog_ids,\n",
    "            output_file=output_file,\n",
    "            batch_size=25  # Save progress every 25 races\n",
    "        )\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        print(\"âœ… SCRAPING COMPLETED!\")\n",
    "        print(f\"ðŸ“Š Total records scraped: {total_records}\")\n",
    "        print(f\"â±ï¸ Total time: {elapsed_time:.1f} seconds\")\n",
    "        print(f\"ðŸ• Dogs processed: {len(dog_ids)}\")\n",
    "        print(f\"ðŸ“ˆ Speed: {len(dog_ids)/elapsed_time*60:.1f} dogs per minute\")\n",
    "        print(f\"ðŸ’¾ Data saved to: {output_file}\")\n",
    "        \n",
    "        # FIXED: Proper calculation and analysis\n",
    "        if total_records > 0:\n",
    "            avg_races_per_dog = total_records / len(dog_ids)\n",
    "            print(f\"ðŸ Average records per dog: {avg_races_per_dog:.1f}\")\n",
    "            \n",
    "            # Check if this seems reasonable\n",
    "            if avg_races_per_dog > 200:\n",
    "                print(\"âš ï¸  WARNING: Unusually high records per dog detected!\")\n",
    "                print(\"   This suggests the scraper may be extracting duplicate records\")\n",
    "                print(\"   or pulling data from multiple dogs per race page.\")\n",
    "                print(f\"   Expected: 10-100 races per dog, Got: {avg_races_per_dog:.1f}\")\n",
    "                \n",
    "                # Try to diagnose the issue\n",
    "                if os.path.exists(output_file):\n",
    "                    import pandas as pd\n",
    "                    try:\n",
    "                        df = pd.read_csv(output_file)\n",
    "                        unique_dogs = df['dog_id'].nunique() if 'dog_id' in df.columns else 0\n",
    "                        unique_races = df['race_id'].nunique() if 'race_id' in df.columns else 0\n",
    "                        \n",
    "                        print(f\"   ðŸ” Diagnosis:\")\n",
    "                        print(f\"     - Total records: {len(df)}\")\n",
    "                        print(f\"     - Unique dogs: {unique_dogs}\")\n",
    "                        print(f\"     - Unique races: {unique_races}\")\n",
    "                        print(f\"     - Dogs processed: {len(dog_ids)}\")\n",
    "                        \n",
    "                        if unique_dogs > len(dog_ids):\n",
    "                            print(f\"     âŒ ISSUE: Found {unique_dogs} unique dogs but only processed {len(dog_ids)}\")\n",
    "                            print(f\"        The scraper is extracting ALL dogs from each race page,\")\n",
    "                            print(f\"        not just the target dog!\")\n",
    "                        \n",
    "                        if unique_races > total_records / 6:  # Assuming ~6 dogs per race\n",
    "                            print(f\"     âŒ ISSUE: Too many unique races relative to records\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"   Error analyzing data: {e}\")\n",
    "            else:\n",
    "                print(\"âœ… Records per dog seems reasonable for greyhound racing\")\n",
    "        \n",
    "        return total_records\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during scraping: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "# CONFIGURATION - Edit these values as needed\n",
    "START_DOG_ID = 650947  # Starting dog ID  \n",
    "END_DOG_ID = 650957    # Ending dog ID (reduced to 10 dogs for testing)\n",
    "OUTPUT_FILE = \"dogs3.csv\"  # Output file name\n",
    "\n",
    "# RUN THE SCRAPING\n",
    "print(\"ðŸ”§ Fast Scraping Configuration:\")\n",
    "print(f\"   Start ID: {START_DOG_ID}\")\n",
    "print(f\"   End ID: {END_DOG_ID}\")\n",
    "print(f\"   Total dogs: {END_DOG_ID - START_DOG_ID}\")\n",
    "print(f\"   Output: {OUTPUT_FILE}\")\n",
    "print()\n",
    "\n",
    "# Execute the scraping\n",
    "total_scraped = run_fast_scraping(\n",
    "    start_id=START_DOG_ID,\n",
    "    end_id=END_DOG_ID,\n",
    "    output_file=OUTPUT_FILE\n",
    ")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\nðŸŽ¯ FINAL RESULT: {total_scraped} total records scraped\")\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    file_size = os.path.getsize(OUTPUT_FILE) / 1024  # KB\n",
    "    print(f\"ðŸ“ File size: {file_size:.1f} KB\")\n",
    "    \n",
    "    # Additional analysis\n",
    "    print(\"\\nðŸ”¬ DETAILED ANALYSIS:\")\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv(OUTPUT_FILE)\n",
    "        \n",
    "        if 'dog_id' in df.columns:\n",
    "            unique_dogs = df['dog_id'].nunique()\n",
    "            dogs_processed = END_DOG_ID - START_DOG_ID\n",
    "            \n",
    "            print(f\"   ðŸ“Š Data Breakdown:\")\n",
    "            print(f\"     - Total records: {len(df)}\")\n",
    "            print(f\"     - Unique dogs in data: {unique_dogs}\")  \n",
    "            print(f\"     - Dogs we tried to scrape: {dogs_processed}\")\n",
    "            print(f\"     - Records per unique dog: {len(df)/unique_dogs:.1f}\")\n",
    "            \n",
    "            if unique_dogs != dogs_processed:\n",
    "                print(f\"   âš ï¸  MISMATCH: Expected {dogs_processed} dogs, found {unique_dogs}\")\n",
    "                \n",
    "            # Show sample of dog IDs found\n",
    "            sample_dogs = sorted(df['dog_id'].unique())[:10]\n",
    "            print(f\"     - Sample dog IDs found: {sample_dogs}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"   âŒ No 'dog_id' column found in output\")\n",
    "            print(f\"     Available columns: {list(df.columns)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error analyzing output: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f59c0ad",
   "metadata": {},
   "source": [
    "# Greyhound Racing Dataset - Column Explanations\n",
    "\n",
    "## Overview\n",
    "This CSV file contains comprehensive greyhound racing data with 42 columns capturing race information, dog characteristics, performance metrics, and derived features for machine learning analysis.\n",
    "\n",
    "## Column Descriptions\n",
    "\n",
    "### Basic Race Information\n",
    "- **Column 1-2**: `meeting_id`, `race_id` - Unique identifiers for the racing meeting and specific race\n",
    "- **Column 3**: `date` - Race date (e.g., \"Thursday 6th February 2025\")\n",
    "- **Column 4**: `track` - Racing venue (Newcastle, Towcester, Doncaster, etc.)\n",
    "- **Column 5**: `time` - Race start time\n",
    "- **Column 6**: `grade` - Race grade/class (A3, A4, B4, etc.)\n",
    "- **Column 7**: `distance` - Race distance in meters (450m, 480m, 500m, etc.)\n",
    "- **Column 8**: `prize_info` - Prize money breakdown for winners and placed dogs\n",
    "\n",
    "### Race Performance\n",
    "- **Column 9**: `finishing_position_text` - Finishing position as text (1st, 2nd, 3rd, etc.)\n",
    "- **Column 10**: `trap_number` - Starting trap number (1-6)\n",
    "- **Column 11**: `dog_id` - Unique identifier for the dog\n",
    "- **Column 12**: `dog_name` - Name of the greyhound\n",
    "- **Column 13**: `trainer` - Trainer's name\n",
    "- **Column 14**: `comment` - Race commentary describing the dog's performance\n",
    "- **Column 15**: `odds` - Betting odds (e.g., \"3/1\", \"11/8F\" where F = favorite)\n",
    "- **Column 16**: `sectional_time` - Split time at specific distance point\n",
    "- **Column 17**: `finish_time` - Final race time with margin behind winner\n",
    "- **Column 18**: `date_of_birth` - Dog's birth date\n",
    "- **Column 19**: `weight` - Dog's racing weight in kg\n",
    "- **Column 20**: `color_sex` - Color and sex code (e.g., \"b - bk\" = bitch - black)\n",
    "- **Column 21**: `sire` - Father's name\n",
    "- **Column 22**: `dam` - Mother's name\n",
    "- **Column 23**: `breeding_info` - Combined breeding information\n",
    "- **Column 24**: `url` - Link to race details\n",
    "\n",
    "### Processed Features\n",
    "- **Column 25**: `distance_numeric` - Race distance as numeric value\n",
    "- **Column 26**: `finishing_position` - Finishing position as number (1.0, 2.0, etc.)\n",
    "- **Column 27**: `weight_numeric` - Weight as numeric value\n",
    "- **Column 28**: `trap_number_numeric` - Trap number as numeric\n",
    "- **Column 29**: `sectional_time_numeric` - Sectional time as number\n",
    "- **Column 30**: `won_race` - Binary indicator (1 if won, 0 if not)\n",
    "\n",
    "### Performance Analysis Features\n",
    "- **Column 31**: `margin_lengths` - Margin behind winner in lengths\n",
    "- **Column 32**: `odds_numeric` - Converted odds as decimal number\n",
    "- **Column 33**: `color_code` - Simplified color code (bk, bd, be, f, etc.)\n",
    "- **Column 34**: `is_favorite` - Binary indicator if dog was favorite\n",
    "- **Column 35**: `early_pace` - Indicator of early speed/position\n",
    "- **Column 36**: `led_at_some_point` - Whether dog led during race\n",
    "- **Column 37**: `bumped_or_crowded` - Indicator of racing interference\n",
    "- **Column 38**: `clear_run` - Whether dog had unimpeded run\n",
    "- **Column 39**: `ran_on` - Whether dog finished strongly\n",
    "- **Column 40**: `checked_or_blocked` - Racing trouble indicators\n",
    "- **Column 41**: `wide_run` - Whether dog raced wide\n",
    "\n",
    "### Statistical Features\n",
    "- **Column 42**: `performance_score` - Calculated performance metric\n",
    "- **Column 43**: `is_short_distance` - Binary indicator for sprint races\n",
    "- **Column 44**: `is_long_distance` - Binary indicator for distance races  \n",
    "- **Column 45**: `is_middle_distance` - Binary indicator for middle distance\n",
    "- **Column 46**: `track_type` - Numeric track type classification\n",
    "\n",
    "## Key Insights from the Data\n",
    "\n",
    "### Race Grades\n",
    "- **A grades**: Higher class races (A2, A3, A4, etc.)\n",
    "- **B grades**: Mid-level competition  \n",
    "- **D grades**: Lower class/maiden races\n",
    "- **HP/OR**: Handicap/Open races\n",
    "\n",
    "### Performance Indicators\n",
    "The comment field contains valuable racing information:\n",
    "- **\"ALed\"** = Always led\n",
    "- **\"QAw\"** = Quick away from traps\n",
    "- **\"Crd\"** = Crowded during race\n",
    "- **\"Bmp\"** = Bumped by other dogs\n",
    "- **\"RnOn\"** = Ran on strongly at finish (positive - dog accelerated/finished with strong pace in final stretch)\n",
    "- **\"SAw\"** = Slow away from traps\n",
    "- **\"Wide\"** = Raced wide around bends\n",
    "\n",
    "### Distance Categories\n",
    "- **Short**: 245m-285m (sprint races)\n",
    "- **Middle**: 400m-450m (standard distances)  \n",
    "- **Long**: 480m-500m+ (staying races)\n",
    "\n",
    "## Racing Commentary Explanation\n",
    "\n",
    "### \"Ran On\" (RnOn) - Detailed Meaning:\n",
    "In greyhound racing, **\"Ran On\"** is a **positive performance indicator** that means:\n",
    "\n",
    "1. **Strong Finish**: The dog accelerated or maintained strong pace in the final portion of the race\n",
    "2. **Late Speed**: Shows the dog has stamina and finishing kick\n",
    "3. **Closing Ground**: Often indicates the dog was gaining on leaders or maintaining position strongly\n",
    "4. **Good Fitness**: Suggests the dog is in good racing condition\n",
    "5. **Distance Suitability**: May indicate the dog suits longer distances where stamina matters\n",
    "\n",
    "**This is GOOD performance** - it shows the dog finished the race strongly rather than tiring. Dogs that \"run on\" are often considered to have good racing fitness and potential for improvement at longer distances.\n",
    "\n",
    "**Contrast with negative terms**:\n",
    "- \"Tired\" = Dog slowed significantly in final stretch\n",
    "- \"Faded\" = Dog lost position/pace late in race\n",
    "- \"Weakened\" = Dog showed lack of stamina\n",
    "\n",
    "This dataset appears designed for predictive modeling of greyhound race outcomes, with features capturing both historical performance and race-day factors that influence results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a89c6f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d210b70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME PROJECTION CALCULATOR\n",
    "# Based on actual performance: 21 dogs in 204 seconds\n",
    "\n",
    "def calculate_scraping_projections(dogs_scraped, time_taken, target_dogs=100000):\n",
    "    \"\"\"Calculate time projections for large-scale scraping\"\"\"\n",
    "    \n",
    "    seconds_per_dog = time_taken / dogs_scraped\n",
    "    \n",
    "    print(\"â±ï¸ SCRAPING TIME PROJECTIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Current Performance:\")\n",
    "    print(f\"  - Dogs scraped: {dogs_scraped}\")\n",
    "    print(f\"  - Time taken: {time_taken:.1f} seconds ({time_taken/60:.1f} minutes)\")\n",
    "    print(f\"  - Rate: {seconds_per_dog:.1f} seconds per dog\")\n",
    "    print(f\"  - Speed: {3600/seconds_per_dog:.1f} dogs per hour\")\n",
    "    \n",
    "    print(f\"\\nProjection for {target_dogs:,} dogs:\")\n",
    "    \n",
    "    total_seconds = target_dogs * seconds_per_dog\n",
    "    hours = total_seconds / 3600\n",
    "    days = hours / 24\n",
    "    \n",
    "    print(f\"  - Total time: {total_seconds:,.0f} seconds\")\n",
    "    print(f\"  - Hours: {hours:.1f} hours\")\n",
    "    print(f\"  - Days: {days:.1f} days\")\n",
    "    \n",
    "    if days > 7:\n",
    "        weeks = days / 7\n",
    "        print(f\"  - Weeks: {weeks:.1f} weeks\")\n",
    "    \n",
    "    # Show different scenarios\n",
    "    print(f\"\\nTime estimates for different scales:\")\n",
    "    scales = [1000, 10000, 50000, 100000, 500000]\n",
    "    \n",
    "    for scale in scales:\n",
    "        scale_seconds = scale * seconds_per_dog\n",
    "        scale_hours = scale_seconds / 3600\n",
    "        scale_days = scale_hours / 24\n",
    "        \n",
    "        if scale_days < 1:\n",
    "            print(f\"  - {scale:,} dogs: {scale_hours:.1f} hours\")\n",
    "        elif scale_days < 7:\n",
    "            print(f\"  - {scale:,} dogs: {scale_days:.1f} days\")\n",
    "        else:\n",
    "            scale_weeks = scale_days / 7\n",
    "            print(f\"  - {scale:,} dogs: {scale_days:.1f} days ({scale_weeks:.1f} weeks)\")\n",
    "    \n",
    "    # Optimization suggestions\n",
    "    print(f\"\\nðŸ’¡ OPTIMIZATION RECOMMENDATIONS:\")\n",
    "    \n",
    "    if seconds_per_dog > 5:\n",
    "        print(\"  âš ï¸ Current speed is quite slow (>5 sec/dog)\")\n",
    "        print(\"  Suggestions:\")\n",
    "        print(\"    - Use more HTTP requests, less Selenium\")\n",
    "        print(\"    - Implement parallel processing\")\n",
    "        print(\"    - Cache/skip already processed dogs\")\n",
    "        print(\"    - Use headless browser optimization\")\n",
    "        \n",
    "    target_speed = 2.0  # 2 seconds per dog target\n",
    "    if seconds_per_dog > target_speed:\n",
    "        improvement_factor = seconds_per_dog / target_speed\n",
    "        optimized_time = total_seconds / improvement_factor\n",
    "        optimized_days = optimized_time / 86400\n",
    "        \n",
    "        print(f\"  ðŸŽ¯ If optimized to {target_speed} sec/dog:\")\n",
    "        print(f\"    - {target_dogs:,} dogs would take: {optimized_days:.1f} days\")\n",
    "        print(f\"    - Speed improvement needed: {improvement_factor:.1f}x faster\")\n",
    "\n",
    "# Calculate based on your actual results\n",
    "calculate_scraping_projections(\n",
    "    dogs_scraped=21,\n",
    "    time_taken=204,\n",
    "    target_dogs=100000\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ðŸš¨ REALITY CHECK:\")\n",
    "print(\"11+ days of continuous scraping is not practical!\")\n",
    "print(\"\\nBetter approach:\")\n",
    "print(\"1. ðŸ“Š Focus on specific valuable ranges (recent dogs)\")\n",
    "print(\"2. âš¡ Optimize scraper to <2 seconds per dog\") \n",
    "print(\"3. ðŸ”„ Use parallel processing\")\n",
    "print(\"4. ðŸ’¾ Implement smart caching/resume capability\")\n",
    "print(\"5. ðŸŽ¯ Target ~10,000-50,000 most valuable dogs instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6332f216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Parallel Scraping Configuration:\n",
      "   Start ID: 651200\n",
      "   End ID: 651250\n",
      "   Total dogs: 50\n",
      "   Output: dogs3.csv (FORCED)\n",
      "   Max workers: Auto-detect\n",
      "\n",
      "ðŸš€ PARALLEL SCRAPING - 50 dogs with 8 workers\n",
      "ðŸ“‚ Output file: dogs3.csv (FIXED)\n",
      "ðŸ”§ System: 12 CPU cores\n",
      "============================================================\n",
      "ðŸš€ Parallel Scraper initialized with 8 workers\n",
      "ðŸ“¦ Split into 9 batches of ~6 dogs each\n",
      "  ðŸ”„ Worker 0: Processing 6 dogs\n",
      "  ðŸ”„ Worker 1: Processing 6 dogs\n",
      "  ðŸ”„ Worker 2: Processing 6 dogs\n",
      "  ðŸ”„ Worker 3: Processing 6 dogs\n",
      "  ðŸ”„ Worker 4: Processing 6 dogs\n",
      "  ðŸ”„ Worker 5: Processing 6 dogs\n",
      "  ðŸ”„ Worker 6: Processing 6 dogs\n",
      "  ðŸ”„ Worker 7: Processing 6 dogs\n",
      "Collecting race URLs from all dogs...\n",
      "Processing dog 1/6: 651242\n",
      "Collecting race URLs from all dogs...\n",
      "Processing dog 1/6: 651206\n",
      "Collecting race URLs from all dogs...\n",
      "Processing dog 1/6: 651212\n",
      "Collecting race URLs from all dogs...\n",
      "Processing dog 1/6: 651200\n",
      "Collecting race URLs from all dogs...\n",
      "Processing dog 1/6: 651236\n",
      "Collecting race URLs from all dogs...\n",
      "Processing dog 1/6: 651218\n",
      "Collecting race URLs from all dogs...\n",
      "Processing dog 1/6: 651224\n",
      "Collecting race URLs from all dogs...\n",
      "Processing dog 1/6: 651230\n",
      "Collecting race URLs from all dogs...\n",
      "Processing dog 1/6: 651242\n",
      "Collecting race URLs from all dogs...\n",
      "Processing dog 1/6: 651206\n",
      "Collecting race URLs from all dogs...\n",
      "Processing dog 1/6: 651212\n",
      "Collecting race URLs from all dogs...\n",
      "Processing dog 1/6: 651200\n",
      "Collecting race URLs from all dogs...\n",
      "Processing dog 1/6: 651236\n",
      "Collecting race URLs from all dogs...\n",
      "Processing dog 1/6: 651218\n",
      "Collecting race URLs from all dogs...\n",
      "Processing dog 1/6: 651224\n",
      "Collecting race URLs from all dogs...\n",
      "Processing dog 1/6: 651230\n",
      "Processing dog 2/6: 651213Processing dog 2/6: 651201\n",
      "\n",
      "Processing dog 2/6: 651207\n",
      "Processing dog 2/6: 651213Processing dog 2/6: 651201\n",
      "\n",
      "Processing dog 2/6: 651207\n",
      "Processing dog 2/6: 651237\n",
      "Processing dog 2/6: 651243\n",
      "Processing dog 2/6: 651219\n",
      "Processing dog 2/6: 651225\n",
      "Processing dog 2/6: 651237\n",
      "Processing dog 2/6: 651243\n",
      "Processing dog 2/6: 651219\n",
      "Processing dog 2/6: 651225\n",
      "Processing dog 2/6: 651231\n",
      "Processing dog 2/6: 651231\n",
      "Processing dog 3/6: 651208\n",
      "Processing dog 3/6: 651214\n",
      "Processing dog 3/6: 651202\n",
      "Processing dog 3/6: 651208\n",
      "Processing dog 3/6: 651214\n",
      "Processing dog 3/6: 651202\n",
      "Processing dog 3/6: 651238\n",
      "Processing dog 3/6: 651238\n",
      "Processing dog 3/6: 651244\n",
      "Processing dog 3/6: 651220\n",
      "Processing dog 3/6: 651226\n",
      "Processing dog 3/6: 651244\n",
      "Processing dog 3/6: 651220\n",
      "Processing dog 3/6: 651226\n",
      "Processing dog 3/6: 651232\n",
      "Processing dog 3/6: 651232\n",
      "Processing dog 4/6: 651215\n",
      "Processing dog 4/6: 651209\n",
      "Processing dog 4/6: 651203\n",
      "Processing dog 4/6: 651215\n",
      "Processing dog 4/6: 651209\n",
      "Processing dog 4/6: 651203\n",
      "Processing dog 4/6: 651239\n",
      "Processing dog 4/6: 651239\n",
      "Processing dog 4/6: 651245\n",
      "Processing dog 4/6: 651227\n",
      "Processing dog 4/6: 651245\n",
      "Processing dog 4/6: 651227\n",
      "Processing dog 4/6: 651221\n",
      "Processing dog 4/6: 651233\n",
      "Processing dog 4/6: 651221\n",
      "Processing dog 4/6: 651233\n",
      "Processing dog 5/6: 651210Processing dog 5/6: 651204\n",
      "\n",
      "Processing dog 5/6: 651216\n",
      "Processing dog 5/6: 651240\n",
      "Processing dog 5/6: 651210Processing dog 5/6: 651204\n",
      "\n",
      "Processing dog 5/6: 651216\n",
      "Processing dog 5/6: 651240\n",
      "Processing dog 5/6: 651246\n",
      "Processing dog 5/6: 651228\n",
      "Processing dog 5/6: 651246\n",
      "Processing dog 5/6: 651228\n",
      "Processing dog 5/6: 651222\n",
      "Processing dog 5/6: 651222\n",
      "Processing dog 5/6: 651234\n",
      "Processing dog 5/6: 651234\n",
      "Processing dog 6/6: 651205\n",
      "Processing dog 6/6: 651241\n",
      "Processing dog 6/6: 651211\n",
      "Processing dog 6/6: 651217\n",
      "Processing dog 6/6: 651205\n",
      "Processing dog 6/6: 651241\n",
      "Processing dog 6/6: 651211\n",
      "Processing dog 6/6: 651217\n",
      "Processing dog 6/6: 651247\n",
      "Processing dog 6/6: 651247\n",
      "Processing dog 6/6: 651229\n",
      "Processing dog 6/6: 651229\n",
      "Processing dog 6/6: 651223\n",
      "Processing dog 6/6: 651223\n",
      "Processing dog 6/6: 651235\n",
      "Processing dog 6/6: 651235\n",
      "Found 1 unique races to process\n",
      "Need to scrape 1 new races\n",
      "Found 2 unique races to process\n",
      "Need to scrape 2 new races\n",
      "Found 0 unique races to process\n",
      "Need to scrape 0 new races\n",
      "Completed! Total records: 0\n",
      "Found 6 unique races to process\n",
      "Need to scrape 6 new races\n",
      "Found 1 unique races to process\n",
      "Need to scrape 1 new races\n",
      "Found 2 unique races to process\n",
      "Need to scrape 2 new races\n",
      "Found 0 unique races to process\n",
      "Need to scrape 0 new races\n",
      "Completed! Total records: 0\n",
      "Found 6 unique races to process\n",
      "Need to scrape 6 new races\n",
      "Found 4 unique races to process\n",
      "Need to scrape 4 new races\n",
      "Completed! Total records: 6\n",
      "Found 4 unique races to process\n",
      "Need to scrape 4 new races\n",
      "Completed! Total records: 6\n",
      "Found 6 unique races to process\n",
      "Need to scrape 6 new races\n",
      "Found 6 unique races to process\n",
      "Need to scrape 6 new races\n",
      "Found 2 unique races to process\n",
      "Need to scrape 2 new races\n",
      "Found 2 unique races to process\n",
      "Need to scrape 2 new races\n",
      "Found 3 unique races to process\n",
      "Need to scrape 3 new races\n",
      "Found 3 unique races to process\n",
      "Need to scrape 3 new races\n",
      "  âœ… Worker 1: 0 records in 36.3s\n",
      "  ðŸ”„ Worker 8: Processing 2 dogs\n",
      "ðŸ“Š Progress: 1/9 batches (11.1%)\n",
      "  âœ… Worker 1: 0 records in 36.3s\n",
      "  ðŸ”„ Worker 8: Processing 2 dogs\n",
      "ðŸ“Š Progress: 1/9 batches (11.1%)\n",
      "Completed! Total records: 12\n",
      "Completed! Total records: 12\n",
      "  âœ… Worker 0: 6 records in 37.4s\n",
      "ðŸ“Š Progress: 2/9 batches (22.2%)\n",
      "Collecting race URLs from all dogs...\n",
      "Processing dog 1/2: 651248\n",
      "  âœ… Worker 0: 6 records in 37.4s\n",
      "ðŸ“Š Progress: 2/9 batches (22.2%)\n",
      "Collecting race URLs from all dogs...\n",
      "Processing dog 1/2: 651248\n",
      "  âœ… Worker 2: 12 records in 38.7s\n",
      "ðŸ“Š Progress: 3/9 batches (33.3%)\n",
      "Completed! Total records: 9\n",
      "  âœ… Worker 2: 12 records in 38.7s\n",
      "ðŸ“Š Progress: 3/9 batches (33.3%)\n",
      "Completed! Total records: 9\n",
      "  âœ… Worker 3: 9 records in 40.9s\n",
      "ðŸ“Š Progress: 4/9 batches (44.4%)\n",
      "  âœ… Worker 3: 9 records in 40.9s\n",
      "ðŸ“Š Progress: 4/9 batches (44.4%)\n",
      "Completed! Total records: 15\n",
      "Completed! Total records: 15\n",
      "Completed! Total records: 35\n",
      "Completed! Total records: 35\n",
      "Completed! Total records: 23\n",
      "Completed! Total records: 23\n",
      "  âœ… Worker 5: 15 records in 44.0s\n",
      "ðŸ“Š Progress: 5/9 batches (55.6%)\n",
      "  âœ… Worker 5: 15 records in 44.0s\n",
      "ðŸ“Š Progress: 5/9 batches (55.6%)\n",
      "  âœ… Worker 6: 35 records in 44.6s\n",
      "ðŸ“Š Progress: 6/9 batches (66.7%)\n",
      "  âœ… Worker 6: 35 records in 44.6s\n",
      "ðŸ“Š Progress: 6/9 batches (66.7%)\n",
      "Processing dog 2/2: 651249\n",
      "  âœ… Worker 7: 23 records in 44.9s\n",
      "ðŸ“Š Progress: 7/9 batches (77.8%)\n",
      "Processing dog 2/2: 651249\n",
      "  âœ… Worker 7: 23 records in 44.9s\n",
      "ðŸ“Š Progress: 7/9 batches (77.8%)\n",
      "Completed! Total records: 36\n",
      "Completed! Total records: 36\n",
      "  âœ… Worker 4: 36 records in 47.3s\n",
      "ðŸ“Š Progress: 8/9 batches (88.9%)\n",
      "  âœ… Worker 4: 36 records in 47.3s\n",
      "ðŸ“Š Progress: 8/9 batches (88.9%)\n",
      "Found 9 unique races to process\n",
      "Need to scrape 9 new races\n",
      "Found 9 unique races to process\n",
      "Need to scrape 9 new races\n",
      "Completed! Total records: 51\n",
      "Completed! Total records: 51\n",
      "  âœ… Worker 8: 51 records in 28.0s\n",
      "ðŸ“Š Progress: 9/9 batches (100.0%)\n",
      "ðŸ”— Merging batch files...\n",
      "  ðŸ“‚ Found existing data: 11124 records\n",
      "  ðŸ“„ Batch 0: 6 records\n",
      "  ðŸ“„ Batch 2: 12 records\n",
      "  ðŸ“„ Batch 3: 9 records\n",
      "  ðŸ“„ Batch 5: 15 records\n",
      "  ðŸ“„ Batch 6: 35 records\n",
      "  ðŸ“„ Batch 7: 23 records\n",
      "  ðŸ“„ Batch 4: 36 records\n",
      "  ðŸ“„ Batch 8: 51 records\n",
      "  ðŸ”— Combined 11124 existing + 187 new = 11311 total records\n",
      "  âœ… Merged 8 batch files into dogs3.csv\n",
      "============================================================\n",
      "âœ… PARALLEL SCRAPING COMPLETED!\n",
      "ðŸ“Š Total records in dogs3.csv: 11311\n",
      "â±ï¸ Total time: 64.4 seconds (1.1 minutes)\n",
      "ðŸ• Dogs processed: 50\n",
      "ðŸ”§ Workers used: 8\n",
      "ðŸ“ˆ Speed: 46.6 dogs per minute\n",
      "ðŸ’¾ Data saved to: dogs3.csv\n",
      "\n",
      "ðŸ“ˆ PERFORMANCE BREAKDOWN:\n",
      "  - Successful batches: 8/9\n",
      "  - Failed batches: 0\n",
      "  - Average time per batch: 40.7s\n",
      "  - Records per second: 175.6\n",
      "ðŸ†• New records added this run: 187\n",
      "ðŸ“Š Final dataset analysis:\n",
      "  - Total records: 11311\n",
      "  - Unique dogs: 4475\n",
      "  - Average records per dog: 2.5\n",
      "  - Latest dog IDs: [np.float64(650951.0), np.float64(649177.0), np.float64(648901.0), np.float64(648644.0), np.float64(648636.0)]\n",
      "\n",
      "ðŸŽ¯ FINAL RESULT: Data appended to dogs3.csv\n",
      "ðŸ“ dogs3.csv file size: 3888.5 KB\n",
      "ðŸ“Š dogs3.csv contains 11311 total records\n",
      "ðŸ• Unique dogs in dataset: 4475\n",
      "\n",
      "ðŸ’¡ IMPORTANT:\n",
      "âœ… All data is now saved to dogs3.csv in the same format\n",
      "âœ… New records are appended to existing data\n",
      "âœ… No separate parallel output files are created\n",
      "  âœ… Worker 8: 51 records in 28.0s\n",
      "ðŸ“Š Progress: 9/9 batches (100.0%)\n",
      "ðŸ”— Merging batch files...\n",
      "  ðŸ“‚ Found existing data: 11124 records\n",
      "  ðŸ“„ Batch 0: 6 records\n",
      "  ðŸ“„ Batch 2: 12 records\n",
      "  ðŸ“„ Batch 3: 9 records\n",
      "  ðŸ“„ Batch 5: 15 records\n",
      "  ðŸ“„ Batch 6: 35 records\n",
      "  ðŸ“„ Batch 7: 23 records\n",
      "  ðŸ“„ Batch 4: 36 records\n",
      "  ðŸ“„ Batch 8: 51 records\n",
      "  ðŸ”— Combined 11124 existing + 187 new = 11311 total records\n",
      "  âœ… Merged 8 batch files into dogs3.csv\n",
      "============================================================\n",
      "âœ… PARALLEL SCRAPING COMPLETED!\n",
      "ðŸ“Š Total records in dogs3.csv: 11311\n",
      "â±ï¸ Total time: 64.4 seconds (1.1 minutes)\n",
      "ðŸ• Dogs processed: 50\n",
      "ðŸ”§ Workers used: 8\n",
      "ðŸ“ˆ Speed: 46.6 dogs per minute\n",
      "ðŸ’¾ Data saved to: dogs3.csv\n",
      "\n",
      "ðŸ“ˆ PERFORMANCE BREAKDOWN:\n",
      "  - Successful batches: 8/9\n",
      "  - Failed batches: 0\n",
      "  - Average time per batch: 40.7s\n",
      "  - Records per second: 175.6\n",
      "ðŸ†• New records added this run: 187\n",
      "ðŸ“Š Final dataset analysis:\n",
      "  - Total records: 11311\n",
      "  - Unique dogs: 4475\n",
      "  - Average records per dog: 2.5\n",
      "  - Latest dog IDs: [np.float64(650951.0), np.float64(649177.0), np.float64(648901.0), np.float64(648644.0), np.float64(648636.0)]\n",
      "\n",
      "ðŸŽ¯ FINAL RESULT: Data appended to dogs3.csv\n",
      "ðŸ“ dogs3.csv file size: 3888.5 KB\n",
      "ðŸ“Š dogs3.csv contains 11311 total records\n",
      "ðŸ• Unique dogs in dataset: 4475\n",
      "\n",
      "ðŸ’¡ IMPORTANT:\n",
      "âœ… All data is now saved to dogs3.csv in the same format\n",
      "âœ… New records are appended to existing data\n",
      "âœ… No separate parallel output files are created\n"
     ]
    }
   ],
   "source": [
    "# PARALLEL FAST SCRAPING NOTEBOOK - Multi-threaded Solution\n",
    "# Import and use the optimized fast scraping function with parallel processing\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "import queue\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "\n",
    "# Add current directory to path to import our module\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Import the fast scraping function\n",
    "from fast_scraping import fast_scrape_multiple_dogs\n",
    "\n",
    "class ParallelScraper:\n",
    "    \"\"\"Parallel scraper class to handle multi-threaded dog scraping\"\"\"\n",
    "    \n",
    "    def __init__(self, max_workers=None):\n",
    "        if max_workers is None:\n",
    "            # Use 4x CPU cores but cap at 8 to avoid overwhelming the server\n",
    "            max_workers = min(8, multiprocessing.cpu_count() * 4)\n",
    "        \n",
    "        self.max_workers = max_workers\n",
    "        self.results_queue = queue.Queue()\n",
    "        self.lock = threading.Lock()\n",
    "        self.total_records = 0\n",
    "        \n",
    "        print(f\"ðŸš€ Parallel Scraper initialized with {max_workers} workers\")\n",
    "    \n",
    "    def scrape_dog_batch(self, dog_batch, batch_id, output_file_base):\n",
    "        \"\"\"Scrape a batch of dogs in parallel\"\"\"\n",
    "        try:\n",
    "            batch_output_file = f\"{output_file_base}_batch_{batch_id}.csv\"\n",
    "            \n",
    "            print(f\"  ðŸ”„ Worker {batch_id}: Processing {len(dog_batch)} dogs\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Use the existing fast scraping function for this batch\n",
    "            batch_records = fast_scrape_multiple_dogs(\n",
    "                dog_ids=dog_batch,\n",
    "                output_file=batch_output_file,\n",
    "                batch_size=10  # Smaller batch size for frequent saves\n",
    "            )\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            \n",
    "            with self.lock:\n",
    "                self.total_records += batch_records\n",
    "            \n",
    "            print(f\"  âœ… Worker {batch_id}: {batch_records} records in {elapsed_time:.1f}s\")\n",
    "            \n",
    "            return {\n",
    "                'batch_id': batch_id,\n",
    "                'records': batch_records,\n",
    "                'output_file': batch_output_file,\n",
    "                'time': elapsed_time,\n",
    "                'dogs_processed': len(dog_batch)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Worker {batch_id} failed: {str(e)}\")\n",
    "            return {\n",
    "                'batch_id': batch_id,\n",
    "                'records': 0,\n",
    "                'output_file': None,\n",
    "                'time': 0,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def merge_batch_files(self, batch_results, final_output_file):\n",
    "        \"\"\"Merge all batch CSV files into one final file\"\"\"\n",
    "        print(\"ðŸ”— Merging batch files...\")\n",
    "        \n",
    "        import pandas as pd\n",
    "        all_dataframes = []\n",
    "        \n",
    "        # Check if final output file already exists and load existing data\n",
    "        existing_data = None\n",
    "        if os.path.exists(final_output_file):\n",
    "            try:\n",
    "                existing_data = pd.read_csv(final_output_file)\n",
    "                print(f\"  ðŸ“‚ Found existing data: {len(existing_data)} records\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âš ï¸ Could not read existing file: {e}\")\n",
    "        \n",
    "        # Process batch files\n",
    "        for result in batch_results:\n",
    "            if result['output_file'] and os.path.exists(result['output_file']):\n",
    "                try:\n",
    "                    df = pd.read_csv(result['output_file'])\n",
    "                    all_dataframes.append(df)\n",
    "                    print(f\"  ðŸ“„ Batch {result['batch_id']}: {len(df)} records\")\n",
    "                    # Clean up batch file\n",
    "                    os.remove(result['output_file'])\n",
    "                except Exception as e:\n",
    "                    print(f\"  âš ï¸ Error reading batch file {result['output_file']}: {e}\")\n",
    "        \n",
    "        if all_dataframes:\n",
    "            # Combine all new data\n",
    "            new_data = pd.concat(all_dataframes, ignore_index=True)\n",
    "            \n",
    "            # Combine with existing data if present\n",
    "            if existing_data is not None:\n",
    "                final_df = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "                print(f\"  ðŸ”— Combined {len(existing_data)} existing + {len(new_data)} new = {len(final_df)} total records\")\n",
    "            else:\n",
    "                final_df = new_data\n",
    "                print(f\"  ðŸ“Š New dataset with {len(final_df)} records\")\n",
    "            \n",
    "            # Save to final output file\n",
    "            final_df.to_csv(final_output_file, index=False)\n",
    "            print(f\"  âœ… Merged {len(all_dataframes)} batch files into {final_output_file}\")\n",
    "            return len(final_df)\n",
    "        else:\n",
    "            print(\"  âŒ No valid batch files to merge\")\n",
    "            return 0\n",
    "\n",
    "def run_parallel_scraping(start_id=650947, end_id=650967, output_file=\"dogs3.csv\", max_workers=None):\n",
    "    \"\"\"\n",
    "    Run parallel scraping for a range of dog IDs\n",
    "    \n",
    "    Args:\n",
    "        start_id: Starting dog ID\n",
    "        end_id: Ending dog ID (exclusive)\n",
    "        output_file: Output CSV file name (MUST be dogs3.csv)\n",
    "        max_workers: Number of parallel workers (None = auto-detect)\n",
    "    \"\"\"\n",
    "    \n",
    "    # FORCE output to be dogs3.csv to match existing data\n",
    "    output_file = \"dogs3.csv\"\n",
    "    \n",
    "    # Generate dog ID list\n",
    "    dog_ids = [str(i) for i in range(start_id, end_id)]\n",
    "    \n",
    "    if max_workers is None:\n",
    "        max_workers = min(8, multiprocessing.cpu_count() * 2)\n",
    "    \n",
    "    print(f\"ðŸš€ PARALLEL SCRAPING - {len(dog_ids)} dogs with {max_workers} workers\")\n",
    "    print(f\"ðŸ“‚ Output file: {output_file} (FIXED)\")\n",
    "    print(f\"ðŸ”§ System: {multiprocessing.cpu_count()} CPU cores\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Initialize parallel scraper\n",
    "        scraper = ParallelScraper(max_workers=max_workers)\n",
    "        \n",
    "        # Split dogs into batches for workers\n",
    "        batch_size = max(1, len(dog_ids) // max_workers)\n",
    "        dog_batches = [dog_ids[i:i + batch_size] for i in range(0, len(dog_ids), batch_size)]\n",
    "        \n",
    "        print(f\"ðŸ“¦ Split into {len(dog_batches)} batches of ~{batch_size} dogs each\")\n",
    "        \n",
    "        # Process batches in parallel\n",
    "        batch_results = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit all batch jobs\n",
    "            future_to_batch = {\n",
    "                executor.submit(scraper.scrape_dog_batch, batch, i, \"temp_batch\"): i\n",
    "                for i, batch in enumerate(dog_batches)\n",
    "            }\n",
    "            \n",
    "            # Collect results as they complete\n",
    "            for future in as_completed(future_to_batch):\n",
    "                batch_id = future_to_batch[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    batch_results.append(result)\n",
    "                    \n",
    "                    progress = len(batch_results) / len(dog_batches) * 100\n",
    "                    print(f\"ðŸ“Š Progress: {len(batch_results)}/{len(dog_batches)} batches ({progress:.1f}%)\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ Batch {batch_id} generated an exception: {e}\")\n",
    "                    batch_results.append({\n",
    "                        'batch_id': batch_id,\n",
    "                        'records': 0,\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "        \n",
    "        # Merge all batch files into dogs3.csv\n",
    "        total_records = scraper.merge_batch_files(batch_results, output_file)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"âœ… PARALLEL SCRAPING COMPLETED!\")\n",
    "        print(f\"ðŸ“Š Total records in {output_file}: {total_records}\")\n",
    "        print(f\"â±ï¸ Total time: {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} minutes)\")\n",
    "        print(f\"ðŸ• Dogs processed: {len(dog_ids)}\")\n",
    "        print(f\"ðŸ”§ Workers used: {max_workers}\")\n",
    "        print(f\"ðŸ“ˆ Speed: {len(dog_ids)/elapsed_time*60:.1f} dogs per minute\")\n",
    "        print(f\"ðŸ’¾ Data saved to: {output_file}\")\n",
    "        \n",
    "        # Performance analysis\n",
    "        successful_batches = [r for r in batch_results if r.get('records', 0) > 0]\n",
    "        failed_batches = [r for r in batch_results if r.get('error')]\n",
    "        \n",
    "        print(f\"\\nðŸ“ˆ PERFORMANCE BREAKDOWN:\")\n",
    "        print(f\"  - Successful batches: {len(successful_batches)}/{len(batch_results)}\")\n",
    "        print(f\"  - Failed batches: {len(failed_batches)}\")\n",
    "        if successful_batches:\n",
    "            print(f\"  - Average time per batch: {sum(r.get('time', 0) for r in successful_batches)/len(successful_batches):.1f}s\")\n",
    "        print(f\"  - Records per second: {total_records/elapsed_time:.1f}\")\n",
    "        \n",
    "        # Data validation\n",
    "        if total_records > 0:\n",
    "            new_records_added = sum(r.get('records', 0) for r in successful_batches)\n",
    "            print(f\"ðŸ†• New records added this run: {new_records_added}\")\n",
    "            \n",
    "            # Analyze the final dataset\n",
    "            if os.path.exists(output_file):\n",
    "                try:\n",
    "                    import pandas as pd\n",
    "                    df = pd.read_csv(output_file)\n",
    "                    \n",
    "                    if 'dog_id' in df.columns:\n",
    "                        unique_dogs = df['dog_id'].nunique()\n",
    "                        print(f\"ðŸ“Š Final dataset analysis:\")\n",
    "                        print(f\"  - Total records: {len(df)}\")\n",
    "                        print(f\"  - Unique dogs: {unique_dogs}\")\n",
    "                        print(f\"  - Average records per dog: {len(df)/unique_dogs:.1f}\")\n",
    "                        \n",
    "                        # Show sample of latest dogs\n",
    "                        latest_dogs = sorted(df['dog_id'].unique(), reverse=True)[:5]\n",
    "                        print(f\"  - Latest dog IDs: {latest_dogs}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  âŒ Error analyzing final dataset: {e}\")\n",
    "        \n",
    "        return total_records\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during parallel scraping: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "# CONFIGURATION - Edit these values as needed\n",
    "START_DOG_ID = 600000  # Starting dog ID  \n",
    "END_DOG_ID = 650000    # Ending dog ID (10 dogs for testing)\n",
    "OUTPUT_FILE = \"dogs3.csv\"  # MUST be dogs3.csv to match existing data\n",
    "MAX_WORKERS = None  # None = auto-detect optimal workers\n",
    "\n",
    "# Choose your test mode\n",
    "TEST_MODE = \"parallel\"  # Options: \"parallel\", \"comparison\", \"scale_test\"\n",
    "\n",
    "if TEST_MODE == \"parallel\":\n",
    "    print(\"ðŸ”§ Parallel Scraping Configuration:\")\n",
    "    print(f\"   Start ID: {START_DOG_ID}\")\n",
    "    print(f\"   End ID: {END_DOG_ID}\")\n",
    "    print(f\"   Total dogs: {END_DOG_ID - START_DOG_ID}\")\n",
    "    print(f\"   Output: {OUTPUT_FILE} (FORCED)\")\n",
    "    print(f\"   Max workers: {MAX_WORKERS or 'Auto-detect'}\")\n",
    "    print()\n",
    "\n",
    "    # Execute parallel scraping - output_file parameter ignored, always uses dogs3.csv\n",
    "    total_scraped = run_parallel_scraping(\n",
    "        start_id=START_DOG_ID,\n",
    "        end_id=END_DOG_ID,\n",
    "        output_file=OUTPUT_FILE,  # This will be forced to dogs3.csv\n",
    "        max_workers=MAX_WORKERS\n",
    "    )\n",
    "\n",
    "    # Final summary\n",
    "    print(f\"\\nðŸŽ¯ FINAL RESULT: Data appended to dogs3.csv\")\n",
    "    if os.path.exists(\"dogs3.csv\"):\n",
    "        file_size = os.path.getsize(\"dogs3.csv\") / 1024  # KB\n",
    "        print(f\"ðŸ“ dogs3.csv file size: {file_size:.1f} KB\")\n",
    "        \n",
    "        # Show file info\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            df = pd.read_csv(\"dogs3.csv\")\n",
    "            print(f\"ðŸ“Š dogs3.csv contains {len(df)} total records\")\n",
    "            if 'dog_id' in df.columns:\n",
    "                unique_dogs = df['dog_id'].nunique()\n",
    "                print(f\"ðŸ• Unique dogs in dataset: {unique_dogs}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error reading dogs3.csv: {e}\")\n",
    "\n",
    "# ...existing test modes...\n",
    "\n",
    "print(f\"\\nðŸ’¡ IMPORTANT:\")\n",
    "print(\"âœ… All data is now saved to dogs3.csv in the same format\")\n",
    "print(\"âœ… New records are appended to existing data\")\n",
    "print(\"âœ… No separate parallel output files are created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b5411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATED TIME PROJECTION CALCULATOR - PARALLEL PERFORMANCE\n",
    "# Based on actual parallel performance: 50 dogs in 64 seconds\n",
    "\n",
    "def calculate_parallel_scraping_projections(dogs_scraped, time_taken, target_dogs=20000):\n",
    "    \"\"\"Calculate time projections for parallel scraping performance\"\"\"\n",
    "    \n",
    "    seconds_per_dog = time_taken / dogs_scraped\n",
    "    \n",
    "    print(\"âš¡ PARALLEL SCRAPING TIME PROJECTIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ðŸš€ CURRENT PARALLEL PERFORMANCE:\")\n",
    "    print(f\"  - Dogs scraped: {dogs_scraped}\")\n",
    "    print(f\"  - Time taken: {time_taken:.1f} seconds ({time_taken/60:.1f} minutes)\")\n",
    "    print(f\"  - Rate: {seconds_per_dog:.2f} seconds per dog\")\n",
    "    print(f\"  - Speed: {3600/seconds_per_dog:.1f} dogs per hour\")\n",
    "    print(f\"  - Speed: {dogs_scraped/(time_taken/60):.1f} dogs per minute\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ PROJECTION FOR {target_dogs:,} DOGS:\")\n",
    "    \n",
    "    total_seconds = target_dogs * seconds_per_dog\n",
    "    hours = total_seconds / 3600\n",
    "    \n",
    "    print(f\"  - Total time: {total_seconds:,.0f} seconds\")\n",
    "    print(f\"  - Hours: {hours:.1f} hours\")\n",
    "    \n",
    "    if hours < 24:\n",
    "        print(f\"  - Time: {hours:.1f} hours ({hours*60:.0f} minutes)\")\n",
    "    else:\n",
    "        days = hours / 24\n",
    "        print(f\"  - Days: {days:.1f} days\")\n",
    "        print(f\"  - Work sessions: {hours/8:.1f} Ã— 8-hour sessions\")\n",
    "    \n",
    "    # Show different scenarios\n",
    "    print(f\"\\nðŸ“Š TIME ESTIMATES FOR DIFFERENT SCALES:\")\n",
    "    scales = [1000, 5000, 10000, 20000, 50000, 100000]\n",
    "    \n",
    "    for scale in scales:\n",
    "        scale_seconds = scale * seconds_per_dog\n",
    "        scale_hours = scale_seconds / 3600\n",
    "        \n",
    "        if scale_hours < 1:\n",
    "            scale_minutes = scale_seconds / 60\n",
    "            print(f\"  - {scale:,} dogs: {scale_minutes:.1f} minutes\")\n",
    "        elif scale_hours < 24:\n",
    "            print(f\"  - {scale:,} dogs: {scale_hours:.1f} hours\")\n",
    "        else:\n",
    "            scale_days = scale_hours / 24\n",
    "            print(f\"  - {scale:,} dogs: {scale_days:.1f} days ({scale_hours:.1f} hours)\")\n",
    "    \n",
    "    # Performance comparison with old system\n",
    "    old_rate = 9.7  # seconds per dog from original system\n",
    "    improvement_factor = old_rate / seconds_per_dog\n",
    "    old_time_for_target = target_dogs * old_rate / 3600\n",
    "    \n",
    "    print(f\"\\nðŸ† PERFORMANCE IMPROVEMENT ANALYSIS:\")\n",
    "    print(f\"  - Old system: {old_rate:.1f} seconds per dog\")\n",
    "    print(f\"  - New parallel: {seconds_per_dog:.2f} seconds per dog\")\n",
    "    print(f\"  - Speed improvement: {improvement_factor:.1f}x faster!\")\n",
    "    print(f\"  - Time saved for {target_dogs:,} dogs:\")\n",
    "    print(f\"    â€¢ Old system: {old_time_for_target:.1f} hours ({old_time_for_target/24:.1f} days)\")\n",
    "    print(f\"    â€¢ New parallel: {hours:.1f} hours\")\n",
    "    print(f\"    â€¢ Time saved: {old_time_for_target - hours:.1f} hours\")\n",
    "    \n",
    "    # Realistic recommendations\n",
    "    print(f\"\\nðŸ’¡ REALISTIC SCRAPING STRATEGY:\")\n",
    "    \n",
    "    # Calculate optimal batch sizes\n",
    "    batch_8_hours = int(8 * 3600 / seconds_per_dog)\n",
    "    batch_4_hours = int(4 * 3600 / seconds_per_dog)\n",
    "    batch_1_hour = int(1 * 3600 / seconds_per_dog)\n",
    "    \n",
    "    print(f\"  ðŸ• What you can scrape in different time windows:\")\n",
    "    print(f\"    â€¢ 1 hour session: ~{batch_1_hour:,} dogs\")\n",
    "    print(f\"    â€¢ 4 hour session: ~{batch_4_hours:,} dogs\") \n",
    "    print(f\"    â€¢ 8 hour session: ~{batch_8_hours:,} dogs\")\n",
    "    \n",
    "    sessions_needed = target_dogs / batch_8_hours\n",
    "    print(f\"\\n  ðŸŽ¯ For {target_dogs:,} dogs:\")\n",
    "    print(f\"    â€¢ {sessions_needed:.1f} Ã— 8-hour sessions\")\n",
    "    print(f\"    â€¢ Or {target_dogs/batch_4_hours:.1f} Ã— 4-hour sessions\")\n",
    "    print(f\"    â€¢ Spread over {sessions_needed:.0f}-{sessions_needed*2:.0f} days\")\n",
    "    \n",
    "    # System recommendations\n",
    "    print(f\"\\nâš™ï¸ SYSTEM OPTIMIZATION TIPS:\")\n",
    "    print(f\"  âœ… Current parallel setup is EXCELLENT!\")\n",
    "    print(f\"  âœ… {improvement_factor:.1f}x improvement achieved\")\n",
    "    print(f\"  ðŸ’¡ Consider these optimizations:\")\n",
    "    print(f\"    â€¢ Save progress every 1000 dogs\")\n",
    "    print(f\"    â€¢ Monitor for rate limiting\")\n",
    "    print(f\"    â€¢ Run during off-peak hours\")\n",
    "    print(f\"    â€¢ Use resume capability for long runs\")\n",
    "\n",
    "# Calculate based on your NEW parallel results\n",
    "calculate_parallel_scraping_projections(\n",
    "    dogs_scraped=50,\n",
    "    time_taken=64,\n",
    "    target_dogs=20000\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ EXCELLENT PROGRESS!\")\n",
    "print(\"Your parallel optimization worked brilliantly!\")\n",
    "print(f\"20,000 dogs is now achievable in ~7 hours instead of 54+ hours!\")\n",
    "print(\"\\nðŸš€ RECOMMENDED APPROACH:\")\n",
    "print(\"1. ðŸ“Š Run in 4-hour chunks (~5,600 dogs each)\")\n",
    "print(\"2. ðŸ”„ Take breaks between sessions\") \n",
    "print(\"3. ðŸ’¾ Monitor progress and file sizes\")\n",
    "print(\"4. ðŸŽ¯ Complete 20K dogs in 3-4 work sessions\")\n",
    "print(\"5. âš¡ Your 7.6x speedup makes this very practical!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
