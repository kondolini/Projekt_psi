{
 "cells": [
  {
<<<<<<< HEAD:fast_scraping_notebook.ipynb
   "cell_type": "markdown",
   "id": "0021a578",
   "metadata": {},
   "source": [
    "# GBGB Dog Racing API Scraper\n",
    "\n",
    "This notebook uses the GBGB API to efficiently collect dog racing data.\n",
    "\n",
    "Benefits of using the API instead of web scraping:\n",
    "- Much faster data collection (10-100x faster)\n",
    "- More reliable - no HTML parsing required\n",
    "- Less resource-intensive - no browser needed\n",
    "- Complete data directly from the source\n",
    "- Less likely to be blocked\n",
    "\n",
    "**Smart Append Mode**: Only scrapes dogs that haven't been scraped yet, preserving existing data.\n",
    "**Batch Processing**: Saves data every 1000 dogs to prevent data loss.\n",
    "**Data Location**: Saves to `../data/dogs5.csv` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3921fb",
=======
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d6e71d7",
>>>>>>> 693537a0d7ed315ff49e848e7adfe5ef2e6d38e1:scraping/fast_scraping_notebook.ipynb
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD:fast_scraping_notebook.ipynb
      "🎯 CONFIGURATION:\n",
      "  - Start ID: 500000\n",
      "  - End ID: 500100\n",
      "  - Total dogs to process: 101\n",
      "  - Output file: dogs5.csv\n",
      "  - Full output path: c:\\Users\\ag67236\\Desktop\\Projekt_psi\\dogs5.csv\n",
      "\n",
      "🚀 SMART APPEND MODE: Scraping dogs 500000 to 500100\n",
      "📂 Output file: dogs5.csv\n",
      "💾 Auto-save every 1000 dogs\n",
      "📊 Found 6893 unique dogs already in dogs5.csv\n",
      "✅ File exists - will append new data only\n",
      "============================================================\n",
      "  ❌ Dog 500100: No profile found\n",
      "⚠️ No records in final batch to save\n",
      "\n",
      "============================================================\n",
      "✅ SMART APPEND COMPLETED!\n",
      "⏱️ Time: 14.5 seconds (0.2 minutes)\n",
      "🐕 Dogs processed: 101\n",
      "✅ New dogs scraped: 0\n",
      "⏭️ Dogs skipped (already existed): 39\n",
      "❌ Dogs not found: 62\n",
      "📊 New records added: 0\n",
      "\n",
      "📈 Final file statistics:\n",
      "  - Total records: 395151\n",
      "  - Unique dogs: 6893\n",
      "  - File size: 55741.9 KB\n",
      "📂 CSV saved to: dogs5.csv\n",
      "📂 Full path: c:\\Users\\ag67236\\Desktop\\Projekt_psi\\dogs5.csv\n",
      "\n",
      "🎯 FINAL RESULT: 0 new records added to dogs5.csv\n"
=======
      "🔧 Fast Scraping Configuration:\n",
      "   Start ID: 637322\n",
      "   End ID: 637332\n",
      "   Total dogs: 10\n",
      "   Output: dogs3.csv\n",
      "\n",
      "🚀 Starting fast scraping for 10 dogs (IDs 637322-637331)\n",
      "📂 Output file: dogs3.csv\n",
      "==================================================\n",
      "Loading existing data from dogs3.csv...\n",
      "Loaded 10038 existing records, 1786 unique races\n",
      "Collecting race URLs from all dogs...\n",
      "Processing dog 1/10: 637322\n",
      "Collecting race URLs from all dogs...\n",
      "Processing dog 1/10: 637322\n",
      "Processing dog 2/10: 637323\n",
      "Processing dog 2/10: 637323\n",
      "Processing dog 3/10: 637324\n",
      "Processing dog 3/10: 637324\n",
      "Processing dog 4/10: 637325\n",
      "Processing dog 4/10: 637325\n",
      "Processing dog 5/10: 637326\n",
      "Processing dog 5/10: 637326\n",
      "Processing dog 6/10: 637327\n",
      "Processing dog 6/10: 637327\n",
      "Processing dog 7/10: 637328\n",
      "Processing dog 7/10: 637328\n",
      "Processing dog 8/10: 637329\n",
      "Processing dog 8/10: 637329\n",
      "Processing dog 9/10: 637330\n",
      "Processing dog 9/10: 637330\n",
      "Processing dog 10/10: 637331\n",
      "Processing dog 10/10: 637331\n",
      "Found 100 unique races to process\n",
      "Need to scrape 75 new races\n",
      "Found 100 unique races to process\n",
      "Need to scrape 75 new races\n",
      "Processed 10/75 new races\n",
      "Processed 10/75 new races\n",
      "Processed 20/75 new races\n",
      "Processed 20/75 new races\n",
      "Saved batch at race 25\n",
      "Saved batch at race 25\n",
      "Processed 30/75 new races\n",
      "Processed 30/75 new races\n",
      "Processed 40/75 new races\n",
      "Processed 40/75 new races\n",
      "Processed 50/75 new races\n",
      "Processed 50/75 new races\n",
      "Saved batch at race 50\n",
      "Saved batch at race 50\n",
      "Processed 60/75 new races\n",
      "Processed 60/75 new races\n",
      "Processed 70/75 new races\n",
      "Processed 70/75 new races\n",
      "Saved batch at race 75\n",
      "Completed! Total records: 10470\n",
      "Saved batch at race 75\n",
      "Completed! Total records: 10470\n",
      "==================================================\n",
      "✅ SCRAPING COMPLETED!\n",
      "📊 Total records scraped: 10470\n",
      "⏱️ Total time: 137.5 seconds\n",
      "🐕 Average per dog: 13.7 seconds\n",
      "📈 Speed: 4.4 dogs per minute\n",
      "💾 Data saved to: dogs3.csv\n",
      "🏁 Average races per dog: 1047.0\n",
      "\n",
      "🎯 FINAL RESULT: 10470 total records scraped\n",
      "📁 File size: 3579.8 KB\n",
      "==================================================\n",
      "✅ SCRAPING COMPLETED!\n",
      "📊 Total records scraped: 10470\n",
      "⏱️ Total time: 137.5 seconds\n",
      "🐕 Average per dog: 13.7 seconds\n",
      "📈 Speed: 4.4 dogs per minute\n",
      "💾 Data saved to: dogs3.csv\n",
      "🏁 Average races per dog: 1047.0\n",
      "\n",
      "🎯 FINAL RESULT: 10470 total records scraped\n",
      "📁 File size: 3579.8 KB\n"
>>>>>>> 693537a0d7ed315ff49e848e7adfe5ef2e6d38e1:scraping/fast_scraping_notebook.ipynb
     ]
    }
   ],
   "source": [
<<<<<<< HEAD:fast_scraping_notebook.ipynb
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "API_BASE = \"https://api.gbgb.org.uk/api/results/dog\"\n",
    "\n",
    "# CSV fields exactly as in API 'items'\n",
    "CSV_FIELDS = [\n",
    "    \"dogId\",  # Add this first - we'll inject it manually\n",
    "    \"dogName\",  # Add missing dog name\n",
    "    \"SP\",\n",
    "    \"resultPosition\",\n",
    "    \"resultBtnDistance\",\n",
    "    \"resultSectionalTime\",\n",
    "    \"resultComment\",\n",
    "    \"resultRunTime\",\n",
    "    \"resultDogWeight\",\n",
    "    \"winnerOr2ndName\",\n",
    "    \"winnerOr2ndId\",\n",
    "    \"resultAdjustedTime\",\n",
    "    \"trapNumber\",\n",
    "    \"raceTime\",\n",
    "    \"raceDate\",\n",
    "    \"raceId\",\n",
    "    \"raceNumber\",\n",
    "    \"raceType\",\n",
    "    \"raceClass\",\n",
    "    \"raceDistance\",\n",
    "    \"raceGoing\",\n",
    "    \"raceWinTime\",\n",
    "    \"meetingId\",\n",
    "    \"trackName\",\n",
    "    \"trainerName\",\n",
    "    \"ownerName\"\n",
    "]\n",
    "\n",
    "def get_existing_dog_ids(filename=\"dogs5.csv\"):\n",
    "    \"\"\"Get set of dog IDs that have already been scraped\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        return set()\n",
    "    \n",
    "    try:\n",
    "        # Read only the dogId column for efficiency\n",
    "        df = pd.read_csv(filename, usecols=['dogId'])\n",
    "        existing_ids = set(df['dogId'].astype(str).unique())\n",
    "        print(f\"📊 Found {len(existing_ids)} unique dogs already in {filename}\")\n",
    "        return existing_ids\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error reading existing file: {e}\")\n",
    "        return set()\n",
    "\n",
    "def fetch_items(dog_id, per_page=1000):\n",
    "    \"\"\"Fetch up to 'per_page' items in one request.\"\"\"\n",
    "    url = f\"{API_BASE}/{dog_id}\"\n",
    "    params = {\"page\": 1, \"itemsPerPage\": per_page}\n",
    "    \n",
    "    try:\n",
    "        resp = requests.get(url, params=params, timeout=10)\n",
    "        if resp.status_code == 404:\n",
    "            return None  # Dog doesn't exist\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        return data.get(\"items\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error fetching dog {dog_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def normalize_item(item, dog_id):\n",
    "    \"\"\"Extract fields from API item and add dog_id\"\"\"\n",
    "    record = {field: item.get(field, \"\") for field in CSV_FIELDS}\n",
    "    # Override dogId since it's not in the API response\n",
    "    record[\"dogId\"] = dog_id\n",
    "    return record\n",
    "\n",
    "def save_to_csv(records, filename=\"dogs5.csv\", header=False):\n",
    "    \"\"\"Save records to CSV file\"\"\"\n",
    "    if not records:\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        df = pd.DataFrame(records, columns=CSV_FIELDS)\n",
    "        df.to_csv(filename, mode=\"a\", index=False, header=header, encoding='utf-8')\n",
    "        return len(records)\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error saving to CSV: {e}\")\n",
    "        return 0\n",
    "\n",
    "def main_smart_append(start_id=600000, end_id=600200, output_file=\"dogs5.csv\"):\n",
    "    \"\"\"Smart append mode - only scrapes new dogs, saves every 1000 dogs\"\"\"\n",
    "    print(f\"🚀 SMART APPEND MODE: Scraping dogs {start_id} to {end_id}\")\n",
    "    print(f\"📂 Output file: {output_file}\")\n",
    "    print(f\"💾 Auto-save every 1000 dogs\")\n",
    "    \n",
    "    # Get existing dog IDs to avoid duplicates\n",
    "    existing_dog_ids = get_existing_dog_ids(output_file)\n",
    "    \n",
    "    # Check if file exists to determine if we need header\n",
    "    file_exists = os.path.exists(output_file)\n",
    "    header_needed = not file_exists\n",
    "    \n",
    "    if file_exists:\n",
    "        print(f\"✅ File exists - will append new data only\")\n",
    "    else:\n",
    "        print(f\"🆕 Creating new file\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    total_records = 0\n",
    "    successful_dogs = 0\n",
    "    skipped_dogs = 0\n",
    "    missing_dogs = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Batch processing variables\n",
    "    batch_records = []\n",
    "    batch_start_id = start_id\n",
    "    dogs_processed = 0\n",
    "    \n",
    "    for dog_id in range(start_id, end_id + 1):\n",
    "        dogs_processed += 1\n",
    "        \n",
    "        # Skip if already scraped\n",
    "        if str(dog_id) in existing_dog_ids:\n",
    "            skipped_dogs += 1\n",
    "            \n",
    "            # Check if we should save batch (every 1000 dogs processed)\n",
    "            if dogs_processed % 1000 == 0:\n",
    "                if batch_records:\n",
    "                    saved_count = save_to_csv(batch_records, output_file, header_needed)\n",
    "                    total_records += saved_count\n",
    "                    header_needed = False\n",
    "                    print(f\"📊 Batch save: {len(batch_records)} records from dogs {batch_start_id}-{dog_id}\")\n",
    "                    batch_records = []\n",
    "                    batch_start_id = dog_id + 1\n",
    "                print(f\"🔄 Progress: {dogs_processed}/{end_id - start_id + 1} dogs processed\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Fetch items from API\n",
    "            items = fetch_items(dog_id)\n",
    "            \n",
    "            if items is None:\n",
    "                missing_dogs += 1\n",
    "                if dog_id % 50 == 0:  # Show missing dogs occasionally\n",
    "                    print(f\"  ❌ Dog {dog_id}: No profile found\")\n",
    "                continue\n",
    "                \n",
    "            if not items:\n",
    "                if dog_id % 50 == 0:  # Show empty profiles occasionally\n",
    "                    print(f\"  ⚠️ Dog {dog_id}: Profile exists but no race items\")\n",
    "                continue\n",
    "            \n",
    "            # Add records to batch instead of saving immediately\n",
    "            records = [normalize_item(item, dog_id) for item in items]\n",
    "            batch_records.extend(records)\n",
    "            successful_dogs += 1\n",
    "            print(f\"  ✅ Dog {dog_id}: Queued {len(records)} records\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error processing dog {dog_id}: {str(e)}\")\n",
    "        \n",
    "        # Save batch every 1000 dogs processed\n",
    "        if dogs_processed % 1000 == 0:\n",
    "            if batch_records:\n",
    "                saved_count = save_to_csv(batch_records, output_file, header_needed)\n",
    "                total_records += saved_count\n",
    "                header_needed = False\n",
    "                print(f\"📊 Batch save: {len(batch_records)} records from dogs {batch_start_id}-{dog_id}\")\n",
    "                batch_records = []\n",
    "                batch_start_id = dog_id + 1\n",
    "            print(f\"🔄 Progress: {dogs_processed}/{end_id - start_id + 1} dogs processed\")\n",
    "        \n",
    "        # Small delay to be nice to the API\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    # Save any remaining records in the final batch\n",
    "    if batch_records:\n",
    "        saved_count = save_to_csv(batch_records, output_file, header_needed)\n",
    "        total_records += saved_count\n",
    "        print(f\"📊 Final batch save: {len(batch_records)} records from dogs {batch_start_id}-{end_id}\")\n",
    "        print(f\"🎯 IMPORTANT: Final batch of {len(batch_records)} records was saved!\")\n",
    "    else:\n",
    "        print(f\"⚠️ No records in final batch to save\")\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"✅ SMART APPEND COMPLETED!\")\n",
    "    print(f\"⏱️ Time: {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} minutes)\")\n",
    "    print(f\"🐕 Dogs processed: {end_id - start_id + 1}\")\n",
    "    print(f\"✅ New dogs scraped: {successful_dogs}\")\n",
    "    print(f\"⏭️ Dogs skipped (already existed): {skipped_dogs}\")\n",
    "    print(f\"❌ Dogs not found: {missing_dogs}\")\n",
    "    print(f\"📊 New records added: {total_records}\")\n",
    "    \n",
    "    if successful_dogs > 0:\n",
    "        print(f\"⚡ Speed: {successful_dogs/elapsed_time:.2f} dogs/second\")\n",
    "    \n",
    "    # Show final file stats\n",
    "    if os.path.exists(output_file):\n",
    "        try:\n",
    "            df = pd.read_csv(output_file)\n",
    "            unique_dogs = df['dogId'].nunique() if 'dogId' in df.columns else 0\n",
    "            print(f\"\\n📈 Final file statistics:\")\n",
    "            print(f\"  - Total records: {len(df)}\")\n",
    "            print(f\"  - Unique dogs: {unique_dogs}\")\n",
    "            print(f\"  - File size: {os.path.getsize(output_file) / 1024:.1f} KB\")\n",
    "            \n",
    "            # Show preview of new data\n",
    "            if total_records > 0:\n",
    "                print(f\"\\n📋 Preview of newly added data:\")\n",
    "                print(df.tail(3).to_string())\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not read final stats: {e}\")\n",
    "    \n",
    "    print(f\"📂 CSV saved to: {output_file}\")\n",
    "    print(f\"📂 Full path: {os.path.abspath(output_file)}\")\n",
    "    \n",
    "    return total_records\n",
    "\n",
    "# CONFIGURATION - Edit these values\n",
    "START_DOG_ID = 500100  # Starting dog ID - pick up where script left off\n",
    "END_DOG_ID = 500200    # Ending dog ID \n",
    "OUTPUT_FILE = \"dogs5.csv\"  # Output file name - saves to current directory\n",
    "\n",
    "print(\"🎯 CONFIGURATION:\")\n",
    "print(f\"  - Start ID: {START_DOG_ID}\")\n",
    "print(f\"  - End ID: {END_DOG_ID}\")\n",
    "print(f\"  - Total dogs to process: {END_DOG_ID - START_DOG_ID + 1}\")\n",
    "print(f\"  - Output file: {OUTPUT_FILE}\")\n",
    "print(f\"  - Full output path: {os.path.abspath(OUTPUT_FILE)}\")\n",
    "print()\n",
    "\n",
    "# Run the smart append scraper\n",
    "total_new_records = main_smart_append(\n",
=======
    "# FAST SCRAPING NOTEBOOK - Single Cell Solution\n",
    "# Import and use the optimized fast scraping function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Add current directory to path to import our module\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Import the fast scraping function\n",
    "from scraping.fast_scraping import fast_scrape_multiple_dogs\n",
    "\n",
    "def run_fast_scraping(start_id=637322, end_id=637332, output_file=\"dogs3.csv\"):\n",
    "    \"\"\"\n",
    "    Run fast scraping for a range of dog IDs\n",
    "    \n",
    "    Args:\n",
    "        start_id: Starting dog ID\n",
    "        end_id: Ending dog ID (exclusive)\n",
    "        output_file: Output CSV file name\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate dog ID list\n",
    "    dog_ids = [str(i) for i in range(start_id, end_id)]\n",
    "    \n",
    "    print(f\"🚀 Starting fast scraping for {len(dog_ids)} dogs (IDs {start_id}-{end_id-1})\")\n",
    "    print(f\"📂 Output file: {output_file}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Run the fast scraping\n",
    "        total_records = fast_scrape_multiple_dogs(\n",
    "            dog_ids=dog_ids,\n",
    "            output_file=output_file,\n",
    "            batch_size=25  # Save progress every 25 races\n",
    "        )\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        print(\"✅ SCRAPING COMPLETED!\")\n",
    "        print(f\"📊 Total records scraped: {total_records}\")\n",
    "        print(f\"⏱️ Total time: {elapsed_time:.1f} seconds\")\n",
    "        print(f\"🐕 Average per dog: {elapsed_time/len(dog_ids):.1f} seconds\")\n",
    "        print(f\"📈 Speed: {len(dog_ids)/elapsed_time*60:.1f} dogs per minute\")\n",
    "        print(f\"💾 Data saved to: {output_file}\")\n",
    "        \n",
    "        # Show some statistics\n",
    "        if total_records > 0:\n",
    "            avg_races_per_dog = total_records / len(dog_ids)\n",
    "            print(f\"🏁 Average races per dog: {avg_races_per_dog:.1f}\")\n",
    "        \n",
    "        return total_records\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during scraping: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "# CONFIGURATION - Edit these values as needed\n",
    "START_DOG_ID = 637322  # Starting dog ID\n",
    "END_DOG_ID = 637332    # Ending dog ID (will scrape 10 dogs)\n",
    "OUTPUT_FILE = \"dogs3.csv\"  # Output file name\n",
    "\n",
    "# RUN THE SCRAPING\n",
    "print(\"🔧 Fast Scraping Configuration:\")\n",
    "print(f\"   Start ID: {START_DOG_ID}\")\n",
    "print(f\"   End ID: {END_DOG_ID}\")\n",
    "print(f\"   Total dogs: {END_DOG_ID - START_DOG_ID}\")\n",
    "print(f\"   Output: {OUTPUT_FILE}\")\n",
    "print()\n",
    "\n",
    "# Execute the scraping\n",
    "total_scraped = run_fast_scraping(\n",
>>>>>>> 693537a0d7ed315ff49e848e7adfe5ef2e6d38e1:scraping/fast_scraping_notebook.ipynb
    "    start_id=START_DOG_ID,\n",
    "    end_id=END_DOG_ID,\n",
    "    output_file=OUTPUT_FILE\n",
    ")\n",
    "\n",
<<<<<<< HEAD:fast_scraping_notebook.ipynb
    "print(f\"\\n🎯 FINAL RESULT: {total_new_records} new records added to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c08ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick fix: Move the existing dogs5.csv to the data folder if it exists\n",
    "import shutil\n",
    "\n",
    "old_file = \"dogs5.csv\"\n",
    "new_file = \"../data/dogs5.csv\"\n",
    "\n",
    "if os.path.exists(old_file) and not os.path.exists(new_file):\n",
    "    # Create data directory\n",
    "    os.makedirs(\"../data\", exist_ok=True)\n",
    "    \n",
    "    # Move the file\n",
    "    shutil.move(old_file, new_file)\n",
    "    print(f\"✅ Moved {old_file} to {new_file}\")\n",
    "    \n",
    "    # Analyze the moved file\n",
    "    analyze_csv_file(new_file)\n",
    "else:\n",
    "    print(f\"📂 File locations:\")\n",
    "    print(f\"  - Old file exists: {os.path.exists(old_file)}\")\n",
    "    print(f\"  - New file exists: {os.path.exists(new_file)}\")\n",
    "    \n",
    "    if os.path.exists(new_file):\n",
    "        print(f\"✅ Data file is already in the correct location\")\n",
    "        analyze_csv_file(new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eaaf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITY FUNCTIONS FOR MANAGING THE CSV FILE\n",
    "\n",
    "def analyze_csv_file(filename=\"dogs5.csv\"):\n",
    "    \"\"\"Analyze the contents of the CSV file\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"❌ File {filename} does not exist\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "        print(f\"📊 ANALYSIS OF {filename}\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"📋 Basic Info:\")\n",
    "        print(f\"  - Total records: {len(df):,}\")\n",
    "        print(f\"  - File size: {os.path.getsize(filename) / 1024:.1f} KB\")\n",
    "        print(f\"  - Columns: {len(df.columns)}\")\n",
    "        \n",
    "        if 'dogId' in df.columns:\n",
    "            unique_dogs = df['dogId'].nunique()\n",
    "            print(f\"  - Unique dogs: {unique_dogs:,}\")\n",
    "            print(f\"  - Average records per dog: {len(df)/unique_dogs:.1f}\")\n",
    "            \n",
    "            # Show dog ID range\n",
    "            dog_ids = df['dogId'].astype(str).astype(int)\n",
    "            print(f\"  - Dog ID range: {dog_ids.min()} to {dog_ids.max()}\")\n",
    "            \n",
    "            # Show top dogs by record count\n",
    "            top_dogs = df['dogId'].value_counts().head(5)\n",
    "            print(f\"\\n🏆 Dogs with most records:\")\n",
    "            for dog_id, count in top_dogs.items():\n",
    "                print(f\"  - Dog {dog_id}: {count} records\")\n",
    "        \n",
    "        if 'raceDate' in df.columns:\n",
    "            print(f\"\\n📅 Date range:\")\n",
    "            print(f\"  - Earliest race: {df['raceDate'].min()}\")\n",
    "            print(f\"  - Latest race: {df['raceDate'].max()}\")\n",
    "        \n",
    "        if 'trackName' in df.columns:\n",
    "            unique_tracks = df['trackName'].nunique()\n",
    "            print(f\"\\n🏁 Track info:\")\n",
    "            print(f\"  - Unique tracks: {unique_tracks}\")\n",
    "            top_tracks = df['trackName'].value_counts().head(3)\n",
    "            for track, count in top_tracks.items():\n",
    "                print(f\"  - {track}: {count} records\")\n",
    "        \n",
    "        print(f\"\\n📋 Sample records:\")\n",
    "        print(df.head(2).to_string())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error analyzing file: {e}\")\n",
    "\n",
    "def find_missing_dogs(start_id, end_id, filename=\"dogs5.csv\"):\n",
    "    \"\"\"Find which dogs in a range haven't been scraped yet\"\"\"\n",
    "    existing_ids = set()\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            df = pd.read_csv(filename, usecols=['dogId'])\n",
    "            existing_ids = set(df['dogId'].astype(str).astype(int))\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error reading file: {e}\")\n",
    "    \n",
    "    all_ids = set(range(start_id, end_id + 1))\n",
    "    missing_ids = sorted(all_ids - existing_ids)\n",
    "    \n",
    "    print(f\"🔍 MISSING DOGS ANALYSIS ({start_id} to {end_id})\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"📊 Total dogs in range: {len(all_ids)}\")\n",
    "    print(f\"✅ Dogs already scraped: {len(all_ids) - len(missing_ids)}\")\n",
    "    print(f\"❌ Dogs missing: {len(missing_ids)}\")\n",
    "    \n",
    "    if missing_ids:\n",
    "        print(f\"\\n📋 Missing dog IDs:\")\n",
    "        # Show in groups of 10 for readability\n",
    "        for i in range(0, len(missing_ids), 10):\n",
    "            group = missing_ids[i:i+10]\n",
    "            print(f\"  {', '.join(map(str, group))}\")\n",
    "        \n",
    "        if len(missing_ids) <= 50:\n",
    "            print(f\"\\n💡 Suggested next scraping range:\")\n",
    "            print(f\"  START_DOG_ID = {min(missing_ids)}\")\n",
    "            print(f\"  END_DOG_ID = {max(missing_ids)}\")\n",
    "    else:\n",
    "        print(f\"\\n✅ All dogs in range {start_id}-{end_id} have been scraped!\")\n",
    "    \n",
    "    return missing_ids\n",
    "\n",
    "# UTILITY USAGE EXAMPLES:\n",
    "\n",
    "# Analyze current CSV file\n",
    "print(\"📊 ANALYZING CURRENT CSV FILE:\")\n",
    "analyze_csv_file(\"dogs5.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Find missing dogs in a range\n",
    "print(\"🔍 CHECKING FOR MISSING DOGS:\")\n",
    "missing = find_missing_dogs(600000, 600200, \"dogs5.csv\")"
   ]
=======
    "# Final summary\n",
    "print(f\"\\n🎯 FINAL RESULT: {total_scraped} total records scraped\")\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    file_size = os.path.getsize(OUTPUT_FILE) / 1024  # KB\n",
    "    print(f\"📁 File size: {file_size:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacbc93e",
   "metadata": {},
   "source": [
    "# Greyhound Racing Dataset - Column Explanations\n",
    "\n",
    "## Overview\n",
    "This CSV file contains comprehensive greyhound racing data with 42 columns capturing race information, dog characteristics, performance metrics, and derived features for machine learning analysis.\n",
    "\n",
    "## Column Descriptions\n",
    "\n",
    "### Basic Race Information\n",
    "- **Column 1-2**: `meeting_id`, `race_id` - Unique identifiers for the racing meeting and specific race\n",
    "- **Column 3**: `date` - Race date (e.g., \"Thursday 6th February 2025\")\n",
    "- **Column 4**: `track` - Racing venue (Newcastle, Towcester, Doncaster, etc.)\n",
    "- **Column 5**: `time` - Race start time\n",
    "- **Column 6**: `grade` - Race grade/class (A3, A4, B4, etc.)\n",
    "- **Column 7**: `distance` - Race distance in meters (450m, 480m, 500m, etc.)\n",
    "- **Column 8**: `prize_info` - Prize money breakdown for winners and placed dogs\n",
    "\n",
    "### Race Performance\n",
    "- **Column 9**: `finishing_position_text` - Finishing position as text (1st, 2nd, 3rd, etc.)\n",
    "- **Column 10**: `trap_number` - Starting trap number (1-6)\n",
    "- **Column 11**: `dog_id` - Unique identifier for the dog\n",
    "- **Column 12**: `dog_name` - Name of the greyhound\n",
    "- **Column 13**: `trainer` - Trainer's name\n",
    "- **Column 14**: `comment` - Race commentary describing the dog's performance\n",
    "- **Column 15**: `odds` - Betting odds (e.g., \"3/1\", \"11/8F\" where F = favorite)\n",
    "- **Column 16**: `sectional_time` - Split time at specific distance point\n",
    "- **Column 17**: `finish_time` - Final race time with margin behind winner\n",
    "- **Column 18**: `date_of_birth` - Dog's birth date\n",
    "- **Column 19**: `weight` - Dog's racing weight in kg\n",
    "- **Column 20**: `color_sex` - Color and sex code (e.g., \"b - bk\" = bitch - black)\n",
    "- **Column 21**: `sire` - Father's name\n",
    "- **Column 22**: `dam` - Mother's name\n",
    "- **Column 23**: `breeding_info` - Combined breeding information\n",
    "- **Column 24**: `url` - Link to race details\n",
    "\n",
    "### Processed Features\n",
    "- **Column 25**: `distance_numeric` - Race distance as numeric value\n",
    "- **Column 26**: `finishing_position` - Finishing position as number (1.0, 2.0, etc.)\n",
    "- **Column 27**: `weight_numeric` - Weight as numeric value\n",
    "- **Column 28**: `trap_number_numeric` - Trap number as numeric\n",
    "- **Column 29**: `sectional_time_numeric` - Sectional time as number\n",
    "- **Column 30**: `won_race` - Binary indicator (1 if won, 0 if not)\n",
    "\n",
    "### Performance Analysis Features\n",
    "- **Column 31**: `margin_lengths` - Margin behind winner in lengths\n",
    "- **Column 32**: `odds_numeric` - Converted odds as decimal number\n",
    "- **Column 33**: `color_code` - Simplified color code (bk, bd, be, f, etc.)\n",
    "- **Column 34**: `is_favorite` - Binary indicator if dog was favorite\n",
    "- **Column 35**: `early_pace` - Indicator of early speed/position\n",
    "- **Column 36**: `led_at_some_point` - Whether dog led during race\n",
    "- **Column 37**: `bumped_or_crowded` - Indicator of racing interference\n",
    "- **Column 38**: `clear_run` - Whether dog had unimpeded run\n",
    "- **Column 39**: `ran_on` - Whether dog finished strongly\n",
    "- **Column 40**: `checked_or_blocked` - Racing trouble indicators\n",
    "- **Column 41**: `wide_run` - Whether dog raced wide\n",
    "\n",
    "### Statistical Features\n",
    "- **Column 42**: `performance_score` - Calculated performance metric\n",
    "- **Column 43**: `is_short_distance` - Binary indicator for sprint races\n",
    "- **Column 44**: `is_long_distance` - Binary indicator for distance races  \n",
    "- **Column 45**: `is_middle_distance` - Binary indicator for middle distance\n",
    "- **Column 46**: `track_type` - Numeric track type classification\n",
    "\n",
    "## Key Insights from the Data\n",
    "\n",
    "### Race Grades\n",
    "- **A grades**: Higher class races (A2, A3, A4, etc.)\n",
    "- **B grades**: Mid-level competition  \n",
    "- **D grades**: Lower class/maiden races\n",
    "- **HP/OR**: Handicap/Open races\n",
    "\n",
    "### Performance Indicators\n",
    "The comment field contains valuable racing information:\n",
    "- **\"ALed\"** = Always led\n",
    "- **\"QAw\"** = Quick away from traps\n",
    "- **\"Crd\"** = Crowded during race\n",
    "- **\"Bmp\"** = Bumped by other dogs\n",
    "- **\"RnOn\"** = Ran on strongly at finish (positive - dog accelerated/finished with strong pace in final stretch)\n",
    "- **\"SAw\"** = Slow away from traps\n",
    "- **\"Wide\"** = Raced wide around bends\n",
    "\n",
    "### Distance Categories\n",
    "- **Short**: 245m-285m (sprint races)\n",
    "- **Middle**: 400m-450m (standard distances)  \n",
    "- **Long**: 480m-500m+ (staying races)\n",
    "\n",
    "## Racing Commentary Explanation\n",
    "\n",
    "### \"Ran On\" (RnOn) - Detailed Meaning:\n",
    "In greyhound racing, **\"Ran On\"** is a **positive performance indicator** that means:\n",
    "\n",
    "1. **Strong Finish**: The dog accelerated or maintained strong pace in the final portion of the race\n",
    "2. **Late Speed**: Shows the dog has stamina and finishing kick\n",
    "3. **Closing Ground**: Often indicates the dog was gaining on leaders or maintaining position strongly\n",
    "4. **Good Fitness**: Suggests the dog is in good racing condition\n",
    "5. **Distance Suitability**: May indicate the dog suits longer distances where stamina matters\n",
    "\n",
    "**This is GOOD performance** - it shows the dog finished the race strongly rather than tiring. Dogs that \"run on\" are often considered to have good racing fitness and potential for improvement at longer distances.\n",
    "\n",
    "**Contrast with negative terms**:\n",
    "- \"Tired\" = Dog slowed significantly in final stretch\n",
    "- \"Faded\" = Dog lost position/pace late in race\n",
    "- \"Weakened\" = Dog showed lack of stamina\n",
    "\n",
    "This dataset appears designed for predictive modeling of greyhound race outcomes, with features capturing both historical performance and race-day factors that influence results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc5dd2d",
   "metadata": {},
   "source": []
>>>>>>> 693537a0d7ed315ff49e848e7adfe5ef2e6d38e1:scraping/fast_scraping_notebook.ipynb
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
