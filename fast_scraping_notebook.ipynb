{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0021a578",
   "metadata": {},
   "source": [
    "# GBGB Dog Racing API Scraper\n",
    "\n",
    "This notebook uses the GBGB API to efficiently collect dog racing data.\n",
    "\n",
    "Benefits of using the API instead of web scraping:\n",
    "- Much faster data collection (10-100x faster)\n",
    "- More reliable - no HTML parsing required\n",
    "- Less resource-intensive - no browser needed\n",
    "- Complete data directly from the source\n",
    "- Less likely to be blocked\n",
    "\n",
    "**Smart Append Mode**: Only scrapes dogs that haven't been scraped yet, preserving existing data.\n",
    "**Batch Processing**: Saves data every 1000 dogs to prevent data loss.\n",
    "**Data Location**: Saves to `../data/dogs5.csv` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3921fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ CONFIGURATION:\n",
      "  - Start ID: 500000\n",
      "  - End ID: 500100\n",
      "  - Total dogs to process: 101\n",
      "  - Output file: dogs5.csv\n",
      "  - Full output path: c:\\Users\\ag67236\\Desktop\\Projekt_psi\\dogs5.csv\n",
      "\n",
      "üöÄ SMART APPEND MODE: Scraping dogs 500000 to 500100\n",
      "üìÇ Output file: dogs5.csv\n",
      "üíæ Auto-save every 1000 dogs\n",
      "üìä Found 6893 unique dogs already in dogs5.csv\n",
      "‚úÖ File exists - will append new data only\n",
      "============================================================\n",
      "  ‚ùå Dog 500100: No profile found\n",
      "‚ö†Ô∏è No records in final batch to save\n",
      "\n",
      "============================================================\n",
      "‚úÖ SMART APPEND COMPLETED!\n",
      "‚è±Ô∏è Time: 14.5 seconds (0.2 minutes)\n",
      "üêï Dogs processed: 101\n",
      "‚úÖ New dogs scraped: 0\n",
      "‚è≠Ô∏è Dogs skipped (already existed): 39\n",
      "‚ùå Dogs not found: 62\n",
      "üìä New records added: 0\n",
      "\n",
      "üìà Final file statistics:\n",
      "  - Total records: 395151\n",
      "  - Unique dogs: 6893\n",
      "  - File size: 55741.9 KB\n",
      "üìÇ CSV saved to: dogs5.csv\n",
      "üìÇ Full path: c:\\Users\\ag67236\\Desktop\\Projekt_psi\\dogs5.csv\n",
      "\n",
      "üéØ FINAL RESULT: 0 new records added to dogs5.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "API_BASE = \"https://api.gbgb.org.uk/api/results/dog\"\n",
    "\n",
    "# CSV fields exactly as in API 'items'\n",
    "CSV_FIELDS = [\n",
    "    \"dogId\",  # Add this first - we'll inject it manually\n",
    "    \"dogName\",  # Add missing dog name\n",
    "    \"SP\",\n",
    "    \"resultPosition\",\n",
    "    \"resultBtnDistance\",\n",
    "    \"resultSectionalTime\",\n",
    "    \"resultComment\",\n",
    "    \"resultRunTime\",\n",
    "    \"resultDogWeight\",\n",
    "    \"winnerOr2ndName\",\n",
    "    \"winnerOr2ndId\",\n",
    "    \"resultAdjustedTime\",\n",
    "    \"trapNumber\",\n",
    "    \"raceTime\",\n",
    "    \"raceDate\",\n",
    "    \"raceId\",\n",
    "    \"raceNumber\",\n",
    "    \"raceType\",\n",
    "    \"raceClass\",\n",
    "    \"raceDistance\",\n",
    "    \"raceGoing\",\n",
    "    \"raceWinTime\",\n",
    "    \"meetingId\",\n",
    "    \"trackName\",\n",
    "    \"trainerName\",\n",
    "    \"ownerName\"\n",
    "]\n",
    "\n",
    "def get_existing_dog_ids(filename=\"dogs5.csv\"):\n",
    "    \"\"\"Get set of dog IDs that have already been scraped\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        return set()\n",
    "    \n",
    "    try:\n",
    "        # Read only the dogId column for efficiency\n",
    "        df = pd.read_csv(filename, usecols=['dogId'])\n",
    "        existing_ids = set(df['dogId'].astype(str).unique())\n",
    "        print(f\"üìä Found {len(existing_ids)} unique dogs already in {filename}\")\n",
    "        return existing_ids\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error reading existing file: {e}\")\n",
    "        return set()\n",
    "\n",
    "def fetch_items(dog_id, per_page=1000):\n",
    "    \"\"\"Fetch up to 'per_page' items in one request.\"\"\"\n",
    "    url = f\"{API_BASE}/{dog_id}\"\n",
    "    params = {\"page\": 1, \"itemsPerPage\": per_page}\n",
    "    \n",
    "    try:\n",
    "        resp = requests.get(url, params=params, timeout=10)\n",
    "        if resp.status_code == 404:\n",
    "            return None  # Dog doesn't exist\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        return data.get(\"items\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error fetching dog {dog_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def normalize_item(item, dog_id):\n",
    "    \"\"\"Extract fields from API item and add dog_id\"\"\"\n",
    "    record = {field: item.get(field, \"\") for field in CSV_FIELDS}\n",
    "    # Override dogId since it's not in the API response\n",
    "    record[\"dogId\"] = dog_id\n",
    "    return record\n",
    "\n",
    "def save_to_csv(records, filename=\"dogs5.csv\", header=False):\n",
    "    \"\"\"Save records to CSV file\"\"\"\n",
    "    if not records:\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        df = pd.DataFrame(records, columns=CSV_FIELDS)\n",
    "        df.to_csv(filename, mode=\"a\", index=False, header=header, encoding='utf-8')\n",
    "        return len(records)\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error saving to CSV: {e}\")\n",
    "        return 0\n",
    "\n",
    "def main_smart_append(start_id=600000, end_id=600200, output_file=\"dogs5.csv\"):\n",
    "    \"\"\"Smart append mode - only scrapes new dogs, saves every 1000 dogs\"\"\"\n",
    "    print(f\"üöÄ SMART APPEND MODE: Scraping dogs {start_id} to {end_id}\")\n",
    "    print(f\"üìÇ Output file: {output_file}\")\n",
    "    print(f\"üíæ Auto-save every 1000 dogs\")\n",
    "    \n",
    "    # Get existing dog IDs to avoid duplicates\n",
    "    existing_dog_ids = get_existing_dog_ids(output_file)\n",
    "    \n",
    "    # Check if file exists to determine if we need header\n",
    "    file_exists = os.path.exists(output_file)\n",
    "    header_needed = not file_exists\n",
    "    \n",
    "    if file_exists:\n",
    "        print(f\"‚úÖ File exists - will append new data only\")\n",
    "    else:\n",
    "        print(f\"üÜï Creating new file\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    total_records = 0\n",
    "    successful_dogs = 0\n",
    "    skipped_dogs = 0\n",
    "    missing_dogs = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Batch processing variables\n",
    "    batch_records = []\n",
    "    batch_start_id = start_id\n",
    "    dogs_processed = 0\n",
    "    \n",
    "    for dog_id in range(start_id, end_id + 1):\n",
    "        dogs_processed += 1\n",
    "        \n",
    "        # Skip if already scraped\n",
    "        if str(dog_id) in existing_dog_ids:\n",
    "            skipped_dogs += 1\n",
    "            \n",
    "            # Check if we should save batch (every 1000 dogs processed)\n",
    "            if dogs_processed % 1000 == 0:\n",
    "                if batch_records:\n",
    "                    saved_count = save_to_csv(batch_records, output_file, header_needed)\n",
    "                    total_records += saved_count\n",
    "                    header_needed = False\n",
    "                    print(f\"üìä Batch save: {len(batch_records)} records from dogs {batch_start_id}-{dog_id}\")\n",
    "                    batch_records = []\n",
    "                    batch_start_id = dog_id + 1\n",
    "                print(f\"üîÑ Progress: {dogs_processed}/{end_id - start_id + 1} dogs processed\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Fetch items from API\n",
    "            items = fetch_items(dog_id)\n",
    "            \n",
    "            if items is None:\n",
    "                missing_dogs += 1\n",
    "                if dog_id % 50 == 0:  # Show missing dogs occasionally\n",
    "                    print(f\"  ‚ùå Dog {dog_id}: No profile found\")\n",
    "                continue\n",
    "                \n",
    "            if not items:\n",
    "                if dog_id % 50 == 0:  # Show empty profiles occasionally\n",
    "                    print(f\"  ‚ö†Ô∏è Dog {dog_id}: Profile exists but no race items\")\n",
    "                continue\n",
    "            \n",
    "            # Add records to batch instead of saving immediately\n",
    "            records = [normalize_item(item, dog_id) for item in items]\n",
    "            batch_records.extend(records)\n",
    "            successful_dogs += 1\n",
    "            print(f\"  ‚úÖ Dog {dog_id}: Queued {len(records)} records\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error processing dog {dog_id}: {str(e)}\")\n",
    "        \n",
    "        # Save batch every 1000 dogs processed\n",
    "        if dogs_processed % 1000 == 0:\n",
    "            if batch_records:\n",
    "                saved_count = save_to_csv(batch_records, output_file, header_needed)\n",
    "                total_records += saved_count\n",
    "                header_needed = False\n",
    "                print(f\"üìä Batch save: {len(batch_records)} records from dogs {batch_start_id}-{dog_id}\")\n",
    "                batch_records = []\n",
    "                batch_start_id = dog_id + 1\n",
    "            print(f\"üîÑ Progress: {dogs_processed}/{end_id - start_id + 1} dogs processed\")\n",
    "        \n",
    "        # Small delay to be nice to the API\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    # Save any remaining records in the final batch\n",
    "    if batch_records:\n",
    "        saved_count = save_to_csv(batch_records, output_file, header_needed)\n",
    "        total_records += saved_count\n",
    "        print(f\"üìä Final batch save: {len(batch_records)} records from dogs {batch_start_id}-{end_id}\")\n",
    "        print(f\"üéØ IMPORTANT: Final batch of {len(batch_records)} records was saved!\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No records in final batch to save\")\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ SMART APPEND COMPLETED!\")\n",
    "    print(f\"‚è±Ô∏è Time: {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} minutes)\")\n",
    "    print(f\"üêï Dogs processed: {end_id - start_id + 1}\")\n",
    "    print(f\"‚úÖ New dogs scraped: {successful_dogs}\")\n",
    "    print(f\"‚è≠Ô∏è Dogs skipped (already existed): {skipped_dogs}\")\n",
    "    print(f\"‚ùå Dogs not found: {missing_dogs}\")\n",
    "    print(f\"üìä New records added: {total_records}\")\n",
    "    \n",
    "    if successful_dogs > 0:\n",
    "        print(f\"‚ö° Speed: {successful_dogs/elapsed_time:.2f} dogs/second\")\n",
    "    \n",
    "    # Show final file stats\n",
    "    if os.path.exists(output_file):\n",
    "        try:\n",
    "            df = pd.read_csv(output_file)\n",
    "            unique_dogs = df['dogId'].nunique() if 'dogId' in df.columns else 0\n",
    "            print(f\"\\nüìà Final file statistics:\")\n",
    "            print(f\"  - Total records: {len(df)}\")\n",
    "            print(f\"  - Unique dogs: {unique_dogs}\")\n",
    "            print(f\"  - File size: {os.path.getsize(output_file) / 1024:.1f} KB\")\n",
    "            \n",
    "            # Show preview of new data\n",
    "            if total_records > 0:\n",
    "                print(f\"\\nüìã Preview of newly added data:\")\n",
    "                print(df.tail(3).to_string())\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not read final stats: {e}\")\n",
    "    \n",
    "    print(f\"üìÇ CSV saved to: {output_file}\")\n",
    "    print(f\"üìÇ Full path: {os.path.abspath(output_file)}\")\n",
    "    \n",
    "    return total_records\n",
    "\n",
    "# CONFIGURATION - Edit these values\n",
    "START_DOG_ID = 500100  # Starting dog ID - pick up where script left off\n",
    "END_DOG_ID = 500200    # Ending dog ID \n",
    "OUTPUT_FILE = \"dogs5.csv\"  # Output file name - saves to current directory\n",
    "\n",
    "print(\"üéØ CONFIGURATION:\")\n",
    "print(f\"  - Start ID: {START_DOG_ID}\")\n",
    "print(f\"  - End ID: {END_DOG_ID}\")\n",
    "print(f\"  - Total dogs to process: {END_DOG_ID - START_DOG_ID + 1}\")\n",
    "print(f\"  - Output file: {OUTPUT_FILE}\")\n",
    "print(f\"  - Full output path: {os.path.abspath(OUTPUT_FILE)}\")\n",
    "print()\n",
    "\n",
    "# Run the smart append scraper\n",
    "total_new_records = main_smart_append(\n",
    "    start_id=START_DOG_ID,\n",
    "    end_id=END_DOG_ID,\n",
    "    output_file=OUTPUT_FILE\n",
    ")\n",
    "\n",
    "print(f\"\\nüéØ FINAL RESULT: {total_new_records} new records added to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c08ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick fix: Move the existing dogs5.csv to the data folder if it exists\n",
    "import shutil\n",
    "\n",
    "old_file = \"dogs5.csv\"\n",
    "new_file = \"../data/dogs5.csv\"\n",
    "\n",
    "if os.path.exists(old_file) and not os.path.exists(new_file):\n",
    "    # Create data directory\n",
    "    os.makedirs(\"../data\", exist_ok=True)\n",
    "    \n",
    "    # Move the file\n",
    "    shutil.move(old_file, new_file)\n",
    "    print(f\"‚úÖ Moved {old_file} to {new_file}\")\n",
    "    \n",
    "    # Analyze the moved file\n",
    "    analyze_csv_file(new_file)\n",
    "else:\n",
    "    print(f\"üìÇ File locations:\")\n",
    "    print(f\"  - Old file exists: {os.path.exists(old_file)}\")\n",
    "    print(f\"  - New file exists: {os.path.exists(new_file)}\")\n",
    "    \n",
    "    if os.path.exists(new_file):\n",
    "        print(f\"‚úÖ Data file is already in the correct location\")\n",
    "        analyze_csv_file(new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eaaf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITY FUNCTIONS FOR MANAGING THE CSV FILE\n",
    "\n",
    "def analyze_csv_file(filename=\"dogs5.csv\"):\n",
    "    \"\"\"Analyze the contents of the CSV file\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"‚ùå File {filename} does not exist\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "        print(f\"üìä ANALYSIS OF {filename}\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"üìã Basic Info:\")\n",
    "        print(f\"  - Total records: {len(df):,}\")\n",
    "        print(f\"  - File size: {os.path.getsize(filename) / 1024:.1f} KB\")\n",
    "        print(f\"  - Columns: {len(df.columns)}\")\n",
    "        \n",
    "        if 'dogId' in df.columns:\n",
    "            unique_dogs = df['dogId'].nunique()\n",
    "            print(f\"  - Unique dogs: {unique_dogs:,}\")\n",
    "            print(f\"  - Average records per dog: {len(df)/unique_dogs:.1f}\")\n",
    "            \n",
    "            # Show dog ID range\n",
    "            dog_ids = df['dogId'].astype(str).astype(int)\n",
    "            print(f\"  - Dog ID range: {dog_ids.min()} to {dog_ids.max()}\")\n",
    "            \n",
    "            # Show top dogs by record count\n",
    "            top_dogs = df['dogId'].value_counts().head(5)\n",
    "            print(f\"\\nüèÜ Dogs with most records:\")\n",
    "            for dog_id, count in top_dogs.items():\n",
    "                print(f\"  - Dog {dog_id}: {count} records\")\n",
    "        \n",
    "        if 'raceDate' in df.columns:\n",
    "            print(f\"\\nüìÖ Date range:\")\n",
    "            print(f\"  - Earliest race: {df['raceDate'].min()}\")\n",
    "            print(f\"  - Latest race: {df['raceDate'].max()}\")\n",
    "        \n",
    "        if 'trackName' in df.columns:\n",
    "            unique_tracks = df['trackName'].nunique()\n",
    "            print(f\"\\nüèÅ Track info:\")\n",
    "            print(f\"  - Unique tracks: {unique_tracks}\")\n",
    "            top_tracks = df['trackName'].value_counts().head(3)\n",
    "            for track, count in top_tracks.items():\n",
    "                print(f\"  - {track}: {count} records\")\n",
    "        \n",
    "        print(f\"\\nüìã Sample records:\")\n",
    "        print(df.head(2).to_string())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing file: {e}\")\n",
    "\n",
    "def find_missing_dogs(start_id, end_id, filename=\"dogs5.csv\"):\n",
    "    \"\"\"Find which dogs in a range haven't been scraped yet\"\"\"\n",
    "    existing_ids = set()\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            df = pd.read_csv(filename, usecols=['dogId'])\n",
    "            existing_ids = set(df['dogId'].astype(str).astype(int))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error reading file: {e}\")\n",
    "    \n",
    "    all_ids = set(range(start_id, end_id + 1))\n",
    "    missing_ids = sorted(all_ids - existing_ids)\n",
    "    \n",
    "    print(f\"üîç MISSING DOGS ANALYSIS ({start_id} to {end_id})\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìä Total dogs in range: {len(all_ids)}\")\n",
    "    print(f\"‚úÖ Dogs already scraped: {len(all_ids) - len(missing_ids)}\")\n",
    "    print(f\"‚ùå Dogs missing: {len(missing_ids)}\")\n",
    "    \n",
    "    if missing_ids:\n",
    "        print(f\"\\nüìã Missing dog IDs:\")\n",
    "        # Show in groups of 10 for readability\n",
    "        for i in range(0, len(missing_ids), 10):\n",
    "            group = missing_ids[i:i+10]\n",
    "            print(f\"  {', '.join(map(str, group))}\")\n",
    "        \n",
    "        if len(missing_ids) <= 50:\n",
    "            print(f\"\\nüí° Suggested next scraping range:\")\n",
    "            print(f\"  START_DOG_ID = {min(missing_ids)}\")\n",
    "            print(f\"  END_DOG_ID = {max(missing_ids)}\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ All dogs in range {start_id}-{end_id} have been scraped!\")\n",
    "    \n",
    "    return missing_ids\n",
    "\n",
    "# UTILITY USAGE EXAMPLES:\n",
    "\n",
    "# Analyze current CSV file\n",
    "print(\"üìä ANALYZING CURRENT CSV FILE:\")\n",
    "analyze_csv_file(\"dogs5.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Find missing dogs in a range\n",
    "print(\"üîç CHECKING FOR MISSING DOGS:\")\n",
    "missing = find_missing_dogs(600000, 600200, \"dogs5.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
