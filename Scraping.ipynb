{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8161a37",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mselenium\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mselenium\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwebdriver\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchrome\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mservice\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Service\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mselenium\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwebdriver\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchrome\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Options\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def create_fast_driver():\n",
    "    \"\"\"Create optimized Chrome driver for speed\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--disable-images\")\n",
    "    chrome_options.add_argument(\"--disable-plugins\") \n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\")\n",
    "    \n",
    "    service = Service()\n",
    "    return webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def extract_complete_race_data(driver, race_info):\n",
    "    \"\"\"Extract all dogs' data from a race page\"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        \n",
    "        # Extract race metadata\n",
    "        race_date_elem = soup.select_one(\".Meeting__header__date\")\n",
    "        race_date = race_date_elem.text.strip() if race_date_elem else race_info.get('race_date', '')\n",
    "        \n",
    "        track_elem = soup.select_one(\".Meeting__header__title__meta\")\n",
    "        track = track_elem.text.strip() if track_elem else \"\"\n",
    "        \n",
    "        # Extract race details from race header\n",
    "        race_time = \"\"\n",
    "        race_class = \"\"\n",
    "        race_distance = \"\"\n",
    "        race_prizes = \"\"\n",
    "        \n",
    "        race_header = soup.select_one(\".MeetingRace__header\")\n",
    "        if race_header:\n",
    "            time_elem = race_header.select_one(\".MeetingRace__time\")\n",
    "            if time_elem:\n",
    "                race_time = time_elem.text.strip()\n",
    "            \n",
    "            class_elem = race_header.select_one(\".MeetingRace__class\")\n",
    "            if class_elem:\n",
    "                race_class = class_elem.text.strip().replace(\"|\", \"\").strip()\n",
    "            \n",
    "            distance_elem = race_header.select_one(\".MeetingRace__distance\")\n",
    "            if distance_elem:\n",
    "                race_distance = distance_elem.text.strip()\n",
    "                \n",
    "            prizes_elem = race_header.select_one(\".MeetingRace__prizes\")\n",
    "            if prizes_elem:\n",
    "                race_prizes = prizes_elem.text.strip()\n",
    "        \n",
    "        # Extract all dogs from race\n",
    "        dogs_data = []\n",
    "        dog_rows = soup.select(\".MeetingRaceTrap\")\n",
    "        \n",
    "        for dog_row in dog_rows:\n",
    "            dog_data = extract_single_dog_data(dog_row)\n",
    "            if dog_data:\n",
    "                # Add race metadata to each dog\n",
    "                dog_data.update({\n",
    "                    'meeting_id': race_info['meeting_id'],\n",
    "                    'race_id': race_info['race_id'],\n",
    "                    'race_url': race_info['race_url'],\n",
    "                    'race_date': race_date,\n",
    "                    'track': track,\n",
    "                    'race_time': race_time,\n",
    "                    'race_class': race_class,\n",
    "                    'race_distance': race_distance,\n",
    "                    'race_prizes': race_prizes\n",
    "                })\n",
    "                dogs_data.append(dog_data)\n",
    "        \n",
    "        return dogs_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def extract_single_dog_data(dog_row):\n",
    "    \"\"\"Extract data for a single dog from its row\"\"\"\n",
    "    try:\n",
    "        data = {}\n",
    "        \n",
    "        # Position\n",
    "        pos_elem = dog_row.select_one(\".MeetingRaceTrap__pos\")\n",
    "        data['position'] = pos_elem.text.strip() if pos_elem else \"\"\n",
    "        \n",
    "        # Dog name and ID\n",
    "        greyhound_elem = dog_row.select_one(\".MeetingRaceTrap__greyhound\")\n",
    "        data['dog_name'] = greyhound_elem.text.strip() if greyhound_elem else \"\"\n",
    "        \n",
    "        # Extract dog ID from greyhound link\n",
    "        if greyhound_elem and greyhound_elem.get('href'):\n",
    "            href = greyhound_elem['href']\n",
    "            dog_id_match = re.search(r'greyhoundId=(\\d+)', href)\n",
    "            data['dog_id'] = dog_id_match.group(1) if dog_id_match else \"\"\n",
    "        else:\n",
    "            data['dog_id'] = \"\"\n",
    "        \n",
    "        # Trainer\n",
    "        trainer_elem = dog_row.select_one(\".MeetingRaceTrap__trainer\")\n",
    "        data['trainer'] = trainer_elem.text.strip() if trainer_elem else \"\"\n",
    "        \n",
    "        # Comments/Remarks\n",
    "        comment_elem = dog_row.select_one(\".MeetingRaceTrap__comment\")\n",
    "        data['comments'] = comment_elem.text.strip() if comment_elem else \"\"\n",
    "        \n",
    "        # Starting Price\n",
    "        sp_elem = dog_row.select_one(\".MeetingRaceTrap__sp\")\n",
    "        data['starting_price'] = sp_elem.text.strip() if sp_elem else \"\"\n",
    "        \n",
    "        # Time (S)\n",
    "        time_s_elem = dog_row.select_one(\".MeetingRaceTrap__timeS\")\n",
    "        data['time_s'] = time_s_elem.text.strip() if time_s_elem else \"\"\n",
    "        \n",
    "        # Time (Distance)\n",
    "        time_dist_elem = dog_row.select_one(\".MeetingRaceTrap__timeDistance\")\n",
    "        data['time_distance'] = time_dist_elem.text.strip() if time_dist_elem else \"\"\n",
    "        \n",
    "        # Extract trap number from trap image\n",
    "        trap_elem = dog_row.select_one(\".MeetingRaceTrap__trap img\")\n",
    "        if trap_elem and trap_elem.get('src'):\n",
    "            trap_match = re.search(r'icn-(\\d+)', trap_elem['src'])\n",
    "            data['trap'] = trap_match.group(1) if trap_match else \"\"\n",
    "        else:\n",
    "            data['trap'] = \"\"\n",
    "        \n",
    "        # Extract breeding info from hound profile\n",
    "        profile_elem = dog_row.select_one(\".MeetingRaceTrap__houndProfile\")\n",
    "        if profile_elem:\n",
    "            profile_text = profile_elem.text.strip()\n",
    "            data['breeding_info'] = profile_text\n",
    "            \n",
    "            # Parse breeding info: \"Oct-2020 | 34.4 | d - bd | Ballymac Best - Ballykett Beauty\"\n",
    "            parts = [p.strip() for p in profile_text.split('|')]\n",
    "            \n",
    "            if len(parts) >= 1:\n",
    "                data['birth_date'] = parts[0]\n",
    "            if len(parts) >= 2:\n",
    "                data['weight'] = parts[1]\n",
    "            if len(parts) >= 3:\n",
    "                data['color'] = parts[2]\n",
    "            if len(parts) >= 4:\n",
    "                # Extract sire and dam from \"Sire - Dam\" format\n",
    "                parents = parts[3]\n",
    "                if ' - ' in parents:\n",
    "                    sire, dam = parents.split(' - ', 1)\n",
    "                    data['sire'] = sire.strip()\n",
    "                    data['dam'] = dam.strip()\n",
    "                else:\n",
    "                    data['sire'] = \"\"\n",
    "                    data['dam'] = \"\"\n",
    "        else:\n",
    "            data['breeding_info'] = \"\"\n",
    "            data['birth_date'] = \"\"\n",
    "            data['weight'] = \"\"\n",
    "            data['color'] = \"\"\n",
    "            data['sire'] = \"\"\n",
    "            data['dam'] = \"\"\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "def save_comprehensive_data(all_race_data, filename=\"dogs3.csv\"):\n",
    "    \"\"\"Save comprehensive race data to CSV sorted by race_id\"\"\"\n",
    "    if not all_race_data:\n",
    "        return\n",
    "    \n",
    "    # Sort by race_id (convert to int for proper sorting)\n",
    "    all_race_data.sort(key=lambda x: int(x.get('race_id', 0)) if x.get('race_id', '').isdigit() else 0)\n",
    "    \n",
    "    fieldnames = [\n",
    "        'meeting_id', 'race_id', 'race_date', 'track', 'race_time', 'race_class', \n",
    "        'race_distance', 'race_prizes', 'position', 'trap', 'dog_id', 'dog_name', \n",
    "        'trainer', 'comments', 'starting_price', 'time_s', 'time_distance',\n",
    "        'birth_date', 'weight', 'color', 'sire', 'dam', 'breeding_info', 'race_url'\n",
    "    ]\n",
    "    \n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for race in all_race_data:\n",
    "            writer.writerow(race)\n",
    "\n",
    "print(\"‚úÖ Scraping functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7341d1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_data(input_file=\"dogs3.csv\", output_file=\"dogs3_processed.csv\", verbose=False):\n",
    "    \"\"\"Process raw scraped data and save it to the processed CSV file\"\"\"\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    from datetime import datetime\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Processing data from {input_file} to {output_file}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load the raw data\n",
    "        df = pd.read_csv(input_file)\n",
    "        \n",
    "        # Make a copy for processing\n",
    "        processed_df = df.copy()\n",
    "        \n",
    "        # Process data - similar to what your prediction script does\n",
    "        # Handle missing values\n",
    "        processed_df = processed_df.fillna({\n",
    "            'birth_date': 'Unknown',\n",
    "            'weight': '0.0',\n",
    "            'time_s': '0.00',\n",
    "            'time_distance': '0.00 (0)',\n",
    "            'trap': '0',\n",
    "            'position': '0',\n",
    "            'comments': '',\n",
    "            'race_distance': '0m'\n",
    "        })\n",
    "        \n",
    "        # Extract distance in meters\n",
    "        processed_df['distance_meters'] = processed_df['race_distance'].apply(\n",
    "            lambda x: int(re.search(r'(\\d+)', str(x)).group(1)) if isinstance(x, str) and re.search(r'(\\d+)', str(x)) else 0\n",
    "        )\n",
    "        \n",
    "        # Convert position to numeric\n",
    "        processed_df['position_numeric'] = processed_df['position'].apply(\n",
    "            lambda x: int(re.search(r'(\\d+)', str(x)).group(1)) if isinstance(x, str) and re.search(r'(\\d+)', str(x)) else 99\n",
    "        )\n",
    "        \n",
    "        # Extract weight as float\n",
    "        processed_df['weight_kg'] = processed_df['weight'].apply(\n",
    "            lambda x: float(str(x).replace(',', '.')) if isinstance(x, str) and re.search(r'^\\d+\\.?\\d*$', x.replace(',', '.')) else 0.0\n",
    "        )\n",
    "        \n",
    "        # Convert trap to numeric\n",
    "        processed_df['trap_numeric'] = processed_df['trap'].apply(\n",
    "            lambda x: int(x) if str(x).isdigit() else 0\n",
    "        )\n",
    "        \n",
    "        # Calculate age at race time\n",
    "        def calculate_age(birth_date_str):\n",
    "            if not isinstance(birth_date_str, str) or birth_date_str == 'Unknown':\n",
    "                return 3.0  # Default age\n",
    "                \n",
    "            try:\n",
    "                # Extract year from birth date (format: \"Aug-2018\")\n",
    "                birth_year = int(birth_date_str.split('-')[1]) if '-' in birth_date_str else 0\n",
    "                if birth_year == 0:\n",
    "                    return 3.0\n",
    "                \n",
    "                # Use current year as race year for simplicity\n",
    "                current_year = datetime.now().year\n",
    "                age = current_year - birth_year\n",
    "                return age if 1 <= age <= 7 else 3.0\n",
    "            except:\n",
    "                return 3.0\n",
    "                \n",
    "        processed_df['age_at_race'] = processed_df['birth_date'].apply(calculate_age)\n",
    "        \n",
    "        # Extract starting price as odds ratio\n",
    "        def parse_starting_price(sp):\n",
    "            if not isinstance(sp, str):\n",
    "                return 0.0\n",
    "            try:\n",
    "                if '/' in sp:\n",
    "                    num, denom = sp.split('/')\n",
    "                    if 'F' in denom:  # Handle cases like \"5/4F\"\n",
    "                        denom = denom.replace('F', '')\n",
    "                    return float(num) / float(denom) + 1.0\n",
    "                elif sp.lower() == 'evs' or sp.lower() == 'evens':\n",
    "                    return 2.0\n",
    "                else:\n",
    "                    return 0.0\n",
    "            except:\n",
    "                return 0.0\n",
    "                \n",
    "        processed_df['odds_ratio'] = processed_df['starting_price'].apply(parse_starting_price)\n",
    "        \n",
    "        # Extract time in seconds\n",
    "        processed_df['time_seconds'] = processed_df['time_s'].apply(\n",
    "            lambda x: float(x) if isinstance(x, str) and x.replace('.', '', 1).isdigit() else 0.0\n",
    "        )\n",
    "        \n",
    "        # Create \"won_race\" target variable\n",
    "        processed_df['won_race'] = (processed_df['position_numeric'] == 1).astype(int)\n",
    "        \n",
    "        # Extract primary color\n",
    "        def extract_primary_color(color_str):\n",
    "            if not isinstance(color_str, str):\n",
    "                return 'unknown'\n",
    "            color_str = color_str.lower()\n",
    "            colors = ['bk', 'bd', 'be', 'wbk', 'f', 'dkbd', 'wbd', 'bew', 'bebd', 'wbe']\n",
    "            for color in colors:\n",
    "                if color in color_str:\n",
    "                    return color\n",
    "            return 'other'\n",
    "            \n",
    "        processed_df['primary_color'] = processed_df['color'].apply(extract_primary_color)\n",
    "        \n",
    "        # Process comments\n",
    "        processed_df['comment_early_pace'] = processed_df['comments'].apply(\n",
    "            lambda x: 1 if isinstance(x, str) and any(term in x.upper() for term in ['EP', 'QAW', 'VQAW']) else 0\n",
    "        )\n",
    "        \n",
    "        processed_df['comment_led'] = processed_df['comments'].apply(\n",
    "            lambda x: 1 if isinstance(x, str) and any(term in x.upper() for term in ['LD', 'LED', 'ALT']) else 0\n",
    "        )\n",
    "        \n",
    "        processed_df['comment_crowded'] = processed_df['comments'].apply(\n",
    "            lambda x: 1 if isinstance(x, str) and any(term in x.upper() for term in ['CRD', 'BMP', 'CROWD']) else 0\n",
    "        )\n",
    "        \n",
    "        processed_df['comment_strong_finish'] = processed_df['comments'].apply(\n",
    "            lambda x: 1 if isinstance(x, str) and any(term in x.upper() for term in ['FINSTR', 'RANON', 'STYD']) else 0\n",
    "        )\n",
    "        \n",
    "        processed_df['comment_middle_runner'] = processed_df['comments'].apply(\n",
    "            lambda x: 1 if isinstance(x, str) and any(term in x.upper() for term in ['MID', 'MIDDLE']) else 0\n",
    "        )\n",
    "        \n",
    "        processed_df['comment_rails_runner'] = processed_df['comments'].apply(\n",
    "            lambda x: 1 if isinstance(x, str) and any(term in x.upper() for term in ['RLS', 'RAILS']) else 0\n",
    "        )\n",
    "        \n",
    "        processed_df['comment_wide_runner'] = processed_df['comments'].apply(\n",
    "            lambda x: 1 if isinstance(x, str) and any(term in x.upper() for term in ['WIDE', 'W']) else 0\n",
    "        )\n",
    "        \n",
    "        # Process trap track bias\n",
    "        trap_track_stats = processed_df.groupby(['track', 'trap_numeric']).agg(\n",
    "            win_rate=('won_race', 'mean'),\n",
    "            count=('won_race', 'count')\n",
    "        ).reset_index()\n",
    "        \n",
    "        trap_track_stats = trap_track_stats[trap_track_stats['count'] >= 5]\n",
    "        \n",
    "        trap_track_map = {}\n",
    "        for _, row in trap_track_stats.iterrows():\n",
    "            track = row['track']\n",
    "            trap = row['trap_numeric']\n",
    "            win_rate = row['win_rate']\n",
    "            \n",
    "            if track not in trap_track_map:\n",
    "                trap_track_map[track] = {}\n",
    "            trap_track_map[track][trap] = win_rate\n",
    "        \n",
    "        def get_trap_track_bias(row):\n",
    "            track = row['track']\n",
    "            trap = row['trap_numeric']\n",
    "            \n",
    "            if track in trap_track_map and trap in trap_track_map[track]:\n",
    "                return trap_track_map[track][trap]\n",
    "            return 0.15  # Default win rate\n",
    "            \n",
    "        processed_df['trap_track_bias'] = processed_df.apply(get_trap_track_bias, axis=1)\n",
    "        \n",
    "        # Distance preference\n",
    "        distance_ranges = [(0, 300), (301, 500), (501, 700), (701, 1000)]\n",
    "        \n",
    "        for min_dist, max_dist in distance_ranges:\n",
    "            range_name = f\"dist_range_{min_dist}_{max_dist}m\"\n",
    "            processed_df[range_name] = ((processed_df['distance_meters'] >= min_dist) & \n",
    "                                     (processed_df['distance_meters'] <= max_dist)).astype(int)\n",
    "        \n",
    "        # Add grade_numeric feature\n",
    "        def get_grade_numeric(race_class):\n",
    "            match = re.match(r'([A|D|S])(\\d+)', str(race_class).upper())\n",
    "            if match:\n",
    "                grade_type, grade_num = match.groups()\n",
    "                return int(grade_num)\n",
    "            if str(race_class).upper() in ['OR', 'OPEN', 'INV', 'IT']:\n",
    "                return 0\n",
    "            return 99\n",
    "            \n",
    "        processed_df['grade_numeric'] = processed_df['race_class'].apply(get_grade_numeric)\n",
    "        \n",
    "        # Save the processed data\n",
    "        processed_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Error processing data: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f288f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_scrape_multiple_dogs(dog_ids, output_file=\"dogs3.csv\", processed_file=\"dogs3_processed.csv\", batch_size=50, verbose=False):\n",
    "    \"\"\"\n",
    "    Optimized scraping with minimal delays and better error handling\n",
    "    dog_ids: list of dog IDs to scrape\n",
    "    output_file: CSV file to save results\n",
    "    processed_file: CSV file to save processed results\n",
    "    \"\"\"\n",
    "    \n",
    "    processed_races = set()\n",
    "    all_race_data = []\n",
    "    \n",
    "    # Load existing data to avoid reprocessing\n",
    "    if os.path.exists(output_file):\n",
    "        try:\n",
    "            with open(output_file, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                for row in reader:\n",
    "                    all_race_data.append(row)\n",
    "                    race_key = f\"{row.get('meeting_id', '')}_{row.get('race_id', '')}\"\n",
    "                    processed_races.add(race_key)\n",
    "            if verbose:\n",
    "                print(f\"Loaded {len(all_race_data)} existing records, {len(processed_races)} unique races\")\n",
    "        except Exception as e:\n",
    "            all_race_data = []\n",
    "            processed_races = set()\n",
    "\n",
    "    # Create optimized driver\n",
    "    driver = create_fast_driver()\n",
    "    wait = WebDriverWait(driver, 5)  # 5 second timeout\n",
    "    \n",
    "    try:\n",
    "        print(f\"Scraping {len(dog_ids)} dogs...\")\n",
    "        all_race_urls = set()\n",
    "        \n",
    "        # Phase 1: Collect all race URLs from all dogs\n",
    "        for i, dog_id in enumerate(dog_ids, 1):\n",
    "            if verbose or i % 5 == 0 or i == len(dog_ids):\n",
    "                print(f\"\\rProcessing dog {i}/{len(dog_ids)}: {dog_id}\", end=\"\", flush=True)\n",
    "            \n",
    "            try:\n",
    "                race_urls = fast_get_dog_race_urls(driver, dog_id, wait, verbose)\n",
    "                \n",
    "                for race_info in race_urls:\n",
    "                    race_tuple = (\n",
    "                        race_info['race_url'],\n",
    "                        race_info['meeting_id'], \n",
    "                        race_info['race_id'],\n",
    "                        race_info['race_date']\n",
    "                    )\n",
    "                    all_race_urls.add(race_tuple)\n",
    "                    \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        print()  # New line after progress indicator\n",
    "        \n",
    "        # Phase 2: Filter out already processed races\n",
    "        new_races = []\n",
    "        for race_tuple in all_race_urls:\n",
    "            race_info = {\n",
    "                'race_url': race_tuple[0],\n",
    "                'meeting_id': race_tuple[1],\n",
    "                'race_id': race_tuple[2],\n",
    "                'race_date': race_tuple[3]\n",
    "            }\n",
    "            \n",
    "            race_key = f\"{race_info['meeting_id']}_{race_info['race_id']}\"\n",
    "            if race_key not in processed_races:\n",
    "                new_races.append(race_info)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Found {len(new_races)} new races to scrape\")\n",
    "        \n",
    "        # Phase 3: Scrape new race data\n",
    "        if new_races:\n",
    "            for i, race_info in enumerate(new_races, 1):\n",
    "                if verbose and i % 10 == 0:\n",
    "                    print(f\"\\rScraping races: {i}/{len(new_races)}\", end=\"\", flush=True)\n",
    "                \n",
    "                try:\n",
    "                    # Navigate to race page\n",
    "                    driver.get(race_info['race_url'])\n",
    "                    \n",
    "                    # Wait for race data to load\n",
    "                    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \".MeetingRaceTrap\")))\n",
    "                    \n",
    "                    # Extract race data\n",
    "                    race_data = extract_complete_race_data(driver, race_info)\n",
    "                    \n",
    "                    if race_data:\n",
    "                        all_race_data.extend(race_data)\n",
    "                        race_key = f\"{race_info['meeting_id']}_{race_info['race_id']}\"\n",
    "                        processed_races.add(race_key)\n",
    "                        \n",
    "                        # Save progress every 25 races to avoid data loss\n",
    "                        if i % 25 == 0:\n",
    "                            save_comprehensive_data(all_race_data, output_file)\n",
    "                except Exception:\n",
    "                    continue\n",
    "            \n",
    "            if verbose:\n",
    "                print()  # New line after progress\n",
    "        \n",
    "        # Final save\n",
    "        save_comprehensive_data(all_race_data, output_file)\n",
    "        \n",
    "        # Process the complete dataset - silence process_and_save_data output\n",
    "        process_and_save_data(output_file, processed_file, verbose=False)\n",
    "        \n",
    "        # Single line output at the end\n",
    "        print(f\"Scraping completed: {len(all_race_data)} total records for {len(set(row.get('dog_id', '') for row in all_race_data if row.get('dog_id')))} unique dogs\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Critical error during scraping: {e}\")\n",
    "        # Still try to save what we have\n",
    "        if all_race_data:\n",
    "            save_comprehensive_data(all_race_data, output_file)\n",
    "            process_and_save_data(output_file, processed_file, verbose=False)\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return len(all_race_data)\n",
    "\n",
    "def fast_get_dog_race_urls(driver, dog_id, wait, verbose=False):\n",
    "    \"\"\"Get race URLs with minimal delays and improved error handling\"\"\"\n",
    "    race_urls = []\n",
    "    debug_mode = (dog_id == \"607694\")  # Enable extra logging for problematic dog\n",
    "    \n",
    "    try:\n",
    "        profile_url = f\"https://www.gbgb.org.uk/greyhound-profile/?greyhoundId={dog_id}\"\n",
    "        if verbose or debug_mode:\n",
    "            print(f\"   Accessing dog profile: {profile_url}\")\n",
    "        driver.get(profile_url)\n",
    "        \n",
    "        # Check if dog exists by looking for specific elements or error message\n",
    "        if \"No greyhound found\" in driver.page_source or \"No results found\" in driver.page_source:\n",
    "            if verbose or debug_mode:\n",
    "                print(f\"   ‚ùå Dog ID {dog_id} not found on GBGB website\")\n",
    "            return race_urls\n",
    "        \n",
    "        # More flexible element detection - don't use wait.until which can time out\n",
    "        dog_exists = False\n",
    "        \n",
    "        # Use find_elements which doesn't throw exceptions if nothing is found\n",
    "        dog_name_elems = driver.find_elements(By.CSS_SELECTOR, \".GreyhoundProfile__name, h1, .greyhound-name\")\n",
    "        if dog_name_elems:\n",
    "            dog_exists = True\n",
    "            if debug_mode:\n",
    "                print(f\"   ‚úÖ Found dog name: {dog_name_elems[0].text}\")\n",
    "        \n",
    "        # Check for race table as additional verification\n",
    "        if not dog_exists:\n",
    "            race_tables = driver.find_elements(By.CSS_SELECTOR, \"table, .race-history, .results-table\")\n",
    "            if race_tables:\n",
    "                dog_exists = True\n",
    "                if debug_mode:\n",
    "                    print(f\"   ‚úÖ Found race tables: {len(race_tables)}\")\n",
    "                    \n",
    "        if not dog_exists:\n",
    "            if verbose or debug_mode:\n",
    "                print(\"   ‚ùå Could not verify dog profile exists\")\n",
    "            return race_urls\n",
    "        \n",
    "        # Quick cookie handling with minimal output\n",
    "        try:\n",
    "            cookie_buttons = driver.find_elements(By.CSS_SELECTOR, \"button.consent-btn, button.accept-cookies, .cookie-consent-btn\")\n",
    "            if cookie_buttons:\n",
    "                driver.execute_script(\"arguments[0].click();\", cookie_buttons[0])\n",
    "                time.sleep(0.5)\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        # SPECIAL HANDLING FOR PROBLEMATIC DOG ID - Use last resort method first \n",
    "        # This is now our primary method for this dog ID\n",
    "        if dog_id == \"607694\":\n",
    "            if debug_mode:\n",
    "                print(\"   üîç Using direct link extraction for problematic dog ID\")\n",
    "                \n",
    "            all_links = driver.find_elements(By.CSS_SELECTOR, \"a[href*='meeting'], a[href*='race']\")\n",
    "            if debug_mode:\n",
    "                print(f\"   Found {len(all_links)} potential race links\")\n",
    "                \n",
    "            for link in all_links:\n",
    "                try:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and ('meetingId' in href or 'raceId' in href):\n",
    "                        meeting_match = re.search(r'meetingId=(\\d+)', href)\n",
    "                        race_match = re.search(r'raceId=(\\d+)', href)\n",
    "                        meeting_id = meeting_match.group(1) if meeting_match else \"\"\n",
    "                        race_id = race_match.group(1) if race_match else \"\"\n",
    "                        \n",
    "                        if meeting_id and race_id:\n",
    "                            race_info = {\n",
    "                                'race_url': href,\n",
    "                                'meeting_id': meeting_id,\n",
    "                                'race_id': race_id,\n",
    "                                'race_date': \"\"  # Date will be filled later\n",
    "                            }\n",
    "                            race_urls.append(race_info)\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "            if race_urls and debug_mode:\n",
    "                print(f\"   ‚úÖ Found {len(race_urls)} race links for problematic dog ID\")\n",
    "                return race_urls\n",
    "        \n",
    "        # For all other dogs, continue with regular approach\n",
    "        # Function to directly extract race links from table - our primary approach now\n",
    "        def extract_race_links_from_table():\n",
    "            nonlocal race_urls\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            \n",
    "            # Try multiple selectors for tables containing race data\n",
    "            table_found = False\n",
    "            for table_selector in [\"table.race-history\", \"table\", \".race-history-table\", \".greyhound-races\", \".data-table\"]:\n",
    "                table = soup.select_one(table_selector)\n",
    "                if table:\n",
    "                    table_found = True\n",
    "                    if debug_mode:\n",
    "                        print(f\"   ‚úÖ Found race table using selector: {table_selector}\")\n",
    "                    \n",
    "                    # Extract all links from the table\n",
    "                    links = table.select(\"a[href*='meeting'], a[href*='race']\")\n",
    "                    if debug_mode:\n",
    "                        print(f\"   Found {len(links)} race links in table\")\n",
    "                        \n",
    "                    for link in links:\n",
    "                        href = link.get('href', '')\n",
    "                        if href and ('meetingId' in href or 'raceId' in href):\n",
    "                            if href.startswith('http'):\n",
    "                                full_url = href\n",
    "                            else:\n",
    "                                full_url = \"https://www.gbgb.org.uk\" + href\n",
    "                            \n",
    "                            meeting_match = re.search(r'meetingId=(\\d+)', full_url)\n",
    "                            race_match = re.search(r'raceId=(\\d+)', full_url)\n",
    "                            meeting_id = meeting_match.group(1) if meeting_match else \"\"\n",
    "                            race_id = race_match.group(1) if race_match else \"\"\n",
    "                            \n",
    "                            # Find date info nearby\n",
    "                            row = link.find_parent(\"tr\") or link.find_parent(\"div\")\n",
    "                            race_date = \"\"\n",
    "                            if row:\n",
    "                                date_elem = row.select_one(\".date, .race-date, td:first-child, .race-date-cell\")\n",
    "                                if date_elem:\n",
    "                                    race_date = date_elem.text.strip()\n",
    "                            \n",
    "                            race_info = {\n",
    "                                'race_url': full_url,\n",
    "                                'meeting_id': meeting_id,\n",
    "                                'race_id': race_id,\n",
    "                                'race_date': race_date\n",
    "                            }\n",
    "                            \n",
    "                            # Add to results if it has both IDs\n",
    "                            if meeting_id and race_id:\n",
    "                                race_urls.append(race_info)\n",
    "                    \n",
    "                    break\n",
    "            \n",
    "            return table_found\n",
    "        \n",
    "        # Always try direct table extraction first - especially for problematic dogs\n",
    "        table_found = extract_race_links_from_table()\n",
    "        if table_found and race_urls:\n",
    "            if debug_mode:\n",
    "                print(f\"   ‚úÖ Successfully extracted {len(race_urls)} races using direct table method\")\n",
    "            return race_urls\n",
    "        \n",
    "        # If no races found yet, try alternative extraction approaches\n",
    "        if not race_urls:\n",
    "            if debug_mode:\n",
    "                print(\"   Trying alternative race history formats...\")\n",
    "            \n",
    "            # Try to find race elements with various selectors\n",
    "            race_elements = []\n",
    "            for selector in [\n",
    "                \"table.race-history tr\", \n",
    "                \".race-history-row\", \n",
    "                \".race-entry\",\n",
    "                \".race-item\",\n",
    "                \".result-row\",\n",
    "                \"tr.race-data\",\n",
    "                \".race-record\",\n",
    "                \"tr\"  # Most generic - try last\n",
    "            ]:\n",
    "                elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                if elements:\n",
    "                    race_elements = elements\n",
    "                    if debug_mode:\n",
    "                        print(f\"   ‚úÖ Found {len(elements)} potential race elements using selector: {selector}\")\n",
    "                    break\n",
    "            \n",
    "            # Process found race elements\n",
    "            for race_elem in race_elements:\n",
    "                try:\n",
    "                    # Look for links in each element\n",
    "                    links = race_elem.find_elements(By.TAG_NAME, \"a\")\n",
    "                    for link in links:\n",
    "                        href = link.get_attribute(\"href\")\n",
    "                        if href and ('meetingId' in href or 'raceId' in href):\n",
    "                            meeting_match = re.search(r'meetingId=(\\d+)', href)\n",
    "                            race_match = re.search(r'raceId=(\\d+)', href)\n",
    "                            meeting_id = meeting_match.group(1) if meeting_match else \"\"\n",
    "                            race_id = race_match.group(1) if race_match else \"\"\n",
    "                            \n",
    "                            if meeting_id and race_id:\n",
    "                                # Try to find date near this link\n",
    "                                race_date = \"\"\n",
    "                                try:\n",
    "                                    # Get text of first cell/column if this is a table row\n",
    "                                    date_cells = race_elem.find_elements(By.CSS_SELECTOR, \"td:first-child, .date, .race-date\")\n",
    "                                    if date_cells:\n",
    "                                        race_date = date_cells[0].text.strip()\n",
    "                                except:\n",
    "                                    pass\n",
    "                                \n",
    "                                race_info = {\n",
    "                                    'race_url': href,\n",
    "                                    'meeting_id': meeting_id,\n",
    "                                    'race_id': race_id,\n",
    "                                    'race_date': race_date\n",
    "                                }\n",
    "                                race_urls.append(race_info)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if race_urls:\n",
    "                if debug_mode:\n",
    "                    print(f\"   ‚úÖ Extracted {len(race_urls)} race URLs using alternative format\")\n",
    "                return race_urls\n",
    "            \n",
    "            # Last resort: Try to find ANY links that might be race links\n",
    "            if not race_urls:\n",
    "                if debug_mode:\n",
    "                    print(\"   üîç Trying last resort: finding any race links on page\")\n",
    "                    \n",
    "                all_links = driver.find_elements(By.CSS_SELECTOR, \"a[href*='meeting'], a[href*='race']\")\n",
    "                for link in all_links:\n",
    "                    try:\n",
    "                        href = link.get_attribute(\"href\")\n",
    "                        if href and ('meetingId' in href or 'raceId' in href):\n",
    "                            meeting_match = re.search(r'meetingId=(\\d+)', href)\n",
    "                            race_match = re.search(r'raceId=(\\d+)', href)\n",
    "                            meeting_id = meeting_match.group(1) if meeting_match else \"\"\n",
    "                            race_id = race_match.group(1) if race_match else \"\"\n",
    "                            \n",
    "                            if meeting_id and race_id:\n",
    "                                race_info = {\n",
    "                                    'race_url': href,\n",
    "                                    'meeting_id': meeting_id,\n",
    "                                    'race_id': race_id,\n",
    "                                    'race_date': \"\"  # Can't determine date from random links\n",
    "                                }\n",
    "                                race_urls.append(race_info)\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if race_urls and debug_mode:\n",
    "                    print(f\"   ‚úÖ Found {len(race_urls)} race links with last resort method\")\n",
    "        \n",
    "        return race_urls\n",
    "    \n",
    "    except Exception as e:\n",
    "        if verbose or debug_mode:\n",
    "            print(f\"   ‚ùå Error in fast_get_dog_race_urls: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ff1d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during scraping: name 'fast_scrape_multiple_dogs' is not defined\n",
      "\n",
      "Running diagnostic on problematic dog ID...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'diagnose_dog_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Add diagnostic to check why the problematic dog might be failing\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRunning diagnostic on problematic dog ID...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mdiagnose_dog_id\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33m607694\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'diagnose_dog_id' is not defined"
     ]
    }
   ],
   "source": [
    "# SCRAPE SPECIFIC DOGS - with minimal output\n",
    "# Edit the dog_ids list below and run this cell\n",
    "\n",
    "dog_ids_to_scrape = [\n",
    "    \"621059\", \n",
    "]\n",
    "\n",
    "# Use try/except to ensure we continue even if one dog fails\n",
    "try:\n",
    "    total_records = fast_scrape_multiple_dogs(dog_ids_to_scrape, \"dogs3.csv\", \"dogs3_processed.csv\", verbose=True)\n",
    "    print(f\"‚úÖ Successfully scraped {total_records} records\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during scraping: {str(e)}\")\n",
    "    \n",
    "# Add diagnostic to check why the problematic dog might be failing\n",
    "print(\"\\nRunning diagnostic on problematic dog ID...\")\n",
    "diagnose_dog_id(\"607694\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36121026",
   "metadata": {},
   "source": [
    "## Scraping Notebook\n",
    "\n",
    "This notebook contains only the essential scraping functions:\n",
    "\n",
    "### Usage:\n",
    "1. **Scrape Specific Dogs**: Edit cell 4 with your dog IDs\n",
    "2. **Scrape Dog Range**: Edit cell 5 with start/end IDs  \n",
    "3. **Run Cell**: Data will be saved to `dogs3.csv`\n",
    "\n",
    "### Features:\n",
    "- ‚úÖ Avoids duplicate scraping\n",
    "- üöÄ Optimized for speed\n",
    "- üíæ Auto-saves progress\n",
    "- üìä Shows detailed statistics\n",
    "\n",
    "For ML predictions, use the separate `ML_Predictor.ipynb` notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c389d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ SCRAPING DOG RANGE 607694-607694\n",
      "Loaded 9307 existing records, 1664 unique races\n",
      "Scraping 1 dogs...\n",
      "Processing dog 1/1: 607694Scraping 1 dogs...\n",
      "Processing dog 1/1: 607694   Accessing dog profile: https://www.gbgb.org.uk/greyhound-profile/?greyhoundId=607694\n",
      "   Accessing dog profile: https://www.gbgb.org.uk/greyhound-profile/?greyhoundId=607694\n",
      "   ‚úÖ Found dog name: Sweet Soul Music\n",
      "   üîç Using direct link extraction for problematic dog ID\n",
      "   Found 22 potential race links\n",
      "   ‚úÖ Found 20 race links for problematic dog ID\n",
      "\n",
      "Found 0 new races to scrape\n",
      "   ‚úÖ Found dog name: Sweet Soul Music\n",
      "   üîç Using direct link extraction for problematic dog ID\n",
      "   Found 22 potential race links\n",
      "   ‚úÖ Found 20 race links for problematic dog ID\n",
      "\n",
      "Found 0 new races to scrape\n",
      "Scraping completed: 9307 total records for 3550 unique dogs\n",
      "Scraping completed: 9307 total records for 3550 unique dogs\n",
      "‚úÖ COMPLETED! Total records in database: 9307\n",
      "‚úÖ COMPLETED! Total records in database: 9307\n"
     ]
    }
   ],
   "source": [
    "# SCRAPE DOGS BY RANGE\n",
    "# Scrape dogs with IDs in a specific range\n",
    "\n",
    "start_id = 607694  # Start with our problematic dog ID\n",
    "end_id = 607695    # Just try one dog for testing\n",
    "\n",
    "dog_range = [str(i) for i in range(start_id, end_id)]\n",
    "print(f\"üöÄ SCRAPING DOG RANGE {start_id}-{end_id-1}\")\n",
    "\n",
    "total_records = fast_scrape_multiple_dogs(dog_range, \"dogs3.csv\", \"dogs3_processed.csv\", verbose=True)\n",
    "print(f\"‚úÖ COMPLETED! Total records in database: {total_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9b0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to diagnose issues with a specific dog ID\n",
    "def diagnose_dog_id(dog_id, verbose=True):\n",
    "    \"\"\"Direct diagnosis of issues with a specific dog ID\"\"\"\n",
    "    if verbose:\n",
    "        print(f\"Diagnosing dog ID: {dog_id}\")\n",
    "    \n",
    "    driver = create_fast_driver()\n",
    "    wait = WebDriverWait(driver, 10)  # Longer timeout for diagnosis\n",
    "    \n",
    "    try:\n",
    "        # Manual check of dog profile\n",
    "        profile_url = f\"https://www.gbgb.org.uk/greyhound-profile/?greyhoundId={dog_id}\"\n",
    "        if verbose:\n",
    "            print(f\"Accessing: {profile_url}\")\n",
    "        driver.get(profile_url)\n",
    "        \n",
    "        # Take a screenshot\n",
    "        screenshot_file = f\"dog_{dog_id}_screenshot.png\"\n",
    "        driver.save_screenshot(screenshot_file)\n",
    "        if verbose:\n",
    "            print(f\"Screenshot saved as: {screenshot_file}\")\n",
    "        \n",
    "        # Check dog name\n",
    "        try:\n",
    "            dog_name_elems = driver.find_elements(By.CSS_SELECTOR, \".GreyhoundProfile__name, h1, .greyhound-name\")\n",
    "            if dog_name_elems and verbose:\n",
    "                print(f\"Dog name found: {dog_name_elems[0].text}\")\n",
    "            elif verbose:\n",
    "                print(\"Dog name element not found\")\n",
    "                \n",
    "            # Check page title\n",
    "            if verbose:\n",
    "                print(f\"Page title: {driver.title}\")\n",
    "            \n",
    "            # Check if there's an error message\n",
    "            error_elems = driver.find_elements(By.CSS_SELECTOR, \".error-message, .alert, .message\")\n",
    "            if verbose:\n",
    "                for elem in error_elems:\n",
    "                    print(f\"Error message found: {elem.text}\")\n",
    "            \n",
    "            # Check for race history table\n",
    "            race_tables = driver.find_elements(By.CSS_SELECTOR, \"table, .race-history, .results-table\")\n",
    "            if race_tables and verbose:\n",
    "                print(f\"Race table found with {len(race_tables)} elements\")\n",
    "                \n",
    "                # Count rows\n",
    "                rows = driver.find_elements(By.CSS_SELECTOR, \"tr, .race-row, .result-row\")\n",
    "                print(f\"Found {len(rows)} potential race rows\")\n",
    "            elif verbose:\n",
    "                print(\"No race history table found\")\n",
    "            \n",
    "            # Extract page source and save for inspection\n",
    "            with open(f\"dog_{dog_id}_page.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(driver.page_source)\n",
    "            if verbose:\n",
    "                print(f\"Page source saved to dog_{dog_id}_page.html\")\n",
    "            \n",
    "            # Try the race extraction\n",
    "            race_urls = fast_get_dog_race_urls(driver, dog_id, wait, verbose)\n",
    "            if verbose:\n",
    "                print(f\"Extracted {len(race_urls)} race URLs\")\n",
    "                \n",
    "                # Display the first few races found\n",
    "                if race_urls:\n",
    "                    print(\"\\nFirst few races found:\")\n",
    "                    for i, race in enumerate(race_urls[:3]):\n",
    "                        print(f\"  Race {i+1}: Meeting ID={race['meeting_id']}, Race ID={race['race_id']}\")\n",
    "                        print(f\"  URL: {race['race_url']}\")\n",
    "            \n",
    "            return race_urls\n",
    "            \n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Error during diagnosis: {e}\")\n",
    "            return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "        if verbose:\n",
    "            print(\"Driver closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf081492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing scraping for dog ID: 607694\n",
      "   Accessing dog profile: https://www.gbgb.org.uk/greyhound-profile/?greyhoundId=607694\n",
      "   Accessing dog profile: https://www.gbgb.org.uk/greyhound-profile/?greyhoundId=607694\n",
      "   ‚úÖ Found dog name: Sweet Soul Music\n",
      "   üîç Using direct link extraction for problematic dog ID\n",
      "   Found 22 potential race links\n",
      "   ‚úÖ Found 20 race links for problematic dog ID\n",
      "üìä Found 20 races for dog 607694\n",
      "\n",
      "Sample of races found:\n",
      "  1. Meeting ID: 426921, Race ID: 1115765\n",
      "     URL: https://www.gbgb.org.uk/meeting/?meetingId=426921&raceId=1115765\n",
      "     Date: \n",
      "  2. Meeting ID: 426548, Race ID: 1114075\n",
      "     URL: https://www.gbgb.org.uk/meeting/?meetingId=426548&raceId=1114075\n",
      "     Date: \n",
      "  3. Meeting ID: 425995, Race ID: 1111481\n",
      "     URL: https://www.gbgb.org.uk/meeting/?meetingId=425995&raceId=1111481\n",
      "     Date: \n",
      "  4. Meeting ID: 425687, Race ID: 1109872\n",
      "     URL: https://www.gbgb.org.uk/meeting/?meetingId=425687&raceId=1109872\n",
      "     Date: \n",
      "  5. Meeting ID: 425212, Race ID: 1107894\n",
      "     URL: https://www.gbgb.org.uk/meeting/?meetingId=425212&raceId=1107894\n",
      "     Date: \n",
      "  ... and 15 more races\n",
      "   ‚úÖ Found dog name: Sweet Soul Music\n",
      "   üîç Using direct link extraction for problematic dog ID\n",
      "   Found 22 potential race links\n",
      "   ‚úÖ Found 20 race links for problematic dog ID\n",
      "üìä Found 20 races for dog 607694\n",
      "\n",
      "Sample of races found:\n",
      "  1. Meeting ID: 426921, Race ID: 1115765\n",
      "     URL: https://www.gbgb.org.uk/meeting/?meetingId=426921&raceId=1115765\n",
      "     Date: \n",
      "  2. Meeting ID: 426548, Race ID: 1114075\n",
      "     URL: https://www.gbgb.org.uk/meeting/?meetingId=426548&raceId=1114075\n",
      "     Date: \n",
      "  3. Meeting ID: 425995, Race ID: 1111481\n",
      "     URL: https://www.gbgb.org.uk/meeting/?meetingId=425995&raceId=1111481\n",
      "     Date: \n",
      "  4. Meeting ID: 425687, Race ID: 1109872\n",
      "     URL: https://www.gbgb.org.uk/meeting/?meetingId=425687&raceId=1109872\n",
      "     Date: \n",
      "  5. Meeting ID: 425212, Race ID: 1107894\n",
      "     URL: https://www.gbgb.org.uk/meeting/?meetingId=425212&raceId=1107894\n",
      "     Date: \n",
      "  ... and 15 more races\n",
      "üèÅ Driver closed\n",
      "üèÅ Driver closed\n"
     ]
    }
   ],
   "source": [
    "# Test scraping for problematic dog ID 607694\n",
    "test_dog_id = \"607694\"\n",
    "print(f\"üîç Testing scraping for dog ID: {test_dog_id}\")\n",
    "\n",
    "driver = create_fast_driver()\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "try:\n",
    "    # Run the enhanced scraping\n",
    "    race_urls = fast_get_dog_race_urls(driver, test_dog_id, wait, verbose=True)\n",
    "    print(f\"üìä Found {len(race_urls)} races for dog {test_dog_id}\")\n",
    "    \n",
    "    # Show sample of races found\n",
    "    if race_urls:\n",
    "        print(\"\\nSample of races found:\")\n",
    "        for i, race in enumerate(race_urls[:5]):\n",
    "            print(f\"  {i+1}. Meeting ID: {race['meeting_id']}, Race ID: {race['race_id']}\")\n",
    "            print(f\"     URL: {race['race_url']}\")\n",
    "            print(f\"     Date: {race['race_date']}\")\n",
    "        \n",
    "        if len(race_urls) > 5:\n",
    "            print(f\"  ... and {len(race_urls) - 5} more races\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "    print(\"üèÅ Driver closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
