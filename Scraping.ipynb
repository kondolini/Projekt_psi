{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8161a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraping functions loaded!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def create_fast_driver():\n",
    "    \"\"\"Create optimized Chrome driver for speed\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--disable-images\")\n",
    "    chrome_options.add_argument(\"--disable-plugins\") \n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\")\n",
    "    \n",
    "    service = Service()\n",
    "    return webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def extract_complete_race_data(driver, race_info):\n",
    "    \"\"\"Extract all dogs' data from a race page\"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        \n",
    "        # Extract race metadata\n",
    "        race_date_elem = soup.select_one(\".Meeting__header__date\")\n",
    "        race_date = race_date_elem.text.strip() if race_date_elem else race_info.get('race_date', '')\n",
    "        \n",
    "        track_elem = soup.select_one(\".Meeting__header__title__meta\")\n",
    "        track = track_elem.text.strip() if track_elem else \"\"\n",
    "        \n",
    "        # Extract race details from race header\n",
    "        race_time = \"\"\n",
    "        race_class = \"\"\n",
    "        race_distance = \"\"\n",
    "        race_prizes = \"\"\n",
    "        \n",
    "        race_header = soup.select_one(\".MeetingRace__header\")\n",
    "        if race_header:\n",
    "            time_elem = race_header.select_one(\".MeetingRace__time\")\n",
    "            if time_elem:\n",
    "                race_time = time_elem.text.strip()\n",
    "            \n",
    "            class_elem = race_header.select_one(\".MeetingRace__class\")\n",
    "            if class_elem:\n",
    "                race_class = class_elem.text.strip().replace(\"|\", \"\").strip()\n",
    "            \n",
    "            distance_elem = race_header.select_one(\".MeetingRace__distance\")\n",
    "            if distance_elem:\n",
    "                race_distance = distance_elem.text.strip()\n",
    "                \n",
    "            prizes_elem = race_header.select_one(\".MeetingRace__prizes\")\n",
    "            if prizes_elem:\n",
    "                race_prizes = prizes_elem.text.strip()\n",
    "        \n",
    "        # Extract all dogs from race\n",
    "        dogs_data = []\n",
    "        dog_rows = soup.select(\".MeetingRaceTrap\")\n",
    "        \n",
    "        for dog_row in dog_rows:\n",
    "            dog_data = extract_single_dog_data(dog_row)\n",
    "            if dog_data:\n",
    "                # Add race metadata to each dog\n",
    "                dog_data.update({\n",
    "                    'meeting_id': race_info['meeting_id'],\n",
    "                    'race_id': race_info['race_id'],\n",
    "                    'race_url': race_info['race_url'],\n",
    "                    'race_date': race_date,\n",
    "                    'track': track,\n",
    "                    'race_time': race_time,\n",
    "                    'race_class': race_class,\n",
    "                    'race_distance': race_distance,\n",
    "                    'race_prizes': race_prizes\n",
    "                })\n",
    "                dogs_data.append(dog_data)\n",
    "        \n",
    "        return dogs_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def extract_single_dog_data(dog_row):\n",
    "    \"\"\"Extract data for a single dog from its row\"\"\"\n",
    "    try:\n",
    "        data = {}\n",
    "        \n",
    "        # Position\n",
    "        pos_elem = dog_row.select_one(\".MeetingRaceTrap__pos\")\n",
    "        data['position'] = pos_elem.text.strip() if pos_elem else \"\"\n",
    "        \n",
    "        # Dog name and ID\n",
    "        greyhound_elem = dog_row.select_one(\".MeetingRaceTrap__greyhound\")\n",
    "        data['dog_name'] = greyhound_elem.text.strip() if greyhound_elem else \"\"\n",
    "        \n",
    "        # Extract dog ID from greyhound link\n",
    "        if greyhound_elem and greyhound_elem.get('href'):\n",
    "            href = greyhound_elem['href']\n",
    "            dog_id_match = re.search(r'greyhoundId=(\\d+)', href)\n",
    "            data['dog_id'] = dog_id_match.group(1) if dog_id_match else \"\"\n",
    "        else:\n",
    "            data['dog_id'] = \"\"\n",
    "        \n",
    "        # Trainer\n",
    "        trainer_elem = dog_row.select_one(\".MeetingRaceTrap__trainer\")\n",
    "        data['trainer'] = trainer_elem.text.strip() if trainer_elem else \"\"\n",
    "        \n",
    "        # Comments/Remarks\n",
    "        comment_elem = dog_row.select_one(\".MeetingRaceTrap__comment\")\n",
    "        data['comments'] = comment_elem.text.strip() if comment_elem else \"\"\n",
    "        \n",
    "        # Starting Price\n",
    "        sp_elem = dog_row.select_one(\".MeetingRaceTrap__sp\")\n",
    "        data['starting_price'] = sp_elem.text.strip() if sp_elem else \"\"\n",
    "        \n",
    "        # Time (S)\n",
    "        time_s_elem = dog_row.select_one(\".MeetingRaceTrap__timeS\")\n",
    "        data['time_s'] = time_s_elem.text.strip() if time_s_elem else \"\"\n",
    "        \n",
    "        # Time (Distance)\n",
    "        time_dist_elem = dog_row.select_one(\".MeetingRaceTrap__timeDistance\")\n",
    "        data['time_distance'] = time_dist_elem.text.strip() if time_dist_elem else \"\"\n",
    "        \n",
    "        # Extract trap number from trap image\n",
    "        trap_elem = dog_row.select_one(\".MeetingRaceTrap__trap img\")\n",
    "        if trap_elem and trap_elem.get('src'):\n",
    "            trap_match = re.search(r'icn-(\\d+)', trap_elem['src'])\n",
    "            data['trap'] = trap_match.group(1) if trap_match else \"\"\n",
    "        else:\n",
    "            data['trap'] = \"\"\n",
    "        \n",
    "        # Extract breeding info from hound profile\n",
    "        profile_elem = dog_row.select_one(\".MeetingRaceTrap__houndProfile\")\n",
    "        if profile_elem:\n",
    "            profile_text = profile_elem.text.strip()\n",
    "            data['breeding_info'] = profile_text\n",
    "            \n",
    "            # Parse breeding info: \"Oct-2020 | 34.4 | d - bd | Ballymac Best - Ballykett Beauty\"\n",
    "            parts = [p.strip() for p in profile_text.split('|')]\n",
    "            \n",
    "            if len(parts) >= 1:\n",
    "                data['birth_date'] = parts[0]\n",
    "            if len(parts) >= 2:\n",
    "                data['weight'] = parts[1]\n",
    "            if len(parts) >= 3:\n",
    "                data['color'] = parts[2]\n",
    "            if len(parts) >= 4:\n",
    "                # Extract sire and dam from \"Sire - Dam\" format\n",
    "                parents = parts[3]\n",
    "                if ' - ' in parents:\n",
    "                    sire, dam = parents.split(' - ', 1)\n",
    "                    data['sire'] = sire.strip()\n",
    "                    data['dam'] = dam.strip()\n",
    "                else:\n",
    "                    data['sire'] = \"\"\n",
    "                    data['dam'] = \"\"\n",
    "        else:\n",
    "            data['breeding_info'] = \"\"\n",
    "            data['birth_date'] = \"\"\n",
    "            data['weight'] = \"\"\n",
    "            data['color'] = \"\"\n",
    "            data['sire'] = \"\"\n",
    "            data['dam'] = \"\"\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "def save_comprehensive_data(all_race_data, filename=\"dogs3.csv\"):\n",
    "    \"\"\"Save comprehensive race data to CSV sorted by race_id\"\"\"\n",
    "    if not all_race_data:\n",
    "        return\n",
    "    \n",
    "    # Sort by race_id (convert to int for proper sorting)\n",
    "    all_race_data.sort(key=lambda x: int(x.get('race_id', 0)) if x.get('race_id', '').isdigit() else 0)\n",
    "    \n",
    "    fieldnames = [\n",
    "        'meeting_id', 'race_id', 'race_date', 'track', 'race_time', 'race_class', \n",
    "        'race_distance', 'race_prizes', 'position', 'trap', 'dog_id', 'dog_name', \n",
    "        'trainer', 'comments', 'starting_price', 'time_s', 'time_distance',\n",
    "        'birth_date', 'weight', 'color', 'sire', 'dam', 'breeding_info', 'race_url'\n",
    "    ]\n",
    "    \n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for race in all_race_data:\n",
    "            writer.writerow(race)\n",
    "\n",
    "print(\"✅ Scraping functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7341d1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_data(input_file=\"dogs3.csv\", output_file=\"dogs3_processed.csv\", verbose=False):\n",
    "    \"\"\"Process raw scraped data and save it to the processed CSV file\"\"\"\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    from datetime import datetime\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Processing data from {input_file} to {output_file}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load the raw data\n",
    "        df = pd.read_csv(input_file)\n",
    "        \n",
    "        # Make a copy for processing\n",
    "        processed_df = df.copy()\n",
    "        \n",
    "        # Process data - similar to what your prediction script does\n",
    "        # Handle missing values\n",
    "        processed_df = processed_df.fillna({\n",
    "            'birth_date': 'Unknown',\n",
    "            'weight': '0.0',\n",
    "            'time_s': '0.00',\n",
    "            'time_distance': '0.00 (0)',\n",
    "            'trap': '0',\n",
    "            'position': '0',\n",
    "            'comments': '',\n",
    "            'race_distance': '0m'\n",
    "        })\n",
    "        \n",
    "        # Extract distance in meters\n",
    "        processed_df['distance_meters'] = processed_df['race_distance'].apply(\n",
    "            lambda x: int(re.search(r'(\\d+)', str(x)).group(1)) if isinstance(x, str) and re.search(r'(\\d+)', str(x)) else 0\n",
    "        )\n",
    "        \n",
    "        # Convert position to numeric\n",
    "        processed_df['position_numeric'] = processed_df['position'].apply(\n",
    "            lambda x: int(re.search(r'(\\d+)', str(x)).group(1)) if isinstance(x, str) and re.search(r'(\\d+)', str(x)) else 99\n",
    "        )\n",
    "        \n",
    "        # Extract weight as float\n",
    "        processed_df['weight_kg'] = processed_df['weight'].apply(\n",
    "            lambda x: float(str(x).replace(',', '.')) if isinstance(x, str) and re.search(r'^\\d+\\.?\\d*$', x.replace(',', '.')) else 0.0\n",
    "        )\n",
    "        \n",
    "        # Convert trap to numeric\n",
    "        processed_df['trap_numeric'] = processed_df['trap'].apply(\n",
    "            lambda x: int(x) if str(x).isdigit() else 0\n",
    "        )\n",
    "        \n",
    "        # Calculate age at race time\n",
    "        def calculate_age(birth_date_str):\n",
    "            if not isinstance(birth_date_str, str) or birth_date_str == 'Unknown':\n",
    "                return 3.0  # Default age\n",
    "                \n",
    "            try:\n",
    "                # Extract year from birth date (format: \"Aug-2018\")\n",
    "                birth_year = int(birth_date_str.split('-')[1]) if '-' in birth_date_str else 0\n",
    "                if birth_year == 0:\n",
    "                    return 3.0\n",
    "                \n",
    "                # Use current year as race year for simplicity\n",
    "                current_year = datetime.now().year\n",
    "                age = current_year - birth_year\n",
    "                return age if 1 <= age <= 7 else 3.0\n",
    "            except:\n",
    "                return 3.0\n",
    "                \n",
    "        processed_df['age_at_race'] = processed_df['birth_date'].apply(calculate_age)\n",
    "        \n",
    "        # Extract starting price as odds ratio\n",
    "        def parse_starting_price(sp):\n",
    "            if not isinstance(sp, str):\n",
    "                return 0.0\n",
    "            try:\n",
    "                if '/' in sp:\n",
    "                    num, denom = sp.split('/')\n",
    "                    if 'F' in denom:  # Handle cases like \"5/4F\"\n",
    "                        denom = denom.replace('F', '')\n",
    "                    return float(num) / float(denom) + 1.0\n",
    "                elif sp.lower() == 'evs' or sp.lower() == 'evens':\n",
    "                    return 2.0\n",
    "                else:\n",
    "                    return 0.0\n",
    "            except:\n",
    "                return 0.0\n",
    "                \n",
    "        processed_df['odds_ratio'] = processed_df['starting_price'].apply(parse_starting_price)\n",
    "        \n",
    "        # Extract time in seconds\n",
    "        processed_df['time_seconds'] = processed_df['time_s'].apply(\n",
    "            lambda x: float(x) if isinstance(x, str) and x.replace('.', '', 1).isdigit() else 0.0\n",
    "        )\n",
    "        \n",
    "        # Create \"won_race\" target variable\n",
    "        processed_df['won_race'] = (processed_df['position_numeric'] == 1).astype(int)\n",
    "        \n",
    "        # Extract primary color\n",
    "        def extract_primary_color(color_str):\n",
    "            if not isinstance(color_str, str):\n",
    "                return 'unknown'\n",
    "            color_str = color_str.lower()\n",
    "            colors = ['bk', 'bd', 'be', 'wbk', 'f', 'dkbd', 'wbd', 'bew', 'bebd', 'wbe']\n",
    "            for color in colors:\n",
    "                if color in color_str:\n",
    "                    return color\n",
    "            return 'other'\n",
    "            \n",
    "        processed_df['primary_color'] = processed_df['color'].apply(extract_primary_color)\n",
    "        \n",
    "        # Process comments\n",
    "        processed_df['comment_early_pace'] = processed_df['comments'].apply(\n",
    "            lambda x: 1 if isinstance(x, str) and any(term in x.upper() for term in ['EP', 'QAW', 'VQAW']) else 0\n",
    "        )\n",
    "        \n",
    "        processed_df['comment_led'] = processed_df['comments'].apply(\n",
    "            lambda x: 1 if isinstance(x, str) and any(term in x.upper() for term in ['LD', 'LED', 'ALT']) else 0\n",
    "        )\n",
    "        \n",
    "        processed_df['comment_crowded'] = processed_df['comments'].apply(\n",
    "            lambda x: 1 if isinstance(x, str) and any(term in x.upper() for term in ['CRD', 'BMP', 'CROWD']) else 0\n",
    "        )\n",
    "        \n",
    "        processed_df['comment_strong_finish'] = processed_df['comments'].apply(\n",
    "            lambda x: 1 if isinstance(x, str) and any(term in x.upper() for term in ['FINSTR', 'RANON', 'STYD']) else 0\n",
    "        )\n",
    "        \n",
    "        processed_df['comment_middle_runner'] = processed_df['comments'].apply(\n",
    "            lambda x: 1 if isinstance(x, str) and any(term in x.upper() for term in ['MID', 'MIDDLE']) else 0\n",
    "        )\n",
    "        \n",
    "        processed_df['comment_rails_runner'] = processed_df['comments'].apply(\n",
    "            lambda x: 1 if isinstance(x, str) and any(term in x.upper() for term in ['RLS', 'RAILS']) else 0\n",
    "        )\n",
    "        \n",
    "        processed_df['comment_wide_runner'] = processed_df['comments'].apply(\n",
    "            lambda x: 1 if isinstance(x, str) and any(term in x.upper() for term in ['WIDE', 'W']) else 0\n",
    "        )\n",
    "        \n",
    "        # Process trap track bias\n",
    "        trap_track_stats = processed_df.groupby(['track', 'trap_numeric']).agg(\n",
    "            win_rate=('won_race', 'mean'),\n",
    "            count=('won_race', 'count')\n",
    "        ).reset_index()\n",
    "        \n",
    "        trap_track_stats = trap_track_stats[trap_track_stats['count'] >= 5]\n",
    "        \n",
    "        trap_track_map = {}\n",
    "        for _, row in trap_track_stats.iterrows():\n",
    "            track = row['track']\n",
    "            trap = row['trap_numeric']\n",
    "            win_rate = row['win_rate']\n",
    "            \n",
    "            if track not in trap_track_map:\n",
    "                trap_track_map[track] = {}\n",
    "            trap_track_map[track][trap] = win_rate\n",
    "        \n",
    "        def get_trap_track_bias(row):\n",
    "            track = row['track']\n",
    "            trap = row['trap_numeric']\n",
    "            \n",
    "            if track in trap_track_map and trap in trap_track_map[track]:\n",
    "                return trap_track_map[track][trap]\n",
    "            return 0.15  # Default win rate\n",
    "            \n",
    "        processed_df['trap_track_bias'] = processed_df.apply(get_trap_track_bias, axis=1)\n",
    "        \n",
    "        # Distance preference\n",
    "        distance_ranges = [(0, 300), (301, 500), (501, 700), (701, 1000)]\n",
    "        \n",
    "        for min_dist, max_dist in distance_ranges:\n",
    "            range_name = f\"dist_range_{min_dist}_{max_dist}m\"\n",
    "            processed_df[range_name] = ((processed_df['distance_meters'] >= min_dist) & \n",
    "                                     (processed_df['distance_meters'] <= max_dist)).astype(int)\n",
    "        \n",
    "        # Add grade_numeric feature\n",
    "        def get_grade_numeric(race_class):\n",
    "            match = re.match(r'([A|D|S])(\\d+)', str(race_class).upper())\n",
    "            if match:\n",
    "                grade_type, grade_num = match.groups()\n",
    "                return int(grade_num)\n",
    "            if str(race_class).upper() in ['OR', 'OPEN', 'INV', 'IT']:\n",
    "                return 0\n",
    "            return 99\n",
    "            \n",
    "        processed_df['grade_numeric'] = processed_df['race_class'].apply(get_grade_numeric)\n",
    "        \n",
    "        # Save the processed data\n",
    "        processed_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Error processing data: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f288f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_scrape_multiple_dogs(dog_ids, output_file=\"dogs3.csv\", processed_file=\"dogs3_processed.csv\", batch_size=50, verbose=False):\n",
    "    \"\"\"\n",
    "    Optimized scraping with minimal delays and better error handling\n",
    "    dog_ids: list of dog IDs to scrape\n",
    "    output_file: CSV file to save results\n",
    "    processed_file: CSV file to save processed results\n",
    "    \"\"\"\n",
    "    \n",
    "    processed_races = set()\n",
    "    all_race_data = []\n",
    "    \n",
    "    # Load existing data to avoid reprocessing\n",
    "    if os.path.exists(output_file):\n",
    "        try:\n",
    "            with open(output_file, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                for row in reader:\n",
    "                    all_race_data.append(row)\n",
    "                    race_key = f\"{row.get('meeting_id', '')}_{row.get('race_id', '')}\"\n",
    "                    processed_races.add(race_key)\n",
    "            if verbose:\n",
    "                print(f\"Loaded {len(all_race_data)} existing records, {len(processed_races)} unique races\")\n",
    "        except Exception as e:\n",
    "            all_race_data = []\n",
    "            processed_races = set()\n",
    "\n",
    "    # Create optimized driver\n",
    "    driver = create_fast_driver()\n",
    "    wait = WebDriverWait(driver, 5)  # 5 second timeout\n",
    "    \n",
    "    try:\n",
    "        print(f\"Scraping {len(dog_ids)} dogs...\")\n",
    "        all_race_urls = set()\n",
    "        \n",
    "        # Phase 1: Collect all race URLs from all dogs\n",
    "        for i, dog_id in enumerate(dog_ids, 1):\n",
    "            if verbose or i % 5 == 0 or i == len(dog_ids):\n",
    "                print(f\"\\rProcessing dog {i}/{len(dog_ids)}: {dog_id}\", end=\"\", flush=True)\n",
    "            \n",
    "            try:\n",
    "                race_urls = fast_get_dog_race_urls(driver, dog_id, wait, verbose)\n",
    "                \n",
    "                for race_info in race_urls:\n",
    "                    race_tuple = (\n",
    "                        race_info['race_url'],\n",
    "                        race_info['meeting_id'], \n",
    "                        race_info['race_id'],\n",
    "                        race_info['race_date']\n",
    "                    )\n",
    "                    all_race_urls.add(race_tuple)\n",
    "                    \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        print()  # New line after progress indicator\n",
    "        \n",
    "        # Phase 2: Filter out already processed races\n",
    "        new_races = []\n",
    "        for race_tuple in all_race_urls:\n",
    "            race_info = {\n",
    "                'race_url': race_tuple[0],\n",
    "                'meeting_id': race_tuple[1],\n",
    "                'race_id': race_tuple[2],\n",
    "                'race_date': race_tuple[3]\n",
    "            }\n",
    "            \n",
    "            race_key = f\"{race_info['meeting_id']}_{race_info['race_id']}\"\n",
    "            if race_key not in processed_races:\n",
    "                new_races.append(race_info)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Found {len(new_races)} new races to scrape\")\n",
    "        \n",
    "        # Phase 3: Scrape new race data\n",
    "        if new_races:\n",
    "            for i, race_info in enumerate(new_races, 1):\n",
    "                if verbose and i % 10 == 0:\n",
    "                    print(f\"\\rScraping races: {i}/{len(new_races)}\", end=\"\", flush=True)\n",
    "                \n",
    "                try:\n",
    "                    # Navigate to race page\n",
    "                    driver.get(race_info['race_url'])\n",
    "                    \n",
    "                    # Wait for race data to load\n",
    "                    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \".MeetingRaceTrap\")))\n",
    "                    \n",
    "                    # Extract race data\n",
    "                    race_data = extract_complete_race_data(driver, race_info)\n",
    "                    \n",
    "                    if race_data:\n",
    "                        all_race_data.extend(race_data)\n",
    "                        race_key = f\"{race_info['meeting_id']}_{race_info['race_id']}\"\n",
    "                        processed_races.add(race_key)\n",
    "                        \n",
    "                        # Save progress every 25 races to avoid data loss\n",
    "                        if i % 25 == 0:\n",
    "                            save_comprehensive_data(all_race_data, output_file)\n",
    "                except Exception:\n",
    "                    continue\n",
    "            \n",
    "            if verbose:\n",
    "                print()  # New line after progress\n",
    "        \n",
    "        # Final save\n",
    "        save_comprehensive_data(all_race_data, output_file)\n",
    "        \n",
    "        # Process the complete dataset - silence process_and_save_data output\n",
    "        process_and_save_data(output_file, processed_file, verbose=False)\n",
    "        \n",
    "        # Single line output at the end\n",
    "        print(f\"Scraping completed: {len(all_race_data)} total records for {len(set(row.get('dog_id', '') for row in all_race_data if row.get('dog_id')))} unique dogs\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Critical error during scraping: {e}\")\n",
    "        # Still try to save what we have\n",
    "        if all_race_data:\n",
    "            save_comprehensive_data(all_race_data, output_file)\n",
    "            process_and_save_data(output_file, processed_file, verbose=False)\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return len(all_race_data)\n",
    "\n",
    "def fast_get_dog_race_urls(driver, dog_id, wait, verbose=False):\n",
    "    \"\"\"Get race URLs with proper pagination handling to collect ALL races\"\"\"\n",
    "    race_urls = []\n",
    "    debug_mode = (dog_id == \"607694\") or verbose\n",
    "    \n",
    "    try:\n",
    "        profile_url = f\"https://www.gbgb.org.uk/greyhound-profile/?greyhoundId={dog_id}\"\n",
    "        if verbose or debug_mode:\n",
    "            print(f\"   Accessing dog profile: {profile_url}\")\n",
    "        driver.get(profile_url)\n",
    "        \n",
    "        # Check if dog exists by looking for specific elements or error message\n",
    "        if \"No greyhound found\" in driver.page_source or \"No results found\" in driver.page_source:\n",
    "            if verbose or debug_mode:\n",
    "                print(f\"   ❌ Dog ID {dog_id} not found on GBGB website\")\n",
    "            return race_urls\n",
    "        \n",
    "        # More flexible element detection\n",
    "        dog_exists = False\n",
    "        dog_name = \"Unknown\"\n",
    "        \n",
    "        # Get dog name for better tracking\n",
    "        dog_name_elems = driver.find_elements(By.CSS_SELECTOR, \".GreyhoundProfile__name, h1, .greyhound-name\")\n",
    "        if dog_name_elems:\n",
    "            dog_exists = True\n",
    "            dog_name = dog_name_elems[0].text.strip()\n",
    "            if debug_mode:\n",
    "                print(f\"   ✅ Found dog: {dog_name}\")\n",
    "        \n",
    "        # Check for race table as additional verification\n",
    "        if not dog_exists:\n",
    "            race_tables = driver.find_elements(By.CSS_SELECTOR, \"table, .race-history, .results-table\")\n",
    "            if race_tables:\n",
    "                dog_exists = True\n",
    "                if debug_mode:\n",
    "                    print(f\"   ✅ Found race tables: {len(race_tables)}\")\n",
    "                    \n",
    "        if not dog_exists:\n",
    "            if verbose or debug_mode:\n",
    "                print(\"   ❌ Could not verify dog profile exists\")\n",
    "            return race_urls\n",
    "        \n",
    "        # Quick cookie handling\n",
    "        try:\n",
    "            cookie_buttons = driver.find_elements(By.CSS_SELECTOR, \"button.consent-btn, button.accept-cookies, .cookie-consent-btn\")\n",
    "            if cookie_buttons:\n",
    "                driver.execute_script(\"arguments[0].click();\", cookie_buttons[0])\n",
    "                time.sleep(0.5)\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        # Set page size to maximum to reduce pagination\n",
    "        try:\n",
    "            from selenium.webdriver.support.ui import Select\n",
    "            select_elems = driver.find_elements(By.CSS_SELECTOR, \"select\")\n",
    "            page_size_set = False\n",
    "            \n",
    "            for select_elem in select_elems:\n",
    "                try:\n",
    "                    select = Select(select_elem)\n",
    "                    options = [option.text for option in select.options]\n",
    "                    \n",
    "                    # Look for typical page size options and select the largest\n",
    "                    for size in [\"All\", \"100\", \"50\", \"25\"]:\n",
    "                        if size in options:\n",
    "                            select.select_by_visible_text(size)\n",
    "                            if debug_mode:\n",
    "                                print(f\"   📄 Set page size to {size}\")\n",
    "                            page_size_set = True\n",
    "                            time.sleep(2)  # Wait for page refresh\n",
    "                            break\n",
    "                    \n",
    "                    if page_size_set:\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "        except Exception:\n",
    "            if debug_mode:\n",
    "                print(\"   ⚠️ Could not set page size\")\n",
    "        \n",
    "        # Function to extract races from current page\n",
    "        def extract_races_from_current_page():\n",
    "            page_race_urls = []\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            \n",
    "            # Try multiple methods to find race links\n",
    "            # Method 1: Look for race table rows\n",
    "            race_rows = soup.select(\".GreyhoundRow, tr\")\n",
    "            for row in race_rows:\n",
    "                race_link = row.select_one(\"a[href*='meeting'], a[href*='race']\")\n",
    "                if race_link and race_link.get('href'):\n",
    "                    href = race_link['href']\n",
    "                    if href.startswith('http'):\n",
    "                        full_url = href\n",
    "                    else:\n",
    "                        full_url = \"https://www.gbgb.org.uk\" + href\n",
    "                    \n",
    "                    meeting_match = re.search(r'meetingId=(\\d+)', full_url)\n",
    "                    race_match = re.search(r'raceId=(\\d+)', full_url)\n",
    "                    meeting_id = meeting_match.group(1) if meeting_match else \"\"\n",
    "                    race_id = race_match.group(1) if race_match else \"\"\n",
    "                    \n",
    "                    if meeting_id and race_id:\n",
    "                        # Find date info\n",
    "                        race_date = \"\"\n",
    "                        date_elem = row.select_one(\".GreyhoundRow__date, .date, .race-date, td:first-child\")\n",
    "                        if date_elem:\n",
    "                            race_date = date_elem.text.strip()\n",
    "                        \n",
    "                        race_info = {\n",
    "                            'race_url': full_url,\n",
    "                            'meeting_id': meeting_id,\n",
    "                            'race_id': race_id,\n",
    "                            'race_date': race_date\n",
    "                        }\n",
    "                        \n",
    "                        # Avoid duplicates\n",
    "                        race_key = f\"{meeting_id}_{race_id}\"\n",
    "                        if not any(f\"{r['meeting_id']}_{r['race_id']}\" == race_key for r in page_race_urls):\n",
    "                            page_race_urls.append(race_info)\n",
    "            \n",
    "            # Method 2: If no races found in table, try all links\n",
    "            if not page_race_urls:\n",
    "                all_links = soup.select(\"a[href*='meeting'], a[href*='race']\")\n",
    "                for link in all_links:\n",
    "                    href = link.get('href', '')\n",
    "                    if href and ('meetingId' in href or 'raceId' in href):\n",
    "                        if href.startswith('http'):\n",
    "                            full_url = href\n",
    "                        else:\n",
    "                            full_url = \"https://www.gbgb.org.uk\" + href\n",
    "                        \n",
    "                        meeting_match = re.search(r'meetingId=(\\d+)', full_url)\n",
    "                        race_match = re.search(r'raceId=(\\d+)', full_url)\n",
    "                        meeting_id = meeting_match.group(1) if meeting_match else \"\"\n",
    "                        race_id = race_match.group(1) if race_match else \"\"\n",
    "                        \n",
    "                        if meeting_id and race_id:\n",
    "                            race_info = {\n",
    "                                'race_url': full_url,\n",
    "                                'meeting_id': meeting_id,\n",
    "                                'race_id': race_id,\n",
    "                                'race_date': \"\"\n",
    "                            }\n",
    "                            \n",
    "                            race_key = f\"{meeting_id}_{race_id}\"\n",
    "                            if not any(f\"{r['meeting_id']}_{r['race_id']}\" == race_key for r in page_race_urls):\n",
    "                                page_race_urls.append(race_info)\n",
    "            \n",
    "            return page_race_urls\n",
    "        \n",
    "        # Extract races from first page\n",
    "        race_urls.extend(extract_races_from_current_page())\n",
    "        if debug_mode:\n",
    "            print(f\"   📋 Found {len(race_urls)} races on page 1\")\n",
    "        \n",
    "        # Determine total number of pages\n",
    "        max_page = 1\n",
    "        \n",
    "        # Method 1: Look for pagination elements\n",
    "        pagination_elements = driver.find_elements(By.CSS_SELECTOR, \n",
    "            \".pagination li, .pagination-item, a[data-page], .LiveResultsPagination__page, .page-link\")\n",
    "        \n",
    "        for elem in pagination_elements:\n",
    "            try:\n",
    "                page_text = elem.text.strip()\n",
    "                if page_text.isdigit():\n",
    "                    max_page = max(max_page, int(page_text))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Method 2: Look for \"Page X of Y\" text\n",
    "        if max_page == 1:\n",
    "            try:\n",
    "                page_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "                page_patterns = [\n",
    "                    r\"Page\\s+\\d+\\s+of\\s+(\\d+)\",\n",
    "                    r\"page\\s+\\d+\\s+of\\s+(\\d+)\",\n",
    "                    r\"Showing.*?of\\s+(\\d+)\\s+pages\",\n",
    "                    r\"(\\d+)\\s+pages?\"\n",
    "                ]\n",
    "                \n",
    "                for pattern in page_patterns:\n",
    "                    matches = re.search(pattern, page_text, re.IGNORECASE)\n",
    "                    if matches:\n",
    "                        max_page = int(matches.group(1))\n",
    "                        break\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Method 3: Keep clicking next until no more pages\n",
    "        if max_page == 1:\n",
    "            # Try to estimate by looking at total results\n",
    "            try:\n",
    "                result_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "                total_match = re.search(r'(\\d+)\\s+total|(\\d+)\\s+results|(\\d+)\\s+races', result_text, re.IGNORECASE)\n",
    "                if total_match:\n",
    "                    total_results = int(total_match.group(1) or total_match.group(2) or total_match.group(3))\n",
    "                    # Estimate pages (assuming ~20 results per page)\n",
    "                    max_page = max(1, (total_results + 19) // 20)\n",
    "                    if debug_mode:\n",
    "                        print(f\"   📊 Estimated {max_page} pages based on {total_results} total results\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if debug_mode:\n",
    "            print(f\"   📚 Detected {max_page} total pages\")\n",
    "        \n",
    "        # Navigate through remaining pages\n",
    "        current_page = 1\n",
    "        consecutive_failures = 0\n",
    "        \n",
    "        while current_page < max_page and consecutive_failures < 3:\n",
    "            target_page = current_page + 1\n",
    "            \n",
    "            if debug_mode:\n",
    "                print(f\"   🔄 Navigating to page {target_page}\")\n",
    "            \n",
    "            page_found = False\n",
    "            \n",
    "            # Method 1: Click specific page number\n",
    "            try:\n",
    "                # Refresh pagination elements\n",
    "                pagination_elements = driver.find_elements(By.CSS_SELECTOR, \n",
    "                    \".pagination li, .pagination-item, a[data-page], .LiveResultsPagination__page, .page-link\")\n",
    "                \n",
    "                for elem in pagination_elements:\n",
    "                    if elem.text.strip() == str(target_page):\n",
    "                        driver.execute_script(\"arguments[0].click();\", elem)\n",
    "                        page_found = True\n",
    "                        time.sleep(2)\n",
    "                        break\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Method 2: Click 'Next' button\n",
    "            if not page_found:\n",
    "                try:\n",
    "                    next_buttons = driver.find_elements(By.CSS_SELECTOR, \n",
    "                        \".next-page, .pagination-next, a.next, [aria-label='Next page'], .LiveResultsPagination__next\")\n",
    "                    \n",
    "                    for btn in next_buttons:\n",
    "                        if btn.is_enabled() and btn.is_displayed():\n",
    "                            driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                            page_found = True\n",
    "                            time.sleep(2)\n",
    "                            break\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Method 3: URL manipulation\n",
    "            if not page_found:\n",
    "                try:\n",
    "                    current_url = driver.current_url\n",
    "                    if \"page=\" in current_url:\n",
    "                        new_url = re.sub(r'page=\\d+', f'page={target_page}', current_url)\n",
    "                    else:\n",
    "                        separator = \"&\" if \"?\" in current_url else \"?\"\n",
    "                        new_url = f\"{current_url}{separator}page={target_page}\"\n",
    "                    \n",
    "                    driver.get(new_url)\n",
    "                    page_found = True\n",
    "                    time.sleep(2)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            if page_found:\n",
    "                # Extract races from this page\n",
    "                new_races = extract_races_from_current_page()\n",
    "                if debug_mode:\n",
    "                    print(f\"   📋 Found {len(new_races)} races on page {target_page}\")\n",
    "                \n",
    "                if new_races:\n",
    "                    race_urls.extend(new_races)\n",
    "                    current_page = target_page\n",
    "                    consecutive_failures = 0\n",
    "                else:\n",
    "                    # No new races found, might be at the end\n",
    "                    consecutive_failures += 1\n",
    "                    current_page = target_page\n",
    "            else:\n",
    "                consecutive_failures += 1\n",
    "                if debug_mode:\n",
    "                    print(f\"   ❌ Could not navigate to page {target_page}\")\n",
    "                break\n",
    "        \n",
    "        if debug_mode:\n",
    "            print(f\"   ✅ Total races found for {dog_name}: {len(race_urls)}\")\n",
    "        \n",
    "        return race_urls\n",
    "    \n",
    "    except Exception as e:\n",
    "        if verbose or debug_mode:\n",
    "            print(f\"   ❌ Error in fast_get_dog_race_urls: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30ff1d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9426 existing records, 1684 unique races\n",
      "Scraping 1 dogs...\n",
      "Processing dog 1/1: 621059   Accessing dog profile: https://www.gbgb.org.uk/greyhound-profile/?greyhoundId=621059\n",
      "\n",
      "Found 0 new races to scrape\n",
      "Scraping completed: 9426 total records for 3598 unique dogs\n",
      "✅ Successfully scraped 9426 records\n",
      "\n",
      "Running diagnostic on problematic dog ID...\n",
      "Diagnosing dog ID: 607694\n",
      "Accessing: https://www.gbgb.org.uk/greyhound-profile/?greyhoundId=607694\n",
      "Screenshot saved as: dog_607694_screenshot.png\n",
      "Dog name found: Sweet Soul Music\n",
      "Page title: Greyhound Profile | Greyhound Board of Great Britain\n",
      "Race table found with 2 elements\n",
      "Found 14 potential race rows\n",
      "Page source saved to dog_607694_page.html\n",
      "   Accessing dog profile: https://www.gbgb.org.uk/greyhound-profile/?greyhoundId=607694\n",
      "   ✅ Found race tables: 2\n",
      "   🔍 Using direct link extraction for problematic dog ID\n",
      "   Found 2 potential race links\n",
      "   ✅ Found race table using selector: table\n",
      "   Found 0 race links in table\n",
      "   Trying alternative race history formats...\n",
      "   ✅ Found 14 potential race elements using selector: tr\n",
      "   🔍 Trying last resort: finding any race links on page\n",
      "   ✅ Found 20 race links with last resort method\n",
      "Extracted 20 race URLs\n",
      "\n",
      "First few races found:\n",
      "  Race 1: Meeting ID=426921, Race ID=1115765\n",
      "  URL: https://www.gbgb.org.uk/meeting/?meetingId=426921&raceId=1115765\n",
      "  Race 2: Meeting ID=426548, Race ID=1114075\n",
      "  URL: https://www.gbgb.org.uk/meeting/?meetingId=426548&raceId=1114075\n",
      "  Race 3: Meeting ID=425995, Race ID=1111481\n",
      "  URL: https://www.gbgb.org.uk/meeting/?meetingId=425995&raceId=1111481\n",
      "Driver closed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'race_url': 'https://www.gbgb.org.uk/meeting/?meetingId=426921&raceId=1115765',\n",
       "  'meeting_id': '426921',\n",
       "  'race_id': '1115765',\n",
       "  'race_date': ''},\n",
       " {'race_url': 'https://www.gbgb.org.uk/meeting/?meetingId=426548&raceId=1114075',\n",
       "  'meeting_id': '426548',\n",
       "  'race_id': '1114075',\n",
       "  'race_date': ''},\n",
       " {'race_url': 'https://www.gbgb.org.uk/meeting/?meetingId=425995&raceId=1111481',\n",
       "  'meeting_id': '425995',\n",
       "  'race_id': '1111481',\n",
       "  'race_date': ''},\n",
       " {'race_url': 'https://www.gbgb.org.uk/meeting/?meetingId=425687&raceId=1109872',\n",
       "  'meeting_id': '425687',\n",
       "  'race_id': '1109872',\n",
       "  'race_date': ''},\n",
       " {'race_url': 'https://www.gbgb.org.uk/meeting/?meetingId=425212&raceId=1107894',\n",
       "  'meeting_id': '425212',\n",
       "  'race_id': '1107894',\n",
       "  'race_date': ''},\n",
       " {'race_url': 'https://www.gbgb.org.uk/meeting/?meetingId=425013&raceId=1106725',\n",
       "  'meeting_id': '425013',\n",
       "  'race_id': '1106725',\n",
       "  'race_date': ''},\n",
       " {'race_url': 'https://www.gbgb.org.uk/meeting/?meetingId=424578&raceId=1104593',\n",
       "  'meeting_id': '424578',\n",
       "  'race_id': '1104593',\n",
       "  'race_date': ''},\n",
       " {'race_url': 'https://www.gbgb.org.uk/meeting/?meetingId=424352&raceId=1103227',\n",
       "  'meeting_id': '424352',\n",
       "  'race_id': '1103227',\n",
       "  'race_date': ''},\n",
       " {'race_url': 'https://www.gbgb.org.uk/meeting/?meetingId=423965&raceId=1101988',\n",
       "  'meeting_id': '423965',\n",
       "  'race_id': '1101988',\n",
       "  'race_date': ''},\n",
       " {'race_url': 'https://www.gbgb.org.uk/meeting/?meetingId=423736&raceId=1100668',\n",
       "  'meeting_id': '423736',\n",
       "  'race_id': '1100668',\n",
       "  'race_date': ''},\n",
       " {'race_url': 'https://www.gbgb.org.uk/meeting/?meetingId=422975&raceId=1097567',\n",
       "  'meeting_id': '422975',\n",
       "  'race_id': '1097567',\n",
       "  'race_date': ''},\n",
       " {'race_url': 'https://www.gbgb.org.uk/meeting/?meetingId=422644&raceId=1095584',\n",
       "  'meeting_id': '422644',\n",
       "  'race_id': '1095584',\n",
       "  'race_date': ''},\n",
       " {'race_url': 'https://www.gbgb.org.uk/meeting/?meetingId=422454&raceId=1094490',\n",
       "  'meeting_id': '422454',\n",
       "  'race_id': '1094490',\n",
       "  'race_date': ''},\n",
       " {'race_url': 'https://www.gbgb.org.uk/meeting/?meetingId=422203&raceId=1093142',\n",
       "  'meeting_id': '422203',\n",
       "  'race_id': '1093142',\n",
       "  'race_date': ''},\n",
       " {'race_url': 'https://www.gbgb.org.uk/meeting/?meetingId=421538&raceId=1090410',\n",
       "  'meeting_id': '421538',\n",
       "  'race_id': '1090410',\n",
       "  'race_date': ''},\n",
       " {'race_url': 'https://www.gbgb.org.uk/meeting/?meetingId=421175&raceId=1088667',\n",
       "  'meeting_id': '421175',\n",
       "  'race_id': '1088667',\n",
       "  'race_date': ''},\n",
       " {'race_url': 'https://www.gbgb.org.uk/meeting/?meetingId=418508&raceId=1075803',\n",
       "  'meeting_id': '418508',\n",
       "  'race_id': '1075803',\n",
       "  'race_date': ''},\n",
       " {'race_url': 'https://www.gbgb.org.uk/meeting/?meetingId=418175&raceId=1074573',\n",
       "  'meeting_id': '418175',\n",
       "  'race_id': '1074573',\n",
       "  'race_date': ''},\n",
       " {'race_url': 'https://www.gbgb.org.uk/meeting/?meetingId=417687&raceId=1071940',\n",
       "  'meeting_id': '417687',\n",
       "  'race_id': '1071940',\n",
       "  'race_date': ''},\n",
       " {'race_url': 'https://www.gbgb.org.uk/meeting/?meetingId=417335&raceId=1069970',\n",
       "  'meeting_id': '417335',\n",
       "  'race_id': '1069970',\n",
       "  'race_date': ''}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SCRAPE SPECIFIC DOGS - with minimal output\n",
    "# Edit the dog_ids list below and run this cell\n",
    "\n",
    "dog_ids_to_scrape = [\n",
    "    \"621059\", \n",
    "]\n",
    "\n",
    "# Use try/except to ensure we continue even if one dog fails\n",
    "try:\n",
    "    total_records = fast_scrape_multiple_dogs(dog_ids_to_scrape, \"dogs3.csv\", \"dogs3_processed.csv\", verbose=True)\n",
    "    print(f\"✅ Successfully scraped {total_records} records\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during scraping: {str(e)}\")\n",
    "    \n",
    "# Add diagnostic to check why the problematic dog might be failing\n",
    "print(\"\\nRunning diagnostic on problematic dog ID...\")\n",
    "diagnose_dog_id(\"607694\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36121026",
   "metadata": {},
   "source": [
    "## Scraping Notebook\n",
    "\n",
    "This notebook contains only the essential scraping functions:\n",
    "\n",
    "### Usage:\n",
    "1. **Scrape Specific Dogs**: Edit cell 4 with your dog IDs\n",
    "2. **Scrape Dog Range**: Edit cell 5 with start/end IDs  \n",
    "3. **Run Cell**: Data will be saved to `dogs3.csv`\n",
    "\n",
    "### Features:\n",
    "- ✅ Avoids duplicate scraping\n",
    "- 🚀 Optimized for speed\n",
    "- 💾 Auto-saves progress\n",
    "- 📊 Shows detailed statistics\n",
    "\n",
    "For ML predictions, use the separate `ML_Predictor.ipynb` notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c389d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 SCRAPING DOG RANGE 607694-607694\n",
      "Loaded 9307 existing records, 1664 unique races\n",
      "Scraping 1 dogs...\n",
      "Processing dog 1/1: 607694Scraping 1 dogs...\n",
      "Processing dog 1/1: 607694   Accessing dog profile: https://www.gbgb.org.uk/greyhound-profile/?greyhoundId=607694\n",
      "   Accessing dog profile: https://www.gbgb.org.uk/greyhound-profile/?greyhoundId=607694\n",
      "   ✅ Found dog name: Sweet Soul Music\n",
      "   🔍 Using direct link extraction for problematic dog ID\n",
      "   Found 22 potential race links\n",
      "   ✅ Found 20 race links for problematic dog ID\n",
      "\n",
      "Found 0 new races to scrape\n",
      "   ✅ Found dog name: Sweet Soul Music\n",
      "   🔍 Using direct link extraction for problematic dog ID\n",
      "   Found 22 potential race links\n",
      "   ✅ Found 20 race links for problematic dog ID\n",
      "\n",
      "Found 0 new races to scrape\n",
      "Scraping completed: 9307 total records for 3550 unique dogs\n",
      "Scraping completed: 9307 total records for 3550 unique dogs\n",
      "✅ COMPLETED! Total records in database: 9307\n",
      "✅ COMPLETED! Total records in database: 9307\n"
     ]
    }
   ],
   "source": [
    "# SCRAPE DOGS BY RANGE\n",
    "# Scrape dogs with IDs in a specific range\n",
    "\n",
    "start_id = 607694  # Start with our problematic dog ID\n",
    "end_id = 607695    # Just try one dog for testing\n",
    "\n",
    "dog_range = [str(i) for i in range(start_id, end_id)]\n",
    "print(f\"🚀 SCRAPING DOG RANGE {start_id}-{end_id-1}\")\n",
    "\n",
    "total_records = fast_scrape_multiple_dogs(dog_range, \"dogs3.csv\", \"dogs3_processed.csv\", verbose=True)\n",
    "print(f\"✅ COMPLETED! Total records in database: {total_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a9b0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to diagnose issues with a specific dog ID\n",
    "def diagnose_dog_id(dog_id, verbose=True):\n",
    "    \"\"\"Direct diagnosis of issues with a specific dog ID\"\"\"\n",
    "    if verbose:\n",
    "        print(f\"Diagnosing dog ID: {dog_id}\")\n",
    "    \n",
    "    driver = create_fast_driver()\n",
    "    wait = WebDriverWait(driver, 10)  # Longer timeout for diagnosis\n",
    "    \n",
    "    try:\n",
    "        # Manual check of dog profile\n",
    "        profile_url = f\"https://www.gbgb.org.uk/greyhound-profile/?greyhoundId={dog_id}\"\n",
    "        if verbose:\n",
    "            print(f\"Accessing: {profile_url}\")\n",
    "        driver.get(profile_url)\n",
    "        \n",
    "        # Take a screenshot\n",
    "        screenshot_file = f\"dog_{dog_id}_screenshot.png\"\n",
    "        driver.save_screenshot(screenshot_file)\n",
    "        if verbose:\n",
    "            print(f\"Screenshot saved as: {screenshot_file}\")\n",
    "        \n",
    "        # Check dog name\n",
    "        try:\n",
    "            dog_name_elems = driver.find_elements(By.CSS_SELECTOR, \".GreyhoundProfile__name, h1, .greyhound-name\")\n",
    "            if dog_name_elems and verbose:\n",
    "                print(f\"Dog name found: {dog_name_elems[0].text}\")\n",
    "            elif verbose:\n",
    "                print(\"Dog name element not found\")\n",
    "                \n",
    "            # Check page title\n",
    "            if verbose:\n",
    "                print(f\"Page title: {driver.title}\")\n",
    "            \n",
    "            # Check if there's an error message\n",
    "            error_elems = driver.find_elements(By.CSS_SELECTOR, \".error-message, .alert, .message\")\n",
    "            if verbose:\n",
    "                for elem in error_elems:\n",
    "                    print(f\"Error message found: {elem.text}\")\n",
    "            \n",
    "            # Check for race history table\n",
    "            race_tables = driver.find_elements(By.CSS_SELECTOR, \"table, .race-history, .results-table\")\n",
    "            if race_tables and verbose:\n",
    "                print(f\"Race table found with {len(race_tables)} elements\")\n",
    "                \n",
    "                # Count rows\n",
    "                rows = driver.find_elements(By.CSS_SELECTOR, \"tr, .race-row, .result-row\")\n",
    "                print(f\"Found {len(rows)} potential race rows\")\n",
    "            elif verbose:\n",
    "                print(\"No race history table found\")\n",
    "            \n",
    "            # Extract page source and save for inspection\n",
    "            with open(f\"dog_{dog_id}_page.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(driver.page_source)\n",
    "            if verbose:\n",
    "                print(f\"Page source saved to dog_{dog_id}_page.html\")\n",
    "            \n",
    "            # Try the race extraction\n",
    "            race_urls = fast_get_dog_race_urls(driver, dog_id, wait, verbose)\n",
    "            if verbose:\n",
    "                print(f\"Extracted {len(race_urls)} race URLs\")\n",
    "                \n",
    "                # Display the first few races found\n",
    "                if race_urls:\n",
    "                    print(\"\\nFirst few races found:\")\n",
    "                    for i, race in enumerate(race_urls[:3]):\n",
    "                        print(f\"  Race {i+1}: Meeting ID={race['meeting_id']}, Race ID={race['race_id']}\")\n",
    "                        print(f\"  URL: {race['race_url']}\")\n",
    "            \n",
    "            return race_urls\n",
    "            \n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Error during diagnosis: {e}\")\n",
    "            return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "        if verbose:\n",
    "            print(\"Driver closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf081492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Testing scraping for dog ID: 607694\n",
      "   Accessing dog profile: https://www.gbgb.org.uk/greyhound-profile/?greyhoundId=607694\n",
      "   Accessing dog profile: https://www.gbgb.org.uk/greyhound-profile/?greyhoundId=607694\n",
      "   ✅ Found dog name: Sweet Soul Music\n",
      "   🔍 Using direct link extraction for problematic dog ID\n",
      "   Found 22 potential race links\n",
      "   ✅ Found 20 race links for problematic dog ID\n",
      "📊 Found 20 races for dog 607694\n",
      "\n",
      "Sample of races found:\n",
      "  1. Meeting ID: 426921, Race ID: 1115765\n",
      "     URL: https://www.gbgb.org.uk/meeting/?meetingId=426921&raceId=1115765\n",
      "     Date: \n",
      "  2. Meeting ID: 426548, Race ID: 1114075\n",
      "     URL: https://www.gbgb.org.uk/meeting/?meetingId=426548&raceId=1114075\n",
      "     Date: \n",
      "  3. Meeting ID: 425995, Race ID: 1111481\n",
      "     URL: https://www.gbgb.org.uk/meeting/?meetingId=425995&raceId=1111481\n",
      "     Date: \n",
      "  4. Meeting ID: 425687, Race ID: 1109872\n",
      "     URL: https://www.gbgb.org.uk/meeting/?meetingId=425687&raceId=1109872\n",
      "     Date: \n",
      "  5. Meeting ID: 425212, Race ID: 1107894\n",
      "     URL: https://www.gbgb.org.uk/meeting/?meetingId=425212&raceId=1107894\n",
      "     Date: \n",
      "  ... and 15 more races\n",
      "   ✅ Found dog name: Sweet Soul Music\n",
      "   🔍 Using direct link extraction for problematic dog ID\n",
      "   Found 22 potential race links\n",
      "   ✅ Found 20 race links for problematic dog ID\n",
      "📊 Found 20 races for dog 607694\n",
      "\n",
      "Sample of races found:\n",
      "  1. Meeting ID: 426921, Race ID: 1115765\n",
      "     URL: https://www.gbgb.org.uk/meeting/?meetingId=426921&raceId=1115765\n",
      "     Date: \n",
      "  2. Meeting ID: 426548, Race ID: 1114075\n",
      "     URL: https://www.gbgb.org.uk/meeting/?meetingId=426548&raceId=1114075\n",
      "     Date: \n",
      "  3. Meeting ID: 425995, Race ID: 1111481\n",
      "     URL: https://www.gbgb.org.uk/meeting/?meetingId=425995&raceId=1111481\n",
      "     Date: \n",
      "  4. Meeting ID: 425687, Race ID: 1109872\n",
      "     URL: https://www.gbgb.org.uk/meeting/?meetingId=425687&raceId=1109872\n",
      "     Date: \n",
      "  5. Meeting ID: 425212, Race ID: 1107894\n",
      "     URL: https://www.gbgb.org.uk/meeting/?meetingId=425212&raceId=1107894\n",
      "     Date: \n",
      "  ... and 15 more races\n",
      "🏁 Driver closed\n",
      "🏁 Driver closed\n"
     ]
    }
   ],
   "source": [
    "# Test scraping for problematic dog ID 607694\n",
    "test_dog_id = \"607694\"\n",
    "print(f\"🔍 Testing scraping for dog ID: {test_dog_id}\")\n",
    "\n",
    "driver = create_fast_driver()\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "try:\n",
    "    # Run the enhanced scraping\n",
    "    race_urls = fast_get_dog_race_urls(driver, test_dog_id, wait, verbose=True)\n",
    "    print(f\"📊 Found {len(race_urls)} races for dog {test_dog_id}\")\n",
    "    \n",
    "    # Show sample of races found\n",
    "    if race_urls:\n",
    "        print(\"\\nSample of races found:\")\n",
    "        for i, race in enumerate(race_urls[:5]):\n",
    "            print(f\"  {i+1}. Meeting ID: {race['meeting_id']}, Race ID: {race['race_id']}\")\n",
    "            print(f\"     URL: {race['race_url']}\")\n",
    "            print(f\"     Date: {race['race_date']}\")\n",
    "        \n",
    "        if len(race_urls) > 5:\n",
    "            print(f\"  ... and {len(race_urls) - 5} more races\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "    print(\"🏁 Driver closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
